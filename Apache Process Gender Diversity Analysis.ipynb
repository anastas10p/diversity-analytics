{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook seeks to explore the gender diversity of the different apache projects & the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "Collecting meetup.api\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/44/a545b860f19cac088cb7f3e39beae8334b63e025729348ddeffb3342122a/meetup_api-0.1.1-py2.py3-none-any.whl (229kB)\n",
      "\u001b[K    100% |████████████████████████████████| 235kB 8.2MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from meetup.api)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from meetup.api)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->meetup.api)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->meetup.api)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->meetup.api)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->meetup.api)\n",
      "Installing collected packages: meetup.api\n",
      "Successfully installed meetup.api\n",
      "blockmgr-e352b968-7f0a-4623-9eac-b6b0b599e202\n",
      "spark-399e062b-2ef6-45d2-a612-c3640a2f853a\n",
      "/\n",
      "--2018-04-12 17:38:26--  https://raw.githubusercontent.com/holdenk/diversity-analytics/master/lazy_helpers.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1265 (1.2K) [text/plain]\n",
      "Saving to: ‘lazy_helpers.py’\n",
      "\n",
      "lazy_helpers.py     100%[=====================>]   1.24K  --.-KB/s   in 0s     \n",
      "\n",
      "2018-04-12 17:38:26 (37.1 MB/s) - ‘lazy_helpers.py’ saved [1265/1265]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!pip install meetup.api\n",
    "!ls /hadoop/spark/tmp/\n",
    "!cd /hadoop/spark/tmp/spark-*/userFiles-*/\n",
    "!pwd\n",
    "!rm lazy_helpers.py\n",
    "!wget https://raw.githubusercontent.com/holdenk/diversity-analytics/master/lazy_helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = os.environ['PATH'] + \":/usr/lib/chromium/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import concat, collect_set, explode, from_json, format_string\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import json\n",
    "import os\n",
    "import meetup.api\n",
    "from copy import copy\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API key configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meetup_key = os.getenv(\"MEETUP_APIKEY\")\n",
    "gh_api_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "fs_prefix = \"gs://boo-stuff/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less secret configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_meetup_events = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = SparkSession.builder.appName(\"whatCanWeLearnFromTheSixties\").getOrCreate()\n",
    "sc = session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to get is the committers and PMC members, this information is stored in LDAP but also available in JSON. Eventually we will want to enrich this with mailing list information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFlatJsonFile(path, explodeKey, schema=None):\n",
    "    \"\"\"Load a flat multi-line json file and convert into Spark & explode\"\"\"\n",
    "    rdd = sc.wholeTextFiles(path).values().setName(\"Input file {}\".format(path))\n",
    "    df = (session.read.schema(schema)\n",
    "            .json(rdd))\n",
    "    return df.select(explode(explodeKey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[username: string, extra: struct<name:string,key_fingerprints:array<string>,urls:array<string>>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_people_schema = StructType([StructField(\"lastCreateTimestamp\", StringType()),\n",
    "                     StructField(\"people\",\n",
    "                                 MapType(StringType(), \n",
    "                                         StructType([StructField('name', StringType()),\n",
    "                                                     StructField('key_fingerprints', ArrayType(StringType())),\n",
    "                                                     StructField('urls', ArrayType(StringType())),\n",
    "                                                    ]))\n",
    "                                )])\n",
    "apache_poeple_df_file = \"{0}{1}\".format(fs_prefix, \"http_data_sources/public_ldap_people.json\") # http://people.apache.org/public/public_ldap_people.json\n",
    "apache_people_df = loadFlatJsonFile(path=apache_poeple_df_file, \n",
    "                                 explodeKey=\"people\", schema=apache_people_schema)\n",
    "apache_people_df = apache_people_df.select(apache_people_df.key.alias(\"username\"), apache_people_df.value.alias(\"extra\")).repartition(100).persist().alias(\"apache_people\")\n",
    "apache_people_df.alias(\"Apache Committers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile(\"lazy_helpers.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lazy_helpers.LazyPool"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a lazy urllib3 pool\n",
    "from lazy_helpers import *\n",
    "    \n",
    "bcast_pool = sc.broadcast(LazyPool)\n",
    "bcast_pool.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_on_github(project):\n",
    "    \"\"\"Returns if a project is on github\"\"\"\n",
    "    import urllib3\n",
    "    http = bcast_pool.value.get()\n",
    "    r = http.request('GET', \"https://github.com/apache/{0}\".format(project))\n",
    "    return r.status == 200\n",
    "session.catalog.registerFunction(\"on_github\", project_on_github, BooleanType())\n",
    "# Except I'm a bad person so....\n",
    "from pyspark.sql.catalog import UserDefinedFunction\n",
    "project_on_github_udf = UserDefinedFunction(project_on_github, BooleanType(), \"on_github\")\n",
    "session.catalog._jsparkSession.udf().registerPython(\"on_github\", project_on_github_udf._judf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_committees_schema = StructType([StructField(\"lastCreateTimestamp\", StringType()),\n",
    "                     StructField(\"committees\",\n",
    "                                 MapType(StringType(), StructType([StructField('roster', ArrayType(StringType())),\n",
    "                                                                  StructField('modifyTimestamp', StringType()),\n",
    "                                                                  StructField('createTimestamp', StringType())\n",
    "                                                                  ])))])\n",
    "apache_committees_df_file = \"{0}{1}\".format(fs_prefix, \"http_data_sources/public_ldap_committees.json\") # http://people.apache.org/public/public_ldap_committees.json\n",
    "apache_committees_df = loadFlatJsonFile(path=apache_committees_df_file,\n",
    "                                 explodeKey=\"committees\", schema=apache_committees_schema)\n",
    "apache_committees_on_github_df = apache_committees_df.filter(project_on_github_udf(apache_committees_df.key))\n",
    "apache_committees_on_github_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "committee_names_df = apache_committees_on_github_df.select(apache_committees_df.key.alias(\"project\")).alias(\"apache_committees\").repartition(200)\n",
    "committee_names_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "committee_names_df.alias(\"Apache Committee Names\")\n",
    "committee_names_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[username: string, extra: struct<name:string,key_fingerprints:array<string>,urls:array<string>>, projects: array<string>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_to_user_df = apache_committees_on_github_df.select(\n",
    "    apache_committees_on_github_df.key.alias(\"project\"),\n",
    "    explode(apache_committees_on_github_df.value.roster).alias(\"username\"))\n",
    "\n",
    "\n",
    "user_to_project_df = project_to_user_df.groupBy(project_to_user_df.username).agg(\n",
    "    collect_set(project_to_user_df.project).alias(\"projects\"))\n",
    "apache_people_df = apache_people_df.join(user_to_project_df, on=\"username\")\n",
    "apache_people_df.alias(\"Apache People joined with projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(username='aching', extra=Row(name='Avery Ching', key_fingerprints=['2DC0BC2C'], urls=None), projects=['giraph'])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_people_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to fetch relevant past & present meetups for each project - idea based on the listing at https://www.apache.org/events/meetups.html but different code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do a non-blocking count to materialize the meetup RDD because this is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some async helpers, in Scala we would use AsyncRDDActions but its not currently available in Python\n",
    "# Support is being considered in https://issues.apache.org/jira/browse/SPARK-20347\n",
    "def non_blocking_rdd_count(rdd):\n",
    "    import threading\n",
    "    def count_magic():\n",
    "        rdd.count()\n",
    "    thread = threading.Thread(target=count_magic)\n",
    "    thread.start()\n",
    "\n",
    "def non_blocking_rdd_save(rdd, target):\n",
    "    import threading\n",
    "    def save_panda():\n",
    "        rdd.saveAsPickleFile(target)\n",
    "    thread = threading.Thread(target=save_panda)\n",
    "    thread.start()\n",
    "\n",
    "def non_blocking_df_save(df, target):\n",
    "    import threading\n",
    "    def save_panda():\n",
    "        df.write.save(target)\n",
    "    thread = threading.Thread(target=save_panda)\n",
    "    thread.start()\n",
    "\n",
    "def non_blocking_df_save_or_load(df, target):\n",
    "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jvm.java.net.URI(fs_prefix), sc._jsc.hadoopConfiguration())\n",
    "    success_files = [\"{0}/SUCCESS.txt\", \"{0}/_SUCCESS\", \"{0}\"]\n",
    "    if any(fs.exists(sc._jvm.org.apache.hadoop.fs.Path(t.format(target))) for t in success_files):\n",
    "        return session.read.load(target)\n",
    "    else:\n",
    "        non_blocking_df_save(df, target)\n",
    "        return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(\"WARN\")\n",
    "# For now, this is an avenue of future exploration, AKA holden's doesn't want her meetup API keys banned\n",
    "def lookup_relevant_meetup(project_name, max_meetup_events=0):\n",
    "    \"\"\"Lookup relevant meetups for a specific project.\"\"\"\n",
    "    import logging\n",
    "    import time\n",
    "    import meetup.api\n",
    "    logger = logging.getLogger()\n",
    "    meetup_delay = 30\n",
    "    meetup_reset_delay = 3600 # 1 hour\n",
    "    standard_keys = {\"text_format\": \"plain\", \"trending\": \"desc=true\", \"and_text\": \"true\", \"city\": \"san francisco\", \"country\": \"usa\", \"text\": \"apache \" + project_name, \"radius\": 10000}\n",
    "    results = {\"upcoming\": [], \"past\": []}\n",
    "    for status in [\"upcoming\", \"past\"]:\n",
    "        keys = copy(standard_keys)\n",
    "        keys[\"status\"] = status\n",
    "        count = 200\n",
    "        base = 0\n",
    "        while (count == 200 and (max_meetup_events == 0 or base < max_meetup_events)):\n",
    "            logging.debug(\"Fetch {0} meetups for {1} on base {2}\".format(status, project_name, base))\n",
    "            project_name = \"spark\"\n",
    "            client = client = meetup.api.Client(meetup_key)\n",
    "            if base > 0:\n",
    "                keys[\"page\"] = base\n",
    "            # Manually sleep for meetup_reset_delay on failure, the meetup-api package retry logic sometimes breaks :(\n",
    "            response = None\n",
    "            retry_count = 0\n",
    "            while response is None and retry_count < 10:\n",
    "                try:\n",
    "                    response = client.GetOpenEvents(**keys)\n",
    "                except:\n",
    "                    response = None\n",
    "                    retry_count += 1\n",
    "                    time.sleep(meetup_reset_delay)\n",
    "                    try:\n",
    "                        response = client.GetOpenEvents(**keys)\n",
    "                    except:\n",
    "                        response = None\n",
    "            try:\n",
    "                count = response.meta['count']\n",
    "                base = base + count\n",
    "                results[status].append(response.results)\n",
    "                time.sleep(meetup_delay)\n",
    "            except:\n",
    "                count = 0\n",
    "    return (project_name, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_rdd = committee_names_df.repartition(500).rdd.map(lambda x: x.project).map(lambda name: lookup_relevant_meetup(name, max_meetup_events))\n",
    "#project_meetups_rdd.setName(\"Meetup Data RDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "#raw_project_meetups_df = project_meetups_rdd.toDF() \n",
    "#raw_project_meetups_df.alias(\"Project -> meetup dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_df = non_blocking_df_save(raw_project_meetups_df, \"mini_meetup_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the provided projects attempt to lookup their GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_project_git(org, project):\n",
    "    \"\"\"Returns the project github for a specific project. Assumes project is git hosted\"\"\"\n",
    "    return \"https://github.com/{0}/{1}.git\".format(org, project)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_project_github_data(org, project):\n",
    "    \"\"\"Fetch the project github data, note this only gets github issues so likely not super useful\"\"\"\n",
    "    from perceval.backends.core.github import GitHub as perceval_github\n",
    "    gh_backend = perceval_github(owner=org, repository=project, api_token=gh_api_token)\n",
    "    # The backend return a generator - which is awesome. However since we want to pull this data into Spark \n",
    "    def append_project_info(result):\n",
    "        \"\"\"Add the project information to the return from perceval\"\"\"\n",
    "        result[\"project_name\"] = project\n",
    "        return result\n",
    "\n",
    "    return list(map(append_project_info, gh_backend.fetch()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_project_git_data(org, project):\n",
    "    from perceval.backends.core.git import Git as perceval_git\n",
    "\n",
    "    git_uri = lookup_project_git(org, project)\n",
    "    import tempfile\n",
    "    import shutil\n",
    "    tempdir = tempfile.mkdtemp()\n",
    "\n",
    "    def append_project_info(result):\n",
    "        \"\"\"Add the project information to the return from perceval\"\"\"\n",
    "        result[\"project_name\"] = project\n",
    "        return result\n",
    "\n",
    "    try:\n",
    "        git_backend = perceval_git(uri=git_uri, gitpath=tempdir + \"/repo\")\n",
    "        return list(map(append_project_info, git_backend.fetch()))\n",
    "    finally:\n",
    "        shutil.rmtree(tempdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the git history info using perceval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceival GIT dat UnionRDD[141] at union at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_git_project_data_rdd = committee_names_df.repartition(400).rdd.flatMap(lambda row: fetch_project_git_data(\"apache\", row.project))\n",
    "jupyter_git_project_data_rdd = sc.parallelize([(\"jupyter\", \"notebook\"), (\"nteract\", \"nteract\")]).flatMap(lambda elem: fetch_project_git_data(elem[0], elem[1]))\n",
    "git_project_data_rdd = apache_git_project_data_rdd.union(jupyter_git_project_data_rdd)\n",
    "git_project_data_rdd.persist()\n",
    "git_project_data_rdd.setName(\"Perceival GIT dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_list = git_project_data_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'backend_name': 'Git',\n",
       "  'backend_version': '0.10.2',\n",
       "  'perceval_version': '0.9.16',\n",
       "  'timestamp': 1523555212.233742,\n",
       "  'origin': 'https://github.com/apache/openwebbeans.git',\n",
       "  'uuid': 'b58c3d4d37e15bccb71a67ebc6a692de023e77e2',\n",
       "  'updated_on': 1227374941.0,\n",
       "  'category': 'commit',\n",
       "  'tag': 'https://github.com/apache/openwebbeans.git',\n",
       "  'data': {'commit': 'aafbe570875a9d3174f0bb126b86ab0875e39e7d',\n",
       "   'parents': [],\n",
       "   'refs': [],\n",
       "   'Author': 'Gurkan Erdogdu <gerdogdu@apache.org>',\n",
       "   'AuthorDate': 'Sat Nov 22 17:29:01 2008 +0000',\n",
       "   'Commit': 'Gurkan Erdogdu <gerdogdu@apache.org>',\n",
       "   'CommitDate': 'Sat Nov 22 17:29:01 2008 +0000',\n",
       "   'message': 'creating trunk folder\\n\\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/openwebbeans/trunk@719871 13f79535-47bb-0310-9956-ffa450edef68',\n",
       "   'files': []},\n",
       "  'project_name': 'openwebbeans'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_project_data_df = git_project_data_rdd.map(lambda row: Row(**row)).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(backend_name,StringType,true),StructField(backend_version,StringType,true),StructField(category,StringType,true),StructField(data,MapType(StringType,StringType,true),true),StructField(origin,StringType,true),StructField(perceval_version,StringType,true),StructField(project_name,StringType,true),StructField(tag,StringType,true),StructField(timestamp,DoubleType,true),StructField(updated_on,DoubleType,true),StructField(uuid,StringType,true)))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_project_data_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_by_project_and_commit_df = git_project_data_df.select(\"project_name\", \"data.Author\", \"data.CommitDate\")\n",
    "raw_distinct_authors_latest_commit = authors_by_project_and_commit_df.groupBy(\"project_name\", \"Author\").agg(F.max(\"CommitDate\").alias(\"latest_commit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boop = \"gs://boo-stuff/distinct_authors_latest_commit\"\n",
    "fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jvm.java.net.URI(fs_prefix), sc._jsc.hadoopConfiguration())\n",
    "fs.exists(sc._jvm.org.apache.hadoop.fs.Path(\"{0}/_SUCCESS\".format(boop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_authors_latest_commit = non_blocking_df_save_or_load(raw_distinct_authors_latest_commit, \"{0}distinct_authors_latest_commit\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[project_name: string, Author: string, latest_commit: string]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_authors_latest_commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20937"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookup info from crunchbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/conda/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/chromium/'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazy_helpers import *\n",
    "\n",
    "bcast_driver = sc.broadcast(LazyDriver)\n",
    "\n",
    "# TBD if we should see this, see comments on robots.txt in function, also consider overhead of firefox req\n",
    "def lookup_crunchbase_info(people_and_projects):\n",
    "    \"\"\"Lookup a person a crunch base and see what the gender & company is.\n",
    "    Filter for at least one mention of their projects.\"\"\"\n",
    "    # Path hack\n",
    "    if not \"chromium\" in os.environ['PATH']:\n",
    "        os.environ['PATH'] = os.environ['PATH'] + \":/usr/lib/chromium/\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    bcast_driver.value._driver = None\n",
    "    driver = bcast_driver.value.get()\n",
    "    import time\n",
    "    import random\n",
    "    for (username, name, projects, urls) in people_and_projects:\n",
    "        yield (username)\n",
    "        time.sleep(random.randint(60, 360))\n",
    "        # robots.txt seems to be ok with person for now as of April 4 2018, double check before re-running this\n",
    "        url = \"https://www.crunchbase.com/person/{0}\".format(name.replace(\" \", \"-\"))\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            text = driver.page_source\n",
    "            lower_text = text.lower()\n",
    "            yield [driver, lower_text]\n",
    "            if any(project.lower() in lower_text for project in projects) or any(url.lower in lower_text for url in urls):\n",
    "                soup = BeautifulSoup(text, \"html.parser\")\n",
    "                stats = soup.findAll(\"div\", { \"class\" : \"component--fields-card\"})[0]\n",
    "                # Hacky but I'm lazy\n",
    "                result = {}\n",
    "                result[\"crunchbase-url\"] = url\n",
    "                result[\"username\"] = username\n",
    "                if \"Female\" in str(stats):\n",
    "                    result[\"gender\"] = \"Female\"\n",
    "                if \"Male\" in str(stats):\n",
    "                    result[\"gender\"] = \"Male\"\n",
    "                try:\n",
    "                    m = re.search(\"\\\" title=\\\"(.+?)\\\" href=\\\"\\/organization\", lower_text)\n",
    "                    result[\"company\"] = m.group(1)\n",
    "                except:\n",
    "                    # No match no foul\n",
    "                    pass\n",
    "                yield result\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-f8ea9f9387a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_crunchbase_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"holden\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"holden karau\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"spark\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"http://www.holdenkarau.com\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-144-c0057ded8ba9>\u001b[0m in \u001b[0;36mlookup_crunchbase_info\u001b[0;34m(people_and_projects)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpeople_and_projects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m360\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# robots.txt seems to be ok with person for now as of April 4 2018, double check before re-running this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://www.crunchbase.com/person/{0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = lookup_crunchbase_info([(\"holden\", \"holden karau\", [\"spark\"], [\"http://www.holdenkarau.com\"])])\n",
    "list(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment the committer info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[username: string, gender: string, company: string]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We do this as an RDD transformation since the cost of the transformation dominates\n",
    "relevant_info = apache_people_df.select(\n",
    "    apache_people_df.username,\n",
    "    apache_people_df.extra.getField(\"name\").alias(\"name\"),\n",
    "    apache_people_df.projects,\n",
    "    apache_people_df.extra.getField(\"urls\").alias(\"urls\"))\n",
    "crunchbase_info_rdd = relevant_info.rdd.map(lambda row: (row.username, row.name, row.projects, row.urls)).mapPartitions(lookup_crunchbase_info)\n",
    "crunchbase_info_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "schema = StructType([\n",
    "    StructField(\"username\", StringType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"company\", StringType()),\n",
    "    StructField(\"crunchbase-url\", StringType())])\n",
    "crunchbase_info_df = crunchbase_info_rdd.toDF(schema = schema)\n",
    "crunchbase_info_df.alias(\"Crunchbase user information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "crunchbase_info_df = non_blocking_df_save_or_load(crunchbase_info_df, \"{0}crunchbase_out_3\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crunchbase_info_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2565"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_people_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to Mechnical turk format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_concat_udf(array_strs):\n",
    "    \"\"\"Concat the array of strs\"\"\"\n",
    "    if array_strs == None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return ' '.join(array_strs)\n",
    "\n",
    "# Except I'm a bad person so....\n",
    "from pyspark.sql.catalog import UserDefinedFunction\n",
    "mini_concat_udf = UserDefinedFunction(mini_concat_udf, StringType(), \"mini_concat_udf\")\n",
    "session.catalog._jsparkSession.udf().registerPython(\"mini_concat_udf\", mini_concat_udf._judf)\n",
    "\n",
    "apache_people_df.select(\n",
    "    apache_people_df.username,\n",
    "    apache_people_df.extra.getField(\"name\").alias(\"name\"),\n",
    "    mini_concat_udf(apache_people_df.extra.getField(\"urls\")).alias(\"personal_websites\"),\n",
    "    mini_concat_udf(apache_people_df.projects).alias(\"projects\")\n",
    "    ).coalesce(1).write.csv(\"{0}/apache_people.csv\".format(fs_prefix), header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crunchbase_info_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One of the things that is interesting is understanding what the tones of the meetup descriptions & mailing list posts are. We can use https://www.ibm.com/watson/developercloud/tone-analyzer/api/v3/?python#introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: take out toneanalyzer3 and use nltk instead because I don't have free access to this API anymore\n",
    "# TODO: use pandas acceleration maybe? some issues with dataproc \"support\"\n",
    "def lookup_tone(document):\n",
    "    \"\"\"Looks up the tone for a specific document. Returns a json blob.\"\"\"\n",
    "    from watson_developer_cloud import ToneAnalyzerV3\n",
    "    tone_analyzer = ToneAnalyzerV3(\n",
    "        username=tone_bluemix_user,\n",
    "        password=tone_bluemix_password,\n",
    "        version='2016-05-19 ')\n",
    "    return tone_analyzer.tone(text=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_no_you = lookup_tone(\"oh no you didn't girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_no_you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ok its time to find some mailing list info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbox_failures = sc.accumulator(0)\n",
    "\n",
    "def fetch_mbox_ids(project_name):\n",
    "    \"\"\"Return the mbox ids\"\"\"\n",
    "    import itertools\n",
    "\n",
    "    def fetch_mbox_ids_apache_site(box_type):\n",
    "        \"\"\"Fetches all of the mbox ids from a given apache project and box type (dev or user)\"\"\"\n",
    "        root_url = \"http://mail-archives.apache.org/mod_mbox/{0}-{1}\".format(project_name, box_type)\n",
    "        \n",
    "        # Fetch the page to parse\n",
    "        pool = bcast_pool.value.get()\n",
    "        result = pool.request('GET', root_url)\n",
    "        \n",
    "        \n",
    "        from bs4 import BeautifulSoup\n",
    "        soup = BeautifulSoup(result.data, \"html.parser\")\n",
    "        mbox_ids = set(map(lambda tag: tag.get('id'), soup.findAll(\"span\", { \"class\" : \"links\"})))\n",
    "        return map(lambda box_id: (project_name, box_type, box_id), mbox_ids)\n",
    "    # We have to return a list here because PySpark doesn't handle generators (TODO: holden)\n",
    "    return list(itertools.chain.from_iterable(map(fetch_mbox_ids_apache_site, [\"dev\", \"user\"])))\n",
    "        \n",
    "        \n",
    "def fetch_and_process_mbox_records(project_name, box_type, mbox_id):\n",
    "        import tempfile\n",
    "        import shutil\n",
    "        from perceval.backends.core.mbox import MBox as perceval_mbox\n",
    "\n",
    "        def process_mbox_directory(base_url, dir_path):\n",
    "            mbox_backend = perceval_mbox(base_url, dir_path)\n",
    "            return mbox_backend.fetch()\n",
    "        \n",
    "        def append_project_info(result):\n",
    "            \"\"\"Add the project information to the return from perceval\"\"\"\n",
    "            result[\"project_name\"] = project_name\n",
    "            result[\"box_type\"] = box_type\n",
    "            result[\"mbox_id\"] = mbox_id\n",
    "            return result\n",
    "\n",
    "        # Make a temp directory to hold the mbox files\n",
    "        tempdir = tempfile.mkdtemp()\n",
    "\n",
    "        try:\n",
    "            root_url = \"http://mail-archives.apache.org/mod_mbox/{0}-{1}\".format(project_name, box_type)\n",
    "            mbox_url = \"{0}/{1}.mbox\".format(root_url, mbox_id)\n",
    "            filename = \"{0}/{1}.mbox\".format(tempdir, mbox_id)\n",
    "        \n",
    "            print(\"fetching {0}\".format(mbox_url))\n",
    "\n",
    "            pool = bcast_pool.value.get()\n",
    "            with pool.request('GET', mbox_url, preload_content=False) as r, open(filename, 'wb') as out_file:       \n",
    "                try:\n",
    "                    shutil.copyfileobj(r, out_file)\n",
    "                    return list(map(append_project_info, process_mbox_directory(root_url, tempdir)))\n",
    "                except:\n",
    "                    mbox_failures.add(1)\n",
    "                    return []\n",
    "        finally:\n",
    "            shutil.rmtree(tempdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching http://mail-archives.apache.org/mod_mbox/spark-dev/201308.mbox\n"
     ]
    }
   ],
   "source": [
    "fetched_mbox_ids = fetch_mbox_ids(\"spark\")\n",
    "list(fetched_mbox_ids)[0]\n",
    "fetched_mbox_data = fetch_and_process_mbox_records('spark', 'dev', '201308')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backend_name': 'MBox',\n",
       " 'backend_version': '0.10.2',\n",
       " 'perceval_version': '0.9.14',\n",
       " 'timestamp': 1523588778.76019,\n",
       " 'origin': 'http://mail-archives.apache.org/mod_mbox/spark-dev',\n",
       " 'uuid': '82e0f3bdbbce4324ac365bc564d1221947eb5405',\n",
       " 'updated_on': 1375383652.0,\n",
       " 'category': 'message',\n",
       " 'tag': 'http://mail-archives.apache.org/mod_mbox/spark-dev',\n",
       " 'data': {'unixfrom': 'dev-return-121-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Aug  1 19:03:23 2013',\n",
       "  'Return-Path': '<dev-return-121-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>',\n",
       "  'X-Original-To': 'apmail-spark-dev-archive@minotaur.apache.org',\n",
       "  'Delivered-To': 'moderator for dev@spark.incubator.apache.org',\n",
       "  'Received': '(qmail 42870 invoked by uid 99); 1 Aug 2013 19:01:19 -0000',\n",
       "  'Mailing-List': 'contact dev-help@spark.incubator.apache.org; run by ezmlm',\n",
       "  'Precedence': 'bulk',\n",
       "  'List-Help': '<mailto:dev-help@spark.incubator.apache.org>',\n",
       "  'List-Unsubscribe': '<mailto:dev-unsubscribe@spark.incubator.apache.org>',\n",
       "  'List-Post': '<mailto:dev@spark.incubator.apache.org>',\n",
       "  'List-Id': '<dev.spark.incubator.apache.org>',\n",
       "  'Reply-To': 'dev@spark.incubator.apache.org',\n",
       "  'X-ASF-Spam-Status': 'No, hits=-0.0 required=5.0\\n\\ttests=SPF_PASS',\n",
       "  'X-Spam-Check-By': 'apache.org',\n",
       "  'Received-SPF': 'pass (athena.apache.org: local policy)',\n",
       "  'From': 'Mike <spark@good-with-numbers.com>',\n",
       "  'To': 'dev@spark.incubator.apache.org',\n",
       "  'Subject': 'Re: Saying hello and helping out',\n",
       "  'MIME-Version': '1.0',\n",
       "  'Content-Type': 'text/plain; charset=us-ascii',\n",
       "  'Content-Disposition': 'inline',\n",
       "  'In-Reply-To': '<48E7FDAD-E0F7-4317-BBAD-2AD45188C0CF@gmail.com>',\n",
       "  'X-Virus-Checked': 'Checked by ClamAV on apache.org',\n",
       "  'body': {'plain': \"Matei,\\n\\nOn Mon, Jul 29, 2013 at 04:17:49PM -0700, Matei Zaharia wrote:\\n> if you prefer to work on the Java VM, there are a bunch of internal \\n> things to do there too -- I can give an overview of what I'd consider \\n> easy to jump into there.\\n\\nI'd be interesting in hearing about this part.\\n\\n--\\nMike\\n\"},\n",
       "  'Message-ID': '<20130801190052.GA30564@64-142-29-25.dsl.static.sonic.net>',\n",
       "  'Date': 'Thu, 1 Aug 2013 19:00:52 +0000'},\n",
       " 'project_name': 'spark',\n",
       " 'box_type': 'dev',\n",
       " 'mbox_id': '201308'}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetched_mbox_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[177] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_key(x):\n",
    "    import random\n",
    "    return (random.randint(0, 40000), x)\n",
    "\n",
    "def de_key(x):\n",
    "    return x[1]\n",
    "\n",
    "mailing_list_posts_mbox_ids = committee_names_df.repartition(400).rdd.flatMap(lambda row: fetch_mbox_ids(row.project))\n",
    "# mbox's can be big, so break up how many partitions we have\n",
    "mailing_list_posts_mbox_ids = mailing_list_posts_mbox_ids.map(random_key).repartition(2000).map(de_key)\n",
    "mailing_list_posts_rdd = mailing_list_posts_mbox_ids.flatMap(lambda args: fetch_and_process_mbox_records(*args))\n",
    "mailing_list_posts_rdd.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[project_name: string, box_type: string, mbox_id: string, backend_name: string, backend_version: string, category: string, data: map<string,string>, origin: string, perceval_version: string, tag: string, timestamp: double, updated_on: double, uuid: string]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"project_name\",StringType()),\n",
    "    StructField(\"box_type\",StringType()), # dev or user\n",
    "    StructField(\"mbox_id\",StringType()),\n",
    "    StructField(\"backend_name\",StringType()),\n",
    "    StructField(\"backend_version\",StringType()),\n",
    "    StructField(\"category\",StringType()),\n",
    "    StructField(\"data\", MapType(StringType(),StringType())), # The \"important\" bits\n",
    "    StructField(\"origin\",StringType()),\n",
    "    StructField(\"perceval_version\",StringType()),\n",
    "    StructField(\"tag\",StringType()),\n",
    "    StructField(\"timestamp\",DoubleType()),\n",
    "    StructField(\"updated_on\",DoubleType()),\n",
    "    StructField(\"uuid\",StringType())])\n",
    "mailing_list_posts_mbox_df_raw = mailing_list_posts_rdd.toDF(schema=schema)\n",
    "mailing_list_posts_mbox_df_raw.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "mailing_list_posts_mbox_df_raw.alias(\"Mailing list perceival information - no post processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailing_list_posts_mbox_df_raw = non_blocking_df_save_or_load(mailing_list_posts_mbox_df_raw, \"mailing_list_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailing_list_posts_mbox_df = mailing_list_posts_mbox_df_raw.select(\"*\",\n",
    "                                                               mailing_list_posts_mbox_df_raw.data.getField(\"From\").alias(\"from\"),\n",
    "                                                               mailing_list_posts_mbox_df_raw.data.getField(\"body\").alias(\"body\")\n",
    "                                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-2830e85e5da4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start using some of the lazily created DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20937"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_authors_latest_commit.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+\n",
      "|project_name|              Author|       latest_commit|\n",
      "+------------+--------------------+--------------------+\n",
      "|    accumulo|Ed Coleman <dev1@...|Wed Jan 7 23:08:4...|\n",
      "|    activemq|Martyn Taylor <mt...|Wed Jun 7 11:09:4...|\n",
      "|         ant|Jesse Stockall <j...|Wed Apr 16 15:20:...|\n",
      "|      aurora|Steve Salevan <st...|Tue Jul 14 10:50:...|\n",
      "|        avro|Douglas Adam Crea...|Wed Feb 29 01:33:...|\n",
      "|        beam|Jean-Baptiste Ono...|Wed Sep 7 06:11:0...|\n",
      "|      bigtop|Andrew Kuchling <...|Wed Sep 21 12:18:...|\n",
      "|      bigtop|Chris Huang <chri...|Fri Jun 21 17:01:...|\n",
      "|      bigtop|roypradeep <roypr...|Thu Mar 23 10:27:...|\n",
      "|      bigtop|wenwu <pengwenwu2...|Wed Mar 19 22:43:...|\n",
      "|        bval|Albert Lee <allee...|Wed Feb 1 19:23:4...|\n",
      "|     calcite|Sree Vaddi <sree_...|Thu Sep 18 01:35:...|\n",
      "|       camel|Adrian Cole <adri...|Fri Feb 17 09:51:...|\n",
      "|       camel|Bruno Marco Visio...|Fri Sep 4 10:18:4...|\n",
      "|  carbondata|sgururajshetty <s...|Wed Mar 21 15:31:...|\n",
      "|  carbondata|sounakr <sounakr@...|Wed Apr 5 23:52:2...|\n",
      "|   cassandra|Joaquin Casares <...|Wed Jun 21 16:58:...|\n",
      "|   cassandra|Johnny Miller <jm...|Thu Jan 15 17:50:...|\n",
      "|   cassandra|Shogo Hoshii <sho...|Wed Aug 17 10:47:...|\n",
      "|   cassandra|T Jake Luciani <j...|Wed Jan 25 16:26:...|\n",
      "+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinct_authors_latest_commit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "| project_name|count(Author)|\n",
      "+-------------+-------------+\n",
      "|         lucy|           21|\n",
      "|      vxquery|           22|\n",
      "|    chemistry|           14|\n",
      "|       roller|           21|\n",
      "|        geode|          186|\n",
      "|       falcon|           82|\n",
      "|          tez|           36|\n",
      "|trafficserver|          434|\n",
      "|       pdfbox|           25|\n",
      "|        httpd|          148|\n",
      "|   carbondata|          205|\n",
      "|        celix|           19|\n",
      "|     accumulo|          135|\n",
      "|       wicket|          123|\n",
      "|        twill|           41|\n",
      "|   servicemix|           24|\n",
      "|     clerezza|           28|\n",
      "|      couchdb|          231|\n",
      "|       bigtop|          205|\n",
      "|     marmotta|           31|\n",
      "+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_authors_by_project = distinct_authors_latest_commit.groupBy(\"project_name\").agg(F.count(\"Author\"))\n",
    "num_authors_by_project.cache()\n",
    "num_authors_by_project.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the sample %s for each project so we can get reasonable confidence bounds for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_num_required_sample(pop_size):\n",
    "    import numpy as np\n",
    "    import scipy.stats\n",
    "    target_margin_of_error = 0.05\n",
    "    Z = 1.96 # 95%\n",
    "    p = 0.5\n",
    "    N = pop_size\n",
    "    # CALC SAMPLE SIZE\n",
    "    n_0 = ((Z**2) * p * (1-p)) / (e**2)\n",
    "    # ADJUST SAMPLE SIZE FOR FINITE POPULATION\n",
    "    n = n_0 / (1 + ((n_0 - 1) / float(N)) )\n",
    "    return int(math.ceil(n)) # THE SAMPLE SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = num_authors_by_project.withColumn(compute_num_required_sample(\"count(Author)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to infer Gender off of name. This should be used as a last-ditch fall back, see https://ironholds.org/names-gender/ for a discussion on why this is problematic. However without doing this it's difficult to get much of a picture (see above where we attempt to gender from other sources, the hit rate leaves something to be desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}