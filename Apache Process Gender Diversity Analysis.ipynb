{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook seeks to explore the gender diversity of the different apache projects & the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import concat, collect_set, explode, from_json, format_string\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import json\n",
    "import os\n",
    "import meetup.api\n",
    "from copy import copy\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API key configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meetup_key = os.getenv(\"MEETUP_APIKEY\")\n",
    "gh_api_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "fs_prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less secret configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_meetup_events = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = SparkSession.builder.appName(\"whatCanWeLearnFromTheSixties\").getOrCreate()\n",
    "sc = session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to get is the committers and PMC members, this information is stored in LDAP but also available in JSON. Eventually we will want to enrich this with mailing list information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFlatJsonFile(path, explodeKey, schema=None):\n",
    "    \"\"\"Load a flat multi-line json file and convert into Spark & explode\"\"\"\n",
    "    rdd = sc.wholeTextFiles(path).values().setName(\"Input file {}\".format(path))\n",
    "    df = (session.read.schema(schema)\n",
    "            .json(rdd))\n",
    "    return df.select(explode(explodeKey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loadFlatJsonFile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-eaacc8fbd95d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                 )])\n\u001b[1;32m      9\u001b[0m \u001b[0mapache_poeple_df_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{0}{1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"http_data_sources/public_ldap_people.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# http://people.apache.org/public/public_ldap_people.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m apache_people_df = loadFlatJsonFile(path=apache_people_df_file, \n\u001b[0m\u001b[1;32m     11\u001b[0m                                  explodeKey=\"people\", schema=apache_people_schema)\n\u001b[1;32m     12\u001b[0m \u001b[0mapache_people_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapache_people_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapache_people_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"username\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapache_people_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"extra\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"apache_people\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loadFlatJsonFile' is not defined"
     ]
    }
   ],
   "source": [
    "apache_people_schema = StructType([StructField(\"lastCreateTimestamp\", StringType()),\n",
    "                     StructField(\"people\",\n",
    "                                 MapType(StringType(), \n",
    "                                         StructType([StructField('name', StringType()),\n",
    "                                                     StructField('key_fingerprints', ArrayType(StringType())),\n",
    "                                                     StructField('urls', ArrayType(StringType())),\n",
    "                                                    ]))\n",
    "                                )])\n",
    "apache_poeple_df_file = \"{0}{1}\".format(fs_prefix, \"http_data_sources/public_ldap_people.json\") # http://people.apache.org/public/public_ldap_people.json\n",
    "apache_people_df = loadFlatJsonFile(path=apache_people_df_file, \n",
    "                                 explodeKey=\"people\", schema=apache_people_schema)\n",
    "apache_people_df = apache_people_df.select(apache_people_df.key.alias(\"username\"), apache_people_df.value.alias(\"extra\")).repartition(100).persist().alias(\"apache_people\")\n",
    "apache_people_df.alias(\"Apache Committers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a lazy urllib3 pool\n",
    "from lazy_helpers import *\n",
    "    \n",
    "bcast_pool = sc.broadcast(LazyPool)\n",
    "bcast_pool.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_on_github(project):\n",
    "    \"\"\"Returns if a project is on github\"\"\"\n",
    "    import urllib3\n",
    "    http = bcast_pool.value.get()\n",
    "    r = http.request('GET', \"https://github.com/apache/{0}\".format(project))\n",
    "    return r.status == 200\n",
    "session.catalog.registerFunction(\"on_github\", project_on_github, BooleanType())\n",
    "# Except I'm a bad person so....\n",
    "from pyspark.sql.catalog import UserDefinedFunction\n",
    "project_on_github_udf = UserDefinedFunction(project_on_github, BooleanType(), \"on_github\")\n",
    "session.catalog._jsparkSession.udf().registerPython(\"on_github\", project_on_github_udf._judf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_committees_schema = StructType([StructField(\"lastCreateTimestamp\", StringType()),\n",
    "                     StructField(\"committees\",\n",
    "                                 MapType(StringType(), StructType([StructField('roster', ArrayType(StringType())),\n",
    "                                                                  StructField('modifyTimestamp', StringType()),\n",
    "                                                                  StructField('createTimestamp', StringType())\n",
    "                                                                  ])))])\n",
    "apache_committees_df_file = \"{0}{1}\".format(fs_prefix, \"http_data_sources/public_ldap_committees.json\") # http://people.apache.org/public/public_ldap_committees.json\n",
    "apache_committees_df = loadFlatJsonFile(path=apache_committees_df_file,\n",
    "                                 explodeKey=\"committees\", schema=apache_committees_schema)\n",
    "apache_committees_on_github_df = apache_committees_df.filter(project_on_github_udf(apache_committees_df.key))\n",
    "apache_committees_on_github_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "committee_names_df = apache_committees_on_github_df.select(apache_committees_df.key.alias(\"project\")).alias(\"apache_committees\").repartition(200)\n",
    "committee_names_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "committee_names_df.alias(\"Apache Committee Names\")\n",
    "committee_names_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_to_user_df = apache_committees_on_github_df.select(\n",
    "    apache_committees_on_github_df.key.alias(\"project\"),\n",
    "    explode(apache_committees_on_github_df.value.roster).alias(\"username\"))\n",
    "\n",
    "\n",
    "user_to_project_df = project_to_user_df.groupBy(project_to_user_df.username).agg(\n",
    "    collect_set(project_to_user_df.project).alias(\"projects\"))\n",
    "apache_people_df = apache_people_df.join(user_to_project_df, on=\"username\")\n",
    "apache_people_df.alias(\"Apache People joined with projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_people_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to fetch relevant past & present meetups for each project - idea based on the listing at https://www.apache.org/events/meetups.html but different code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do a non-blocking count to materialize the meetup RDD because this is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some async helpers, in Scala we would use AsyncRDDActions but its not currently available in Python\n",
    "# Support is being considered in https://issues.apache.org/jira/browse/SPARK-20347\n",
    "def non_blocking_rdd_count(rdd):\n",
    "    import threading\n",
    "    def count_magic():\n",
    "        rdd.count()\n",
    "    thread = threading.Thread(target=count_magic)\n",
    "    thread.start()\n",
    "\n",
    "def non_blocking_rdd_save(rdd, target):\n",
    "    import threading\n",
    "    def save_panda():\n",
    "        rdd.saveAsPickleFile(target)\n",
    "    thread = threading.Thread(target=save_panda)\n",
    "    thread.start()\n",
    "\n",
    "def non_blocking_df_save(df, target):\n",
    "    import threading\n",
    "    def save_panda():\n",
    "        df.write.save(target)\n",
    "    thread = threading.Thread(target=save_panda)\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(\"WARN\")\n",
    "# For now, this is an avenue of future exploration, AKA holden's doesn't want her meetup API keys banned\n",
    "def lookup_relevant_meetup(project_name, max_meetup_events=0):\n",
    "    \"\"\"Lookup relevant meetups for a specific project.\"\"\"\n",
    "    import logging\n",
    "    import time\n",
    "    import meetup.api\n",
    "    logger = logging.getLogger()\n",
    "    meetup_delay = 30\n",
    "    meetup_reset_delay = 3600 # 1 hour\n",
    "    standard_keys = {\"text_format\": \"plain\", \"trending\": \"desc=true\", \"and_text\": \"true\", \"city\": \"san francisco\", \"country\": \"usa\", \"text\": \"apache \" + project_name, \"radius\": 10000}\n",
    "    results = {\"upcoming\": [], \"past\": []}\n",
    "    for status in [\"upcoming\", \"past\"]:\n",
    "        keys = copy(standard_keys)\n",
    "        keys[\"status\"] = status\n",
    "        count = 200\n",
    "        base = 0\n",
    "        while (count == 200 and (max_meetup_events == 0 or base < max_meetup_events)):\n",
    "            logging.debug(\"Fetch {0} meetups for {1} on base {2}\".format(status, project_name, base))\n",
    "            project_name = \"spark\"\n",
    "            client = client = meetup.api.Client(meetup_key)\n",
    "            if base > 0:\n",
    "                keys[\"page\"] = base\n",
    "            # Manually sleep for meetup_reset_delay on failure, the meetup-api package retry logic sometimes breaks :(\n",
    "            response = None\n",
    "            retry_count = 0\n",
    "            while response is None and retry_count < 10:\n",
    "                try:\n",
    "                    response = client.GetOpenEvents(**keys)\n",
    "                except:\n",
    "                    response = None\n",
    "                    retry_count += 1\n",
    "                    time.sleep(meetup_reset_delay)\n",
    "                    try:\n",
    "                        response = client.GetOpenEvents(**keys)\n",
    "                    except:\n",
    "                        response = None\n",
    "            try:\n",
    "                count = response.meta['count']\n",
    "                base = base + count\n",
    "                results[status].append(response.results)\n",
    "                time.sleep(meetup_delay)\n",
    "            except:\n",
    "                count = 0\n",
    "    return (project_name, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_rdd = committee_names_df.repartition(500).rdd.map(lambda x: x.project).map(lambda name: lookup_relevant_meetup(name, max_meetup_events))\n",
    "#project_meetups_rdd.setName(\"Meetup Data RDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "#project_meetups_df = project_meetups_rdd.toDF() \n",
    "#project_meetups_df.alias(\"Project -> meetup dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non_blocking_df_save(project_meetups_df, \"mini_meetup_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the provided projects attempt to lookup their GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_apaceh_project_git(org, project):\n",
    "    \"\"\"Returns the project github for a specific project. Assumes project is git hosted\"\"\"\n",
    "    return \"https://github.com/{0}/{1}.git\".format(org, project)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_project_github_data(project):\n",
    "    \"\"\"Fetch the project github data, note this only gets github issues so likely not super useful\"\"\"\n",
    "    from perceval.backends.core.github import GitHub as perceval_github\n",
    "    gh_backend = perceval_github(owner=\"apache\", repository=project, api_token=gh_api_token)\n",
    "    # The backend return a generator - which is awesome. However since we want to pull this data into Spark \n",
    "    def append_project_info(result):\n",
    "        \"\"\"Add the project information to the return from perceval\"\"\"\n",
    "        result[\"project_name\"] = project\n",
    "        return result\n",
    "\n",
    "    return list(map(append_project_info, gh_backend.fetch()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_project_git_data(org, project):\n",
    "    project_git = lookup_project_git(org, project)\n",
    "    from perceval.backends.core.git import Git as perceval_git\n",
    "\n",
    "    git_uri = lookup_project_git(project)\n",
    "    import tempfile\n",
    "    import shutil\n",
    "    tempdir = tempfile.mkdtemp()\n",
    "\n",
    "    def append_project_info(result):\n",
    "        \"\"\"Add the project information to the return from perceval\"\"\"\n",
    "        result[\"project_name\"] = project\n",
    "        return result\n",
    "\n",
    "    try:\n",
    "        git_backend = perceval_git(uri=git_uri, gitpath=tempdir + \"/repo\")\n",
    "        return list(map(append_project_info, git_backend.fetch()))\n",
    "    finally:\n",
    "        shutil.rmtree(tempdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the git history info using perceval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_git_project_data_rdd = committee_names_df.repartition(400).rdd.flatMap(lambda row: fetch_project_git_data(\"apache\", row.project))\n",
    "jupyter_git_project_data_rdd = sc.parallelize([(\"jupyter\", \"notebook\"), (\"nteract\", \"nteract\")]).flatMap(lambda elem: fetch_project_git_data(elem[0], elem[1]))\n",
    "git_project_data_rdd.persist()\n",
    "git_project_data_rdd.setName(\"Perceival GIT dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_list = git_project_data_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookup info from crunchbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazy_helpers import *\n",
    "\n",
    "bcast_driver = sc.broadcast(LazyDriver)\n",
    "\n",
    "# TBD if we should see this, see comments on robots.txt in function, also consider overhead of firefox req\n",
    "def lookup_crunchbase_info(people_and_projects):\n",
    "    \"\"\"Lookup a person a crunch base and see what the gender & company is.\n",
    "    Filter for at least one mention of their projects.\"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    driver = bcast_driver.value.get()\n",
    "    import time\n",
    "    import random\n",
    "    for (username, name, projects, urls) in people_and_projects:\n",
    "        time.sleep(random.randint(9, 18))\n",
    "        # robots.txt seems to be ok with person for now as of April 4 2018, double check before re-running this\n",
    "        url = \"https://www.crunchbase.com/person/{0}\".format(name.replace(\" \", \"-\"))\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            text = driver.page_source\n",
    "            lower_text = text.lower()\n",
    "            if any(project.lower() in lower_text for project in projects) or any(url.lower in lower_text for url in urls):\n",
    "                soup = BeautifulSoup(text, \"html.parser\")\n",
    "                stats = soup.findAll(\"div\", { \"class\" : \"info-card-overview-content\"})[0]\n",
    "                # Hacky but I'm lazy\n",
    "                result = {}\n",
    "                result[\"username\"] = username\n",
    "                try:\n",
    "                    m = re.search(\"Gender:\\</dt\\>\\<dd\\>(.+?)\\<\", str(stats))\n",
    "                    result[\"gender\"] = m.group(1)\n",
    "                except:\n",
    "                    # If nothing matches thats ok\n",
    "                    pass\n",
    "                try:\n",
    "                    m = re.search(\"data-name=\\\"(.+?)\\\" data-permalink=\\\"\\/organization\", str(stats))\n",
    "                    result[\"company\"] = m.group(1)\n",
    "                except:\n",
    "                    # No match no foul\n",
    "                    pass\n",
    "                yield result\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lookup_crunchbase_info([(\"holden\", \"holden karau\", [\"spark\"], [\"http://www.holdenkarau.com\"])])\n",
    "list(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment the committer info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do this as an RDD transformation since the cost of the transformation dominates\n",
    "relevant_info = apache_people_df.select(\n",
    "    apache_people_df.username,\n",
    "    apache_people_df.extra.getField(\"name\").alias(\"name\"),\n",
    "    apache_people_df.projects,\n",
    "    apache_people_df.extra.getField(\"urls\").alias(\"urls\"))\n",
    "crunchbase_info_rdd = relevant_info.rdd.map(lambda row: (row.username, row.name, row.projects, row.urls)).mapPartitions(lookup_crunchbase_info)\n",
    "crunchbase_info_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "schema = StructType([\n",
    "    StructField(\"username\", StringType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"company\", StringType())])\n",
    "crunchbase_info_df = crunchbase_info_rdd.toDF(schema = schema)\n",
    "crunchbase_info_df.alias(\"Crunchbase user information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_blocking_df_save(crunchbase_info_df, \"crunchbase_out_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crunchbase_info_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_people_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to Mechnical turk format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_concat_udf(array_strs):\n",
    "    \"\"\"Concat the array of strs\"\"\"\n",
    "    if array_strs == None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return ' '.join(array_strs)\n",
    "\n",
    "# Except I'm a bad person so....\n",
    "from pyspark.sql.catalog import UserDefinedFunction\n",
    "mini_concat_udf = UserDefinedFunction(mini_concat_udf, StringType(), \"mini_concat_udf\")\n",
    "session.catalog._jsparkSession.udf().registerPython(\"mini_concat_udf\", mini_concat_udf._judf)\n",
    "\n",
    "apache_people_df.select(\n",
    "    apache_people_df.username,\n",
    "    apache_people_df.extra.getField(\"name\").alias(\"name\"),\n",
    "    mini_concat_udf(apache_people_df.extra.getField(\"urls\")).alias(\"personal_websites\"),\n",
    "    mini_concat_udf(apache_people_df.projects).alias(\"projects\")\n",
    "    ).coalesce(1).write.csv(\"./apache_people.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crunchbase_info_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One of the things that is interesting is understanding what the tones of the meetup descriptions & mailing list posts are. We can use https://www.ibm.com/watson/developercloud/tone-analyzer/api/v3/?python#introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: take out toneanalyzer3 and use nltk instead because I don't have free access to this API anymore\n",
    "# TODO: use pandas acceleration maybe? some issues with dataproc \"support\"\n",
    "def lookup_tone(document):\n",
    "    \"\"\"Looks up the tone for a specific document. Returns a json blob.\"\"\"\n",
    "    from watson_developer_cloud import ToneAnalyzerV3\n",
    "    tone_analyzer = ToneAnalyzerV3(\n",
    "        username=tone_bluemix_user,\n",
    "        password=tone_bluemix_password,\n",
    "        version='2016-05-19 ')\n",
    "    return tone_analyzer.tone(text=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_no_you = lookup_tone(\"oh no you didn't girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_no_you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ok its time to find some mailing list info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbox_failures = sc.accumulator(0)\n",
    "\n",
    "def fetch_mbox_ids(project_name):\n",
    "    \"\"\"Return the mbox ids\"\"\"\n",
    "    import itertools\n",
    "\n",
    "    def fetch_mbox_ids_apache_site(box_type):\n",
    "        \"\"\"Fetches all of the mbox ids from a given apache project and box type (dev or user)\"\"\"\n",
    "        root_url = \"http://mail-archives.apache.org/mod_mbox/{0}-{1}\".format(project_name, box_type)\n",
    "        \n",
    "        # Fetch the page to parse\n",
    "        pool = bcast_pool.value.get()\n",
    "        result = pool.request('GET', root_url)\n",
    "        \n",
    "        \n",
    "        from bs4 import BeautifulSoup\n",
    "        soup = BeautifulSoup(result.data, \"html.parser\")\n",
    "        mbox_ids = set(map(lambda tag: tag.get('id'), soup.findAll(\"span\", { \"class\" : \"links\"})))\n",
    "        return map(lambda box_id: (project_name, box_type, box_id), mbox_ids)\n",
    "    # We have to return a list here because PySpark doesn't handle generators (TODO: holden)\n",
    "    return list(itertools.chain.from_iterable(map(fetch_mbox_ids_apache_site, [\"dev\", \"user\"])))\n",
    "        \n",
    "        \n",
    "def fetch_and_process_mbox_records(project_name, box_type, mbox_id):\n",
    "        import tempfile\n",
    "        import shutil\n",
    "        from perceval.backends.core.mbox import MBox as perceval_mbox\n",
    "\n",
    "        def process_mbox_directory(base_url, dir_path):\n",
    "            mbox_backend = perceval_mbox(base_url, dir_path)\n",
    "            return mbox_backend.fetch()\n",
    "        \n",
    "        def append_project_info(result):\n",
    "            \"\"\"Add the project information to the return from perceval\"\"\"\n",
    "            result[\"project_name\"] = project_name\n",
    "            result[\"box_type\"] = box_type\n",
    "            result[\"mbox_id\"] = mbox_id\n",
    "            return result\n",
    "\n",
    "        # Make a temp directory to hold the mbox files\n",
    "        tempdir = tempfile.mkdtemp()\n",
    "\n",
    "        try:\n",
    "            root_url = \"http://mail-archives.apache.org/mod_mbox/{0}-{1}\".format(project_name, box_type)\n",
    "            mbox_url = \"{0}/{1}.mbox\".format(root_url, mbox_id)\n",
    "            filename = \"{0}/{1}.mbox\".format(tempdir, mbox_id)\n",
    "        \n",
    "            print(\"fetching {0}\".format(mbox_url))\n",
    "\n",
    "            pool = bcast_pool.value.get()\n",
    "            with pool.request('GET', mbox_url, preload_content=False) as r, open(filename, 'wb') as out_file:       \n",
    "                try:\n",
    "                    shutil.copyfileobj(r, out_file)\n",
    "                    return list(map(append_project_info, process_mbox_directory(root_url, tempdir)))\n",
    "                except:\n",
    "                    mbox_failures.add(1)\n",
    "                    return []\n",
    "        finally:\n",
    "            shutil.rmtree(tempdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_mbox_ids = fetch_mbox_ids(\"spark\")\n",
    "list(fetched_mbox_ids)[0]\n",
    "fetched_mbox_data = fetch_and_process_mbox_records('spark', 'dev', '201308')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_mbox_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_key(x):\n",
    "    import random\n",
    "    return (random.randint(0, 40000), x)\n",
    "\n",
    "def de_key(x):\n",
    "    return x[1]\n",
    "\n",
    "mailing_list_posts_mbox_ids = committee_names_df.repartition(400).rdd.flatMap(lambda row: fetch_mbox_ids(row.project))\n",
    "# mbox's can be big, so break up how many partitions we have\n",
    "mailing_list_posts_mbox_ids = mailing_list_posts_mbox_ids.map(random_key).repartition(2000).map(de_key)\n",
    "mailing_list_posts_rdd = mailing_list_posts_mbox_ids.flatMap(lambda args: fetch_and_process_mbox_records(*args))\n",
    "mailing_list_posts_rdd.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"project_name\",StringType()),\n",
    "    StructField(\"box_type\",StringType()), # dev or user\n",
    "    StructField(\"mbox_id\",StringType()),\n",
    "    StructField(\"backend_name\",StringType()),\n",
    "    StructField(\"backend_version\",StringType()),\n",
    "    StructField(\"category\",StringType()),\n",
    "    StructField(\"data\", MapType(StringType(),StringType())), # The \"important\" bits\n",
    "    StructField(\"origin\",StringType()),\n",
    "    StructField(\"perceval_version\",StringType()),\n",
    "    StructField(\"tag\",StringType()),\n",
    "    StructField(\"timestamp\",DoubleType()),\n",
    "    StructField(\"updated_on\",DoubleType()),\n",
    "    StructField(\"uuid\",StringType())])\n",
    "mailing_list_posts_mbox_df_raw = mailing_list_posts_rdd.toDF(schema=schema)\n",
    "mailing_list_posts_mbox_df_raw.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "mailing_list_posts_mbox_df_raw.alias(\"Mailing list perceival information - no post processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_blocking_df_save(mailing_list_posts_mbox_df_raw, \"mailing_list_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailing_list_posts_mbox_df = mailing_list_posts_mbox_df_raw.select(\"*\",\n",
    "                                                               mailing_list_posts_mbox_df_raw.data.getField(\"From\").alias(\"from\"),\n",
    "                                                               mailing_list_posts_mbox_df_raw.data.getField(\"body\").alias(\"body\")\n",
    "                                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to infer Gender off of name. This should be used as a last-ditch fall back, see https://ironholds.org/names-gender/ for a discussion on why this is problematic. However without doing this it's difficult to get much of a picture (see above where we attempt to gender from other sources, the hit rate leaves something to be desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
