{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook seeks to explore the gender diversity of the different apache projects & the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-08-22 22:21:02--  https://raw.githubusercontent.com/holdenk/diversity-analytics/master/lazy_helpers.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1397 (1.4K) [text/plain]\n",
      "Saving to: ‘lazy_helpers.py’\n",
      "\n",
      "lazy_helpers.py     100%[===================>]   1.36K  --.-KB/s    in 0s      \n",
      "\n",
      "2018-08-22 22:21:03 (45.8 MB/s) - ‘lazy_helpers.py’ saved [1397/1397]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Kind of a hack because of the Spark notebook serialization issues\n",
    "!rm lazy_helpers.py*\n",
    "!wget https://raw.githubusercontent.com/holdenk/diversity-analytics/master/lazy_helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yarn\\n',\n",
       " 'yarn\\n',\n",
       " 'yarn\\n',\n",
       " 'yarn\\n',\n",
       " 'yarn\\n',\n",
       " 'yarn\\n',\n",
       " 'yarn\\n',\n",
       " 'yarn\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hack to update sparklingml on a running cluster -\n",
    "#TODO(holden): release sparkling ml properly so no hacks\n",
    "memory_status_count = sc._jsc.sc().getExecutorMemoryStatus().size()\n",
    "estimated_executors = max(sc.defaultParallelism, memory_status_count)\n",
    "rdd = sc.parallelize(range(estimated_executors))\n",
    "def do_update(x):\n",
    "    import os\n",
    "    return str(os.popen(\"whoami && cd /sparklingml && git pull && git log -n 5\").read())\n",
    "result = rdd.map(do_update)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = os.environ['PATH'] + \":/usr/lib/chromium/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import concat, collect_set, explode, from_json, format_string\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.session import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import json\n",
    "import os\n",
    "import meetup.api\n",
    "from copy import copy\n",
    "import time\n",
    "import logging\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API key configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gh_api_token & meetup_key & genderize_key\n",
    "exec(open(\"./secrets.literal\").read())\n",
    "gh_user = \"holdenk\"\n",
    "fs_prefix = \"gs://boo-stuff/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less secret configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_meetup_events = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkSession.builder.getOrCreate().stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = (SparkSession.builder\n",
    "           .appName(\"whatCanWeLearnFromTheSixties\")\n",
    "           .config(\"spark.executor.instances\", \"45\")\n",
    "           .config(\"spark.driver.memoryOverhead\", \"0.25\")\n",
    "           .config(\"spark.executor.memory\", \"16g\")\n",
    "           .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "           .config(\"spark.ui.enabled\", \"true\")\n",
    "          ).getOrCreate()\n",
    "sc = session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'holden-magic-m:18080'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In _theory_ in preview dataproc Spark UI is force disabled but history fills the gap, except history server isn't started by default :(\n",
    "sc.getConf().get(\"spark.yarn.historyServer.address\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to get is the committers and PMC members, this information is stored in LDAP but also available in JSON. Eventually we will want to enrich this with mailing list information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFlatJsonFile(path, explodeKey, schema=None):\n",
    "    \"\"\"Load a flat multi-line json file and convert into Spark & explode\"\"\"\n",
    "    rdd = sc.wholeTextFiles(path).values().setName(\"Input file {}\".format(path))\n",
    "    df = (session.read.schema(schema)\n",
    "            .json(rdd))\n",
    "    return df.select(explode(explodeKey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[username: string, extra: struct<name:string,key_fingerprints:array<string>,urls:array<string>>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_people_schema = StructType([StructField(\"lastCreateTimestamp\", StringType()),\n",
    "                     StructField(\"people\",\n",
    "                                 MapType(StringType(), \n",
    "                                         StructType([StructField('name', StringType()),\n",
    "                                                     StructField('key_fingerprints', ArrayType(StringType())),\n",
    "                                                     StructField('urls', ArrayType(StringType())),\n",
    "                                                    ]))\n",
    "                                )])\n",
    "apache_poeple_df_file = \"{0}{1}\".format(fs_prefix, \"http_data_sources/public_ldap_people.json\") # http://people.apache.org/public/public_ldap_people.json\n",
    "apache_people_df = loadFlatJsonFile(path=apache_poeple_df_file, \n",
    "                                 explodeKey=\"people\", schema=apache_people_schema)\n",
    "apache_people_df = apache_people_df.select(apache_people_df.key.alias(\"username\"), apache_people_df.value.alias(\"extra\")).repartition(100).persist().alias(\"apache_people\")\n",
    "apache_people_df.alias(\"Apache Committers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile(\"lazy_helpers.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lazy_helpers.LazyPool"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a lazy urllib3 pool\n",
    "from lazy_helpers import *\n",
    "    \n",
    "bcast_pool = sc.broadcast(LazyPool)\n",
    "bcast_pool.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_on_github(project):\n",
    "    \"\"\"Returns if a project is on github\"\"\"\n",
    "    import urllib3\n",
    "    http = bcast_pool.value.get()\n",
    "    r = http.request('GET', \"https://github.com/apache/{0}\".format(project))\n",
    "    return r.status == 200\n",
    "session.catalog.registerFunction(\"on_github\", project_on_github, BooleanType())\n",
    "# Except I'm a bad person so....\n",
    "from pyspark.sql.catalog import UserDefinedFunction\n",
    "project_on_github_udf = UserDefinedFunction(project_on_github, BooleanType(), \"on_github\")\n",
    "session.catalog._jsparkSession.udf().registerPython(\"on_github\", project_on_github_udf._judf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_committees_schema = StructType([StructField(\"lastCreateTimestamp\", StringType()),\n",
    "                     StructField(\"committees\",\n",
    "                                 MapType(StringType(), StructType([StructField('roster', ArrayType(StringType())),\n",
    "                                                                  StructField('modifyTimestamp', StringType()),\n",
    "                                                                  StructField('createTimestamp', StringType())\n",
    "                                                                  ])))])\n",
    "apache_committees_df_file = \"{0}{1}\".format(fs_prefix, \"http_data_sources/public_ldap_committees.json\") # http://people.apache.org/public/public_ldap_committees.json\n",
    "apache_committees_df = loadFlatJsonFile(path=apache_committees_df_file,\n",
    "                                 explodeKey=\"committees\", schema=apache_committees_schema)\n",
    "apache_committees_on_github_df = apache_committees_df.filter(project_on_github_udf(apache_committees_df.key))\n",
    "apache_committees_on_github_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "committee_names_df = apache_committees_on_github_df.select(apache_committees_df.key.alias(\"project\")).alias(\"apache_committees\").repartition(200)\n",
    "committee_names_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "committee_names_df.alias(\"Apache Committee Names\")\n",
    "committee_names_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[username: string, extra: struct<name:string,key_fingerprints:array<string>,urls:array<string>>, projects: array<string>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_to_user_df = apache_committees_on_github_df.select(\n",
    "    apache_committees_on_github_df.key.alias(\"project\"),\n",
    "    explode(apache_committees_on_github_df.value.roster).alias(\"username\"))\n",
    "\n",
    "\n",
    "user_to_project_df = project_to_user_df.groupBy(project_to_user_df.username).agg(\n",
    "    collect_set(project_to_user_df.project).alias(\"projects\"))\n",
    "apache_people_df = apache_people_df.join(user_to_project_df, on=\"username\")\n",
    "apache_people_df.alias(\"Apache People joined with projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(username='macdonst', extra=Row(name='Simon MacDonald', key_fingerprints=None, urls=None), projects=['cordova'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_people_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to fetch relevant past & present meetups for each project - idea based on the listing at https://www.apache.org/events/meetups.html but different code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do a non-blocking count to materialize the meetup RDD because this is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some async helpers, in Scala we would use AsyncRDDActions but its not currently available in Python\n",
    "# Support is being considered in https://issues.apache.org/jira/browse/SPARK-20347\n",
    "def non_blocking_rdd_count(rdd):\n",
    "    import threading\n",
    "    def count_magic():\n",
    "        rdd.count()\n",
    "    thread = threading.Thread(target=count_magic)\n",
    "    thread.start()\n",
    "\n",
    "def non_blocking_rdd_save(rdd, target):\n",
    "    import threading\n",
    "    def save_panda():\n",
    "        rdd.saveAsPickleFile(target)\n",
    "    thread = threading.Thread(target=save_panda)\n",
    "    thread.start()\n",
    "\n",
    "def non_blocking_df_save(df, target):\n",
    "    import threading\n",
    "    def save_panda():\n",
    "        df.write.mode(\"overwrite\").save(target)\n",
    "    thread = threading.Thread(target=save_panda)\n",
    "    thread.start()\n",
    "\n",
    "def non_blocking_df_save_csv(df, target):\n",
    "    import threading\n",
    "    def save_panda():\n",
    "        df.write.format(\"csv\").mode(\"overwrite\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"quoteAll\", \"false\") \\\n",
    "            .save(target)\n",
    "    thread = threading.Thread(target=save_panda)\n",
    "    thread.start()\n",
    "\n",
    "def non_blocking_df_save_or_load(df, target):\n",
    "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jvm.java.net.URI(fs_prefix), sc._jsc.hadoopConfiguration())\n",
    "    success_files = [\"{0}/SUCCESS.txt\", \"{0}/_SUCCESS\"]\n",
    "    if any(fs.exists(sc._jvm.org.apache.hadoop.fs.Path(t.format(target))) for t in success_files):\n",
    "        print(\"Reusing\")\n",
    "        return session.read.load(target).persist()\n",
    "    else:\n",
    "        print(\"Saving\")\n",
    "        non_blocking_df_save(df, target)\n",
    "        return df\n",
    "\n",
    "def non_blocking_df_save_or_load_csv(df, target):\n",
    "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jvm.java.net.URI(fs_prefix), sc._jsc.hadoopConfiguration())\n",
    "    success_files = [\"{0}/SUCCESS.txt\", \"{0}/_SUCCESS\"]\n",
    "    if any(fs.exists(sc._jvm.org.apache.hadoop.fs.Path(t.format(target))) for t in success_files):\n",
    "        print(\"Reusing\")\n",
    "        return session.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"true\").load(target).persist()\n",
    "    else:\n",
    "        print(\"Saving\")\n",
    "        non_blocking_df_save_csv(df, target)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(\"WARN\")\n",
    "# For now, this is an avenue of future exploration, AKA holden's doesn't want her meetup API keys banned\n",
    "def lookup_relevant_meetup(project_name, max_meetup_events=0):\n",
    "    \"\"\"Lookup relevant meetups for a specific project.\"\"\"\n",
    "    import logging\n",
    "    import time\n",
    "    import meetup.api\n",
    "    logger = logging.getLogger()\n",
    "    meetup_delay = 30\n",
    "    meetup_reset_delay = 3600 # 1 hour\n",
    "    standard_keys = {\"text_format\": \"plain\", \"trending\": \"desc=true\", \"and_text\": \"true\", \"city\": \"san francisco\", \"country\": \"usa\", \"text\": \"apache \" + project_name, \"radius\": 10000}\n",
    "    results = {\"upcoming\": [], \"past\": []}\n",
    "    for status in [\"upcoming\", \"past\"]:\n",
    "        keys = copy(standard_keys)\n",
    "        keys[\"status\"] = status\n",
    "        count = 200\n",
    "        base = 0\n",
    "        while (count == 200 and (max_meetup_events == 0 or base < max_meetup_events)):\n",
    "            logging.debug(\"Fetch {0} meetups for {1} on base {2}\".format(status, project_name, base))\n",
    "            project_name = \"spark\"\n",
    "            client = client = meetup.api.Client(meetup_key)\n",
    "            if base > 0:\n",
    "                keys[\"page\"] = base\n",
    "            # Manually sleep for meetup_reset_delay on failure, the meetup-api package retry logic sometimes breaks :(\n",
    "            response = None\n",
    "            retry_count = 0\n",
    "            while response is None and retry_count < 10:\n",
    "                try:\n",
    "                    response = client.GetOpenEvents(**keys)\n",
    "                except:\n",
    "                    response = None\n",
    "                    retry_count += 1\n",
    "                    time.sleep(meetup_reset_delay)\n",
    "                    try:\n",
    "                        response = client.GetOpenEvents(**keys)\n",
    "                    except:\n",
    "                        response = None\n",
    "            try:\n",
    "                count = response.meta['count']\n",
    "                base = base + count\n",
    "                results[status].append(response.results)\n",
    "                time.sleep(meetup_delay)\n",
    "            except:\n",
    "                count = 0\n",
    "    return (project_name, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Meetup Data RDD PythonRDD[72] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_meetups_rdd = committee_names_df.repartition(500).rdd.map(lambda x: x.project).map(lambda name: lookup_relevant_meetup(name, max_meetup_events))\n",
    "project_meetups_rdd.setName(\"Meetup Data RDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "#raw_project_meetups_df = project_meetups_rdd.toDF() \n",
    "#raw_project_meetups_df.alias(\"Project -> meetup dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_df = non_blocking_df_save_or_load(\n",
    "#    raw_project_meetups_df, \"mini_meetup_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the provided projects attempt to lookup their GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_project_git(org, project):\n",
    "    \"\"\"Returns the project github for a specific project. Assumes project is git hosted\"\"\"\n",
    "    return \"https://github.com/{0}/{1}.git\".format(org, project)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_project_github_data(org, project):\n",
    "    \"\"\"Fetch the project github data, note this only gets github issues so likely not super useful\"\"\"\n",
    "    from perceval.backends.core.github import GitHub as perceval_github\n",
    "    gh_backend = perceval_github(owner=org, repository=project, api_token=gh_api_token)\n",
    "    # The backend return a generator - which is awesome. However since we want to pull this data into Spark \n",
    "    def append_project_info(result):\n",
    "        \"\"\"Add the project information to the return from perceval\"\"\"\n",
    "        result[\"project_name\"] = project\n",
    "        return result\n",
    "\n",
    "    return list(map(append_project_info, gh_backend.fetch()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_project_git_data(org, project):\n",
    "    from perceval.backends.core.git import Git as perceval_git\n",
    "\n",
    "    git_uri = lookup_project_git(org, project)\n",
    "    import tempfile\n",
    "    import shutil\n",
    "    tempdir = tempfile.mkdtemp()\n",
    "\n",
    "    def append_project_info(result):\n",
    "        \"\"\"Add the project information to the return from perceval\"\"\"\n",
    "        result[\"project_name\"] = project\n",
    "        return result\n",
    "\n",
    "    try:\n",
    "        git_backend = perceval_git(uri=git_uri, gitpath=tempdir + \"/repo\")\n",
    "        return list(map(append_project_info, git_backend.fetch()))\n",
    "    finally:\n",
    "        shutil.rmtree(tempdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the git history info using perceval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceival GIT dat UnionRDD[84] at union at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_git_project_data_rdd = committee_names_df.repartition(400).rdd.flatMap(lambda row: fetch_project_git_data(\"apache\", row.project))\n",
    "jupyter_git_project_data_rdd = sc.parallelize([(\"jupyter\", \"notebook\"), (\"nteract\", \"nteract\")]).flatMap(lambda elem: fetch_project_git_data(elem[0], elem[1]))\n",
    "git_project_data_rdd = apache_git_project_data_rdd.union(jupyter_git_project_data_rdd)\n",
    "git_project_data_rdd.setName(\"Perceival GIT dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "git_project_data_df_raw = git_project_data_rdd.map(lambda row: Row(**row)).toDF().persist()\n",
    "git_project_data_df = non_blocking_df_save_or_load(git_project_data_df_raw, \"{0}/raw_git_data\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_project_data_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_authors_by_project_and_commit_df = git_project_data_df.select(\"project_name\", \"data.Author\", \"data.CommitDate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_authors_by_project_and_commit_df.show()\n",
    "raw_authors_by_project_and_commit_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(StringType())\n",
    "def strip_junk(inputSeries):\n",
    "    \"\"\"Discard timezone information, who needs that anyways.\n",
    "    More accurately we don't care about that here since we're looking at a year long window.\"\"\"\n",
    "    return inputSeries.apply(lambda x: x.split(\"+\")[0])\n",
    "\n",
    "@F.pandas_udf(StringType())\n",
    "def extract_email(inputSeries):\n",
    "    \"\"\"Take e-mails of the form Foo Baz<foobaz@baz.com> and turn it into foobaz@baz.com\"\"\"\n",
    "    import re\n",
    "    def extract_email_record(record):\n",
    "        try:\n",
    "            emails = re.findall('<\\S+>$', record)\n",
    "            return emails[0]\n",
    "        except:\n",
    "            return record\n",
    "    \n",
    "    return inputSeries.apply(extract_email_record)\n",
    "\n",
    "@F.pandas_udf(StringType())\n",
    "def extract_name(inputSeries):\n",
    "    \"\"\"Take e-mails of the form Foo Baz<foobaz@baz.com> and turn it into the probable name e.g. Foo Baz\"\"\"\n",
    "    import re\n",
    "    def extract_name_record(record):\n",
    "        try:\n",
    "            emails = re.findall('([^<]+)<\\S+>$', record)\n",
    "            return emails[0]\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    return inputSeries.apply(extract_name_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_by_project_and_commit_df = raw_authors_by_project_and_commit_df.select(\n",
    "    \"project_name\", \"Author\", extract_email(\"Author\").alias(\"email\"), extract_name(\"Author\").alias(\"name\"),\n",
    "    F.to_date(strip_junk(\"CommitDate\"), format=\"EEE MMM d H:mm:ss YYYY \").alias(\"CommitDate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_by_project_and_commit_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_distinct_authors_latest_commit = authors_by_project_and_commit_df.groupBy(\n",
    "    \"project_name\", \"email\").agg(\n",
    "    F.last(\"Author\").alias(\"Author\"), F.max(\"CommitDate\").alias(\"latest_commit\"))\n",
    "raw_distinct_authors_latest_commit.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_authors_latest_commit = non_blocking_df_save_or_load(\n",
    "    raw_distinct_authors_latest_commit,\n",
    "    \"{0}distinct_authors_latest_commit_4\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(StringType(), functionType=F.PandasUDFType.SCALAR)\n",
    "def lookup_github_user_by_email(emails):\n",
    "    \n",
    "    import time\n",
    "    from github import Github\n",
    "    import backoff\n",
    "    github_client = Github(gh_user, gh_api_token)\n",
    "    # In theory PyGithub handles backoff but we have multiple instances/machines.\n",
    "    @backoff.on_exception(backoff.expo, Exception)\n",
    "    def inner_lookup_github_user_by_email(email):\n",
    "        \"\"\"Lookup github user by e-mail address and returns the github username. Returns None if no user or more than 1 user is found.\"\"\"\n",
    "        users = github_client.search_users(\"{0}\".format(email))\n",
    "        def process_result(users):\n",
    "            if users.totalCount == 1:\n",
    "                return list(users).pop().login\n",
    "            else:\n",
    "                return \"\"\n",
    "\n",
    "        return process_result(users)\n",
    "\n",
    "    return emails.apply(inner_lookup_github_user_by_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_to_github_username = distinct_authors_latest_commit.withColumn(\n",
    "    \"github_username\",\n",
    "    lookup_github_user_by_email(\"email\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(returnType=StringType(), functionType=F.PandasUDFType.SCALAR)\n",
    "def fetch_github_user_bio(logins):\n",
    "    from github import Github\n",
    "    import time\n",
    "    github_client = Github(gh_user, gh_api_token)\n",
    "    import backoff\n",
    "\n",
    "    @backoff.on_exception(backoff.expo, Exception)\n",
    "    def individual_fetch_github_user_bio(login):\n",
    "        if login == None or login == \"\":\n",
    "            return \"\"\n",
    "\n",
    "        result = github_client.get_user(login=login)\n",
    "        try:\n",
    "            return result.bio\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    return logins.apply(individual_fetch_github_user_bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_to_github_username.persist()\n",
    "authors_to_github_username_saved = non_blocking_df_save_or_load(\n",
    "    authors_to_github_username,\n",
    "    \"{0}/authors_to_github-10\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_authors_latest_commit.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_to_github_username_saved.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_authors_with_gh = authors_to_github_username_saved.withColumn(\n",
    "    \"new_unique_id\",\n",
    "    F.when(F.col(\"github_username\") != \"\",\n",
    "         F.col(\"github_username\")).otherwise(\n",
    "        F.col(\"email\")))\n",
    "                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_grouped_by_id = distinct_authors_with_gh.groupBy(\"project_name\", \"new_unique_id\").agg(\n",
    "    collect_set(F.col(\"email\")).alias(\"emails\"),\n",
    "    F.last(F.col(\"Author\")).alias(\"Author\"),\n",
    "    F.first(\"github_username\").alias(\"github_username\"),\n",
    "    F.max(\"latest_commit\").alias(\"latest_commit\"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_grouped_by_id.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_grouped_by_id.persist()\n",
    "authors_grouped_by_id_saved = non_blocking_df_save_or_load(\n",
    "    authors_grouped_by_id,\n",
    "    \"{0}/authors_grouped_by_id-3\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookup info from crunchbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazy_helpers import *\n",
    "\n",
    "bcast_driver = sc.broadcast(LazyDriver)\n",
    "\n",
    "# TBD if we should see this, see comments on robots.txt in function, also consider overhead of firefox req\n",
    "def lookup_crunchbase_info(people_and_projects):\n",
    "    \"\"\"Lookup a person a crunch base and see what the gender & company is.\n",
    "    Filter for at least one mention of their projects.\"\"\"\n",
    "    # Path hack\n",
    "    if not \"chromium\" in os.environ['PATH']:\n",
    "        os.environ['PATH'] = os.environ['PATH'] + \":/usr/lib/chromium/\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    driver = bcast_driver.value.get()\n",
    "    import time\n",
    "    import random\n",
    "    for (username, name, projects, urls) in people_and_projects:\n",
    "        time.sleep(random.randint(60, 2*60))\n",
    "        # robots.txt seems to be ok with person for now as of April 4 2018, double check before re-running this\n",
    "        url = \"https://www.crunchbase.com/person/{0}\".format(name.replace(\" \", \"-\"))\n",
    "        try:\n",
    "            if driver.current_url != url:\n",
    "                driver.get(url)\n",
    "            text = driver.page_source\n",
    "            lower_text = text.lower()\n",
    "            yield[lower_text]\n",
    "            if \"the quick brown fox jumps over the lazy dog\" in lower_text or \"pardon our interruption...\" in lower_text:\n",
    "                time.sleep(random.randint(30*60, 2*60*60))\n",
    "                bcast_driver.value.reset()\n",
    "            if any(project.lower() in lower_text for project in projects) or any(url.lower in lower_text for url in urls):\n",
    "                soup = BeautifulSoup(text, \"html.parser\")\n",
    "                stats = soup.findAll(\"div\", { \"class\" : \"component--fields-card\"})[0]\n",
    "                # Hacky but I'm lazy\n",
    "                result = {}\n",
    "                result[\"crunchbase-url\"] = url\n",
    "                result[\"username\"] = username\n",
    "                if \"Female\" in str(stats):\n",
    "                    result[\"gender\"] = \"Female\"\n",
    "                if \"Male\" in str(stats):\n",
    "                    result[\"gender\"] = \"Male\"\n",
    "                try:\n",
    "                    m = re.search(\"\\\" title=\\\"(.+?)\\\" href=\\\"\\/organization\", lower_text)\n",
    "                    result[\"company\"] = m.group(1)\n",
    "                except:\n",
    "                    # No match no foul\n",
    "                    pass\n",
    "                yield result\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = lookup_crunchbase_info([(\"holden\", \"holden karau\", [\"spark\"], [\"http://www.holdenkarau.com\"])])\n",
    "#list(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment the committer info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do this as an RDD transformation since the cost of the transformation dominates\n",
    "relevant_info = apache_people_df.select(\n",
    "    apache_people_df.username,\n",
    "    apache_people_df.extra.getField(\"name\").alias(\"name\"),\n",
    "    apache_people_df.projects,\n",
    "    apache_people_df.extra.getField(\"urls\").alias(\"urls\"))\n",
    "crunchbase_info_rdd = relevant_info.rdd.map(lambda row: (row.username, row.name, row.projects, row.urls)).mapPartitions(lookup_crunchbase_info)\n",
    "crunchbase_info_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "schema = StructType([\n",
    "    StructField(\"username\", StringType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"company\", StringType()),\n",
    "    StructField(\"crunchbase-url\", StringType())])\n",
    "crunchbase_info_df = crunchbase_info_rdd.toDF(schema = schema)\n",
    "crunchbase_info_df.alias(\"Crunchbase user information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crunchbase_info_df = non_blocking_df_save_or_load(\n",
    "    crunchbase_info_df,\n",
    "    \"{0}crunchbase_out_11\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crunchbase_info_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_people_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_people_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to Mechnical turk format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_concat_udf(array_strs):\n",
    "    \"\"\"Concat the array of strs\"\"\"\n",
    "    if array_strs == None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return ' '.join(array_strs)\n",
    "\n",
    "# Except I'm a bad person so....\n",
    "from pyspark.sql.catalog import UserDefinedFunction\n",
    "mini_concat_udf = UserDefinedFunction(mini_concat_udf, StringType(), \"mini_concat_udf\")\n",
    "session.catalog._jsparkSession.udf().registerPython(\"mini_concat_udf\", mini_concat_udf._judf)\n",
    "\n",
    "mini_csv_data_df = apache_people_df.select(\n",
    "    apache_people_df.username,\n",
    "    apache_people_df.extra.getField(\"name\").alias(\"name\"),\n",
    "    mini_concat_udf(apache_people_df.extra.getField(\"urls\")).alias(\"personal_websites\"),\n",
    "    mini_concat_udf(apache_people_df.projects).alias(\"projects\")\n",
    "    ).coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_csv_data_df = non_blocking_df_save_or_load_csv(\n",
    "    mini_csv_data_df,\n",
    "    \"{0}/apache_people.csv\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crunchbase_info_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One of the things that is interesting is understanding what the tones of the meetup descriptions & mailing list posts are. We can use https://www.ibm.com/watson/developercloud/tone-analyzer/api/v3/?python#introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pandas UDF accelerate (but multiple pieces of informaiton returned at the same time)\n",
    "def lookup_sentiment(document):\n",
    "    \"\"\"Looks up the sentiment for a specific document.\"\"\"\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "    # Hack to download if needed\n",
    "    # TODO(holden): Consider broadcast variable?\n",
    "    try:\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "    except LookupError:\n",
    "        import nltk\n",
    "        nltk.download('vader_lexicon')\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    return sid.polarity_scores(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_sentiment(\"Thanks! I still think it needs a bit more work, but.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_sentiment(\"Who fucking broke the build?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ok its time to find some mailing list info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_schema = StructType([\n",
    "    StructField(\"neg\", DoubleType()),\n",
    "    StructField(\"neu\", DoubleType()),\n",
    "    StructField(\"pos\", DoubleType()),\n",
    "    StructField(\"compound\", DoubleType())])\n",
    "\n",
    "lookup_sentiment_udf = UserDefinedFunction(\n",
    "    lookup_sentiment,\n",
    "    sentiment_schema,\n",
    "    \"lookup_sentiment_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbox_failures = sc.accumulator(0)\n",
    "\n",
    "def fetch_mbox_ids(project_name):\n",
    "    \"\"\"Return the mbox ids\"\"\"\n",
    "    import itertools\n",
    "\n",
    "    def fetch_mbox_ids_apache_site(box_type):\n",
    "        \"\"\"Fetches all of the mbox ids from a given apache project and box type (dev or user)\"\"\"\n",
    "        root_url = \"http://mail-archives.apache.org/mod_mbox/{0}-{1}\".format(project_name, box_type)\n",
    "        \n",
    "        # Fetch the page to parse\n",
    "        pool = bcast_pool.value.get()\n",
    "        result = pool.request('GET', root_url)\n",
    "        \n",
    "        \n",
    "        from bs4 import BeautifulSoup\n",
    "        soup = BeautifulSoup(result.data, \"html.parser\")\n",
    "        mbox_ids = set(map(lambda tag: tag.get('id'), soup.findAll(\"span\", { \"class\" : \"links\"})))\n",
    "        return map(lambda box_id: (project_name, box_type, box_id), mbox_ids)\n",
    "    # We have to return a list here because PySpark doesn't handle generators (TODO: holden)\n",
    "    return list(itertools.chain.from_iterable(map(fetch_mbox_ids_apache_site, [\"dev\", \"user\"])))\n",
    "        \n",
    "        \n",
    "def fetch_and_process_mbox_records(project_name, box_type, mbox_id):\n",
    "        import tempfile\n",
    "        import shutil\n",
    "        from perceval.backends.core.mbox import MBox as perceval_mbox\n",
    "\n",
    "        def process_mbox_directory(base_url, dir_path):\n",
    "            mbox_backend = perceval_mbox(base_url, dir_path)\n",
    "            return mbox_backend.fetch()\n",
    "        \n",
    "        def append_project_info(result):\n",
    "            \"\"\"Add the project information to the return from perceval\"\"\"\n",
    "            result[\"project_name\"] = project_name\n",
    "            result[\"box_type\"] = box_type\n",
    "            result[\"mbox_id\"] = mbox_id\n",
    "            return result\n",
    "\n",
    "        # Make a temp directory to hold the mbox files\n",
    "        tempdir = tempfile.mkdtemp()\n",
    "\n",
    "        try:\n",
    "            root_url = \"http://mail-archives.apache.org/mod_mbox/{0}-{1}\".format(project_name, box_type)\n",
    "            mbox_url = \"{0}/{1}.mbox\".format(root_url, mbox_id)\n",
    "            filename = \"{0}/{1}.mbox\".format(tempdir, mbox_id)\n",
    "        \n",
    "            print(\"fetching {0}\".format(mbox_url))\n",
    "\n",
    "            pool = bcast_pool.value.get()\n",
    "            with pool.request('GET', mbox_url, preload_content=False) as r, open(filename, 'wb') as out_file:       \n",
    "                try:\n",
    "                    shutil.copyfileobj(r, out_file)\n",
    "                    return list(map(append_project_info, process_mbox_directory(root_url, tempdir)))\n",
    "                except:\n",
    "                    mbox_failures.add(1)\n",
    "                    return []\n",
    "        finally:\n",
    "            shutil.rmtree(tempdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_key(x):\n",
    "    import random\n",
    "    return (random.randint(0, 40000), x)\n",
    "\n",
    "def de_key(x):\n",
    "    return x[1]\n",
    "\n",
    "mailing_list_posts_mbox_ids = committee_names_df.repartition(400).rdd.flatMap(lambda row: fetch_mbox_ids(row.project))\n",
    "# mbox's can be big, so break up how many partitions we have\n",
    "mailing_list_posts_mbox_ids = mailing_list_posts_mbox_ids.map(random_key).repartition(2000).map(de_key)\n",
    "mailing_list_posts_rdd = mailing_list_posts_mbox_ids.flatMap(lambda args: fetch_and_process_mbox_records(*args))\n",
    "mailing_list_posts_rdd.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"project_name\",StringType()),\n",
    "    StructField(\"box_type\",StringType()), # dev or user\n",
    "    StructField(\"mbox_id\",StringType()),\n",
    "    StructField(\"backend_name\",StringType()),\n",
    "    StructField(\"backend_version\",StringType()),\n",
    "    StructField(\"category\",StringType()),\n",
    "    StructField(\"data\", MapType(StringType(),StringType())), # The \"important\" bits\n",
    "    StructField(\"origin\",StringType()),\n",
    "    StructField(\"perceval_version\",StringType()),\n",
    "    StructField(\"tag\",StringType()),\n",
    "    StructField(\"timestamp\",DoubleType()),\n",
    "    StructField(\"updated_on\",DoubleType()),\n",
    "    StructField(\"uuid\",StringType())])\n",
    "mailing_list_posts_mbox_df_raw = mailing_list_posts_rdd.toDF(schema=schema)\n",
    "mailing_list_posts_mbox_df_raw.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "mailing_list_posts_mbox_df_raw.alias(\"Mailing list perceival information - no post processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailing_list_posts_mbox_df_raw = non_blocking_df_save_or_load(\n",
    "    mailing_list_posts_mbox_df_raw,\n",
    "    \"{0}mailing_list_info_6\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = mailing_list_posts_mbox_df_raw.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailing_list_posts_mbox_df = mailing_list_posts_mbox_df_raw.select(\n",
    "    \"*\",\n",
    "    mailing_list_posts_mbox_df_raw.data.getField(\"From\").alias(\"from\"),\n",
    "    extract_email(mailing_list_posts_mbox_df_raw.data.getField(\"From\")).alias(\"from_processed_email\"),\n",
    "    mailing_list_posts_mbox_df_raw.data.getField(\"body\").alias(\"body\"),\n",
    "    mailing_list_posts_mbox_df_raw.data.getField(\"Message-ID\").alias(\"message_id\"),\n",
    "    mailing_list_posts_mbox_df_raw.data.getField(\"In-Reply-To\").alias(\"in_reply_to\"),\n",
    "    mailing_list_posts_mbox_df_raw.data.getField(\"Content-Language\").alias(\"content_language\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailing_list_posts_mbox_df_saved = non_blocking_df_save_or_load(\n",
    "    mailing_list_posts_mbox_df,\n",
    "    \"{0}/processed_mbox_data_4\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_sentiment_df = mailing_list_posts_mbox_df_saved.select(\"project_name\", lookup_sentiment_udf(\"body\").alias(\"sentiment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_sentiment_df_saved = non_blocking_df_save_or_load(\n",
    "    post_sentiment_df,\n",
    "    \"{0}/post_sentiment_df_1\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_people_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailing_list_posts_mbox_df_saved.schema"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Find committers welcome e-mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apache_people_df.join(mailing_list_posts_mbox_df_saved,\n",
    "#                     join_conditions(F.instr(mailing_list_posts_mbox_df_saved)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start using some of the lazily created DFs to compute the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_grouped_by_id_saved.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_grouped_by_id_saved.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_authors_by_project = authors_grouped_by_id_saved.groupBy(\"project_name\").agg(F.count(\"Author\").alias(\"author_count\"))\n",
    "num_authors_by_project.cache()\n",
    "num_authors_by_project.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the sample %s for each project so we can get reasonable confidence bounds for sampling.\n",
    "Looking at http://veekaybee.github.io/2015/08/04/how-big-of-a-sample-size-do-you-need/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think is Cochran's formula scaled for small datasets\n",
    "@F.udf(IntegerType())\n",
    "def compute_num_required_sample_1(pop_size):\n",
    "    import numpy as np\n",
    "    import scipy.stats\n",
    "    import math\n",
    "    e = 0.05\n",
    "    Z = 1.64 # 90%, 95%: 1.96\n",
    "    p = 0.5\n",
    "    N = pop_size\n",
    "    # CALC SAMPLE SIZE\n",
    "    n_0 = ((Z**2) * p * (1-p)) / (e**2)\n",
    "    # ADJUST SAMPLE SIZE FOR FINITE POPULATION\n",
    "    n = n_0 / (1 + ((n_0 - 1) / float(N)) )\n",
    "    target = int(math.ceil(n))\n",
    "    # Compute a fall back size\n",
    "    fall_back_size = min(3, pop_size)\n",
    "    return max(fall_back_size, target) # THE SAMPLE SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number 2: https://en.wikipedia.org/wiki/Sample_size_determination#Estimation\n",
    "def walds_method():\n",
    "    return 1/(0.05**2) # +- 5%\n",
    "walds_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sample_sizes = num_authors_by_project.withColumn(\n",
    "    \"sample_size_1\",\n",
    "    compute_num_required_sample_1(\"author_count\")).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = non_blocking_df_save_or_load(\n",
    "    raw_sample_sizes,\n",
    "    \"{0}/sample_sizes_10\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes.groupby().agg(F.sum(\"sample_size_1\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is a bit high to do on a shoestring budget with sampling, but what about if we limit to folks who have recently participated & got rid of projects with limited or no recent participation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_grouped_by_id_saved.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_distinct_authors_latest_commit = authors_grouped_by_id_saved.filter(\n",
    "    (F.date_sub(F.current_date(), 365)) < authors_grouped_by_id_saved.latest_commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_distinct_authors_latest_commit.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_distinct_authors_latest_commit.show()\n",
    "print(active_distinct_authors_latest_commit.count())\n",
    "active_distinct_authors_latest_commit.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_num_authors_by_project = active_distinct_authors_latest_commit.groupBy(\"project_name\").agg(F.count(\"Author\").alias(\"author_count\"))\n",
    "active_num_authors_by_project.cache()\n",
    "active_num_authors_by_project.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_raw_sample_sizes = active_num_authors_by_project.withColumn(\n",
    "    \"sample_size_1\",\n",
    "    compute_num_required_sample_1(\"author_count\")).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_sample_sizes = non_blocking_df_save_or_load_csv(\n",
    "    active_raw_sample_sizes,\n",
    "    \"{0}/active_sample_sizes_13\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_active_sample_sizes = active_sample_sizes.filter(\n",
    "    active_sample_sizes.sample_size_1 > 10).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_active_sample_sizes.groupby().agg(F.sum(\"sample_size_1\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's probably ok, lets go ahead and compute the sample set for each project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_fractions = filtered_active_sample_sizes.withColumn(\n",
    "    \"sample_fraction\",\n",
    "    (filtered_active_sample_sizes.sample_size_1+0.5) / filtered_active_sample_sizes.author_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_sample_fractions = sample_fractions.select(\n",
    "    sample_fractions.project_name, sample_fractions.sample_fraction).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_sample_fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_authors = active_distinct_authors_latest_commit.sampleBy(\n",
    "    \"project_name\",\n",
    "    fractions=dict(map(lambda r: (r[0], min(1.0, r[1])), local_sample_fractions)),\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_authors_saved = non_blocking_df_save_or_load(\n",
    "    sampled_authors, \"{0}/sampled_authors_6\".format(fs_prefix)).alias(\"sampled_authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_authors_saved.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_authors_saved.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_authors_saved.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_authors_grouped_by_author_id = sampled_authors_saved.groupBy(\n",
    "    sampled_authors_saved.new_unique_id).agg(\n",
    "    F.collect_set(sampled_authors_saved.project_name).alias(\"projects\"),\n",
    "    F.first(sampled_authors_saved.emails).alias(\"emails\"),\n",
    "    F.first(sampled_authors_saved.Author).alias(\"Author\"),\n",
    "    F.first(sampled_authors_saved.github_username).alias(\"github_username\"))\n",
    "sampled_authors_grouped_by_author_id_flattened = sampled_authors_grouped_by_author_id.select(\n",
    "    \"new_unique_id\",\n",
    "    F.concat_ws(' ', sampled_authors_grouped_by_author_id.projects).alias(\"projects\"),\n",
    "    \"emails\",\n",
    "    \"Author\",\n",
    "    \"github_username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_authors_grouped_by_author_id_flattened.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_authors_grouped_by_author_id_flattened.cache()\n",
    "sampled_authors_grouped_by_author_id_flattened.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the sampled authors with the e-mails on the dev list and find the top 3 most recent responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailing_list_posts_mbox_df_saved.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_authors_grouped_by_author_id_flattened.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_posts_by_authors(authors):\n",
    "    join_conditions = [\n",
    "        #sampled_authors_saved.project_name == mailing_list_posts_mbox_df_saved.project_name,\n",
    "        F.expr(\"array_contains(emails, from_processed_email)\")]\n",
    "    return authors.join(mailing_list_posts_mbox_df_saved, join_conditions).select(\n",
    "        \"message_id\", \"new_unique_id\").alias(\"posts_by_sampled_authors\")\n",
    "\n",
    "posts_by_sampled_authors = extract_posts_by_authors(\n",
    "    sampled_authors_grouped_by_author_id_flattened).alias(\n",
    "    \"posts_by_sampled_authors\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_by_sampled_authors_saved = non_blocking_df_save_or_load(\n",
    "    posts_by_sampled_authors, \"{0}/posts_by_sampled_authors_5\".format(fs_prefix)).alias(\"posts_by_sampled_authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_by_sampled_authors_saved.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailing_list_posts_in_reply_to = mailing_list_posts_mbox_df_saved.filter(\n",
    "    mailing_list_posts_mbox_df_saved.in_reply_to.isNotNull()).alias(\"mailing_list_posts_in_reply_to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_5k_chars(tokens):\n",
    "    return tokens[0:5000]\n",
    "\n",
    "first_5k_chars_udf = UserDefinedFunction(\n",
    "    first_5k_chars,\n",
    "    StringType(),\n",
    "    \"first_5k_chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_pronoun(tokens):\n",
    "    common_pronouns = [\"they\", \"ze\", \"he\", \"she\", \"her\", \"his\", \"their\"]\n",
    "    return any(pronoun in tokens for pronoun in common_pronouns)\n",
    "\n",
    "contains_pronoun_udf = UserDefinedFunction(\n",
    "    contains_pronoun,\n",
    "    BooleanType(),\n",
    "    \"contains_pronoun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_posts_with_replies(mailing_list_posts_in_reply_to, posts):\n",
    "    \n",
    "    posts_with_replies = posts.join(\n",
    "        mailing_list_posts_in_reply_to,\n",
    "        [F.col(\"mailing_list_posts_in_reply_to.in_reply_to\") == posts.message_id],\n",
    "        \"inner\")\n",
    "    posts_in_response_to_user = posts_with_replies.select(\n",
    "        posts_with_replies.new_unique_id,\n",
    "        posts_with_replies.timestamp,\n",
    "        posts_with_replies.project_name,\n",
    "        posts_with_replies.body.alias(\"orig_body\"),\n",
    "        first_5k_chars_udf(posts_with_replies.body).alias(\"body\"))\n",
    "    posts_in_response_to_user.cache()\n",
    "    \n",
    "    from sparklingml.feature.python_pipelines import SpacyTokenizeTransformer\n",
    "    spacy_tokenizer = SpacyTokenizeTransformer(inputCol=\"body\", outputCol=\"body_tokens\")\n",
    "    \n",
    "    posts_in_response_to_user_tokenized = spacy_tokenizer.transform(\n",
    "        posts_in_response_to_user)\n",
    "    posts_in_response_to_user_tokenized.cache()\n",
    "    # Need to break the chain... its not great.\n",
    "    \n",
    "    posts_in_response_to_user_with_pronouns = posts_in_response_to_user_tokenized.filter(\n",
    "        contains_pronoun_udf(posts_in_response_to_user_tokenized.body_tokens))\n",
    "    # nyet\n",
    "    posts_in_response_to_user_with_pronouns.cache()\n",
    "    #return posts_in_response_to_user_with_pronouns\n",
    "\n",
    "    posts_in_response_to_user_grouped = posts_in_response_to_user_with_pronouns.orderBy(\n",
    "        posts_in_response_to_user.timestamp).groupBy(\n",
    "        posts_in_response_to_user.new_unique_id)\n",
    "    posts_in_response_to_user_collected = posts_in_response_to_user_grouped.agg(\n",
    "        F.collect_list(posts_in_response_to_user_with_pronouns.body).alias(\"emails\"))\n",
    "    # nyet\n",
    "    return posts_in_response_to_user_collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailing_list_posts_in_reply_to.cache().count()\n",
    "posts_by_sampled_authors.cache().count()\n",
    "posts_in_response_to_user_collected = relevant_posts_with_replies(\n",
    "    mailing_list_posts_in_reply_to, posts_by_sampled_authors)\n",
    "#posts_in_response_to_user_collected.first()\n",
    "posts_in_response_to_user_collected_saved = non_blocking_df_save_or_load(\n",
    "    posts_in_response_to_user_collected,\n",
    "    \"{0}/posts_by_user_9\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_in_response_to_user_collected_saved.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_in_response_to_user_collected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#posts_in_response_to_user_collected.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_authors_saved.filter(sampled_authors_saved.new_unique_id == \"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a sample for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first10(posts):\n",
    "    return posts[0:10]\n",
    "first10_udf = UserDefinedFunction(\n",
    "    first10,\n",
    "    ArrayType(StringType()),\n",
    "    \"first10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts_for_user = posts_in_response_to_user_collected_saved.select(\n",
    "    posts_in_response_to_user_collected_saved.new_unique_id,\n",
    "    first10_udf(posts_in_response_to_user_collected_saved.emails).alias(\"top10emails\")).alias(\"top_posts_for_user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_sample = sampled_authors_saved.join(\n",
    "    top_posts_for_user,\n",
    "    top_posts_for_user.new_unique_id == sampled_authors_saved.new_unique_id,\n",
    "    \"LEFT_OUTER\").select(\n",
    "    \"project_name\",\n",
    "    F.col(\"sampled_authors.new_unique_id\").alias(\"id\"),\n",
    "    \"Author\",\n",
    "    \"github_username\",\n",
    "    \"top10emails\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_sample_saved = non_blocking_df_save_or_load(\n",
    "    joined_sample,\n",
    "    \"{0}/joined_sample_3\".format(fs_prefix)).alias(\"joined_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_sample_saved.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(StringType())\n",
    "def create_email_snippet(emails):\n",
    "    import os\n",
    "    def create_snippet(email):\n",
    "        if email is None:\n",
    "            return email\n",
    "        #result = email.replace(os.linesep, ' ')\n",
    "        # IDK but seems required for the other system :(\n",
    "        #result = result.replace('\\n', ' ')\n",
    "        #result = result.replace('\\r', ' ')\n",
    "        #result = result.replace(',', ' ')\n",
    "        #result = result.replace(',', ' ')\n",
    "        result = email\n",
    "        import string\n",
    "        printable = set(string.printable)\n",
    "        result = ''.join(filter(lambda x: x in printable, result))\n",
    "        if len(result) > 500:\n",
    "            result = result[0:500] + \"..... e-mail condensed for readability\"\n",
    "        return result\n",
    "    return emails.apply(create_snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.getConf().get(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_sample = joined_sample_saved.select(\n",
    "    \"project_name\",\n",
    "    \"id\",\n",
    "    \"Author\",\n",
    "    \"github_username\",\n",
    "    create_email_snippet(joined_sample_saved.top10emails[0]).alias(\"email0\"),\n",
    "    create_email_snippet(joined_sample_saved.top10emails[1]).alias(\"email1\"),\n",
    "    create_email_snippet(joined_sample_saved.top10emails[2]).alias(\"email2\")).cache().repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_sample_pq_saved = non_blocking_df_save_or_load(\n",
    "    formatted_sample,\n",
    "    \"{0}/formatted_sample_pq_11\".format(fs_prefix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_escape(raw_string):\n",
    "    import html\n",
    "    if raw_string is None:\n",
    "        return raw_string\n",
    "    initial_escape = html.escape(raw_string)\n",
    "    return initial_escape.replace(os.linesep, '<br>').replace('\\n', '').replace('\\r', '')\n",
    "html_escape_udf = UserDefinedFunction(\n",
    "      html_escape,\n",
    "      StringType(),\n",
    "      \"html_escape_udf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "escaped = formatted_sample_pq_saved.select(\n",
    "    list(map(lambda col: html_escape_udf(col).alias(col), formatted_sample_pq_saved.columns)))\n",
    "formatted_sample_csv_saved = non_blocking_df_save_or_load_csv(\n",
    "    escaped,\n",
    "    \"{0}/formatted_sample_csv_14\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = formatted_sample_pq_saved.groupBy(\n",
    "    formatted_sample_pq_saved.project_name).agg(F.first(\"project_name\")).select(\"project_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_csv = non_blocking_df_save_or_load_csv(projects, \"{0}/projects\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_csv.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load back the human processed data & process the columns into formats that are easier with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_human_data(df):\n",
    "    columns = df.columns\n",
    "    candidates_to_select = filter(lambda column: \"Input\" in column or \"Answer\" in column, columns)\n",
    "\n",
    "    def easy_name(column_name):\n",
    "        return column_name.replace(\".\", \"_\")\n",
    "    \n",
    "    rewrite_literals = map(\n",
    "        lambda column_name: F.col(\"`{0}`\".format(column_name)).alias(easy_name(column_name)),\n",
    "        candidates_to_select)\n",
    "    return df.select(*list(rewrite_literals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_human_raw_df = session.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"true\").load(\n",
    "    \"{0}/human_data/projects\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_human_df = non_blocking_df_save_or_load(\n",
    "    rewrite_human_data(project_human_raw_df),\n",
    "    \"{0}/human_data_cleaned/projects\".format(fs_prefix))                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_contirbutors_human_raw_df = session.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"true\").load(\n",
    "    \"{0}/human_data/sampled_contirbutors\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_contirbutors_human_df = non_blocking_df_save_or_load(\n",
    "    rewrite_human_data(sampled_contirbutors_human_raw_df),\n",
    "    \"{0}/human_data_cleaned/sampled_contributors\".format(fs_prefix)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_people_human_raw_df = session.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"true\").load(\n",
    "    \"{0}/human_data/asf_people\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_people_human_df = non_blocking_df_save_or_load(\n",
    "    rewrite_human_data(asf_people_human_raw_df),\n",
    "    \"{0}/human_data_cleaned/asf_people\".format(fs_prefix)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_human_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_maybe_link(col):\n",
    "    if col is None:\n",
    "        return None\n",
    "    cleaned_ish = col.lower()\n",
    "    if cleaned_ish == \"none\" or cleaned_ish == \"na\":\n",
    "        return None\n",
    "    if \"http://\" in cleaned_ish or \"https://\" in cleaned_ish:\n",
    "        return cleaned_ish\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "clean_maybe_link_udf = UserDefinedFunction(\n",
    "    clean_maybe_link, StringType(), \"clean_maybe_link_field\")\n",
    "\n",
    "def clean_difficulty(col):\n",
    "    if col is None:\n",
    "        return None\n",
    "    cleaned_ish = col.lower()\n",
    "    if cleaned_ish == \"none\" or cleaned_ish == \"na\":\n",
    "        return None\n",
    "    return ''.join(cleaned_ish.split(' '))\n",
    "\n",
    "clean_difficulty_udf = UserDefinedFunction(\n",
    "    clean_difficulty, StringType(), \"clean_difficulty_field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_column(f):\n",
    "    if f == \"Input_project\":\n",
    "        return F.col(\"Input_project\").alias(\"project\")\n",
    "    elif \"Input\" in f:\n",
    "        return f\n",
    "    elif \"difficulty\" in f:\n",
    "        return clean_difficulty_udf(f).alias(f)\n",
    "    elif \"Answer_feedback\" in f:\n",
    "        return f\n",
    "    else:\n",
    "        return clean_maybe_link_udf(f).alias(f)\n",
    "project_human_cleaned_df = project_human_df.select(\n",
    "    *list(map(process_column, project_human_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_human_cleaned_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gender_field(column):\n",
    "    if column is None:\n",
    "        return \"na\"\n",
    "    lowered = column.lower()\n",
    "    if \"female\" in lowered or \"woman\" in lowered or \"she\" in lowered or \"her\" in lowered or lowered == \"f\":\n",
    "        return \"female\"\n",
    "    elif \"enby\" in lowered or \"non-binary\" in lowered or \"they\" in lowered:\n",
    "        return \"enby\"\n",
    "    elif lowered == \"m\" or \"male\" in lowered or \"https://www.linkedin.com/in/moonsoo-lee-4982a511/\" in lowered:\n",
    "        return \"male\"\n",
    "    elif \"n/a\" in lowered or \"na\" in lowered:\n",
    "        return \"na\"\n",
    "    else:\n",
    "        return lowered\n",
    "clean_gender_field_udf = UserDefinedFunction(clean_gender_field, StringType(), \"clean_gender_field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_asf_people_human_df = asf_people_human_df.select(\"*\",\n",
    "                           clean_gender_field_udf(\"Answer_gender\").alias(\"cleaned_gender\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_asf_people_human_df_saved = non_blocking_df_save_or_load(\n",
    "    cleaned_asf_people_human_df,\n",
    "    \"{0}/human_data_cleaned/asf_people_cleaned\".format(fs_prefix)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_asf_people_human_df_saved.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sampled_contirbutors_human_df = sampled_contirbutors_human_df.select(\n",
    "    \"*\",\n",
    "    clean_gender_field_udf(\"Answer_gender\").alias(\"cleaned_gender\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sampled_contirbutors_human_df_saved = non_blocking_df_save_or_load(\n",
    "    cleaned_sampled_contirbutors_human_df,\n",
    "    \"{0}/human_data_cleaned/sampled_contirbutors_cleaned\".format(fs_prefix)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_gender(df):\n",
    "    return df.groupBy(df.cleaned_gender).agg(F.count(df.cleaned_gender))\n",
    "def group_by_project_count_gender(df):\n",
    "    by_gender_and_project = df.withColumn(\n",
    "        \"projects_array\",\n",
    "        F.split(df.Input_projects, \" \")).select(\n",
    "        \"*\",\n",
    "        F.explode(\"projects_array\").alias(\"project\")).groupBy(\n",
    "        \"project\").agg(\n",
    "          F.sum((df.cleaned_gender == \"male\").cast(\"long\")).alias(\"male\"),\n",
    "          F.sum((df.cleaned_gender == \"na\").cast(\"long\")).alias(\"unknown\"),\n",
    "          F.sum((df.cleaned_gender == \"enby\").cast(\"long\")).alias(\"enby\"),\n",
    "          F.sum((df.cleaned_gender == \"female\").cast(\"long\")).alias(\"female\"))\n",
    "    pre_result = by_gender_and_project.select(\n",
    "        \"*\",\n",
    "        ((by_gender_and_project.enby + by_gender_and_project.female) /\n",
    "         (by_gender_and_project.male + by_gender_and_project.enby + by_gender_and_project.female)))\n",
    "    result = pre_result.select(\n",
    "    F.col(\"*\"), F.col(\"((enby + female) / ((male + enby) + female))\").alias(\"nonmale_percentage\"))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group_by_project_count_gender(cleaned_asf_people_human_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_agg_by_gender_df = non_blocking_df_save_or_load_csv(\n",
    "    group_by_gender(cleaned_asf_people_human_df).repartition(1),\n",
    "    \"{0}/asf_people_cleaned_agg_by_gender_3c\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_agg_by_gender_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_agg_by_gender_and_proj_df = non_blocking_df_save_or_load_csv(\n",
    "    group_by_project_count_gender(cleaned_asf_people_human_df_saved).repartition(1),\n",
    "    \"{0}/asf_people_cleaned_agg_by_gender_and_proj_3c\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_agg_by_gender_and_proj_df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sampled_contirbutors_human_df_saved.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = cleaned_sampled_contirbutors_human_df_saved \\\n",
    "  .withColumn(\n",
    "    \"Input_projects\",\n",
    "    cleaned_sampled_contirbutors_human_df_saved.Input_project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_contirbutors_human_agg_by_gender_and_proj_df = non_blocking_df_save_or_load_csv(\n",
    "    group_by_project_count_gender(sampled).repartition(1),\n",
    "    \"{0}/sampled_contirbutors_human_agg_by_gender_and_proj_3c\".format(fs_prefix)).alias(\"sampled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_contirbutors_human_agg_by_gender_and_proj_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to infer Gender off of name. This has problems, see https://ironholds.org/names-gender/ for a discussion on why this is problematic, but if it matches our statistical samples from above it can augment our understanding of the data. However without doing this it's difficult to get much of a picture (see above where we attempt to gender from other sources, the hit rate leaves something to be desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_name_info(input_elem):\n",
    "    from nameparser import HumanName\n",
    "    # Kind of a hack but wing seems like a commen name more than a title.\n",
    "    from nameparser.config import CONSTANTS\n",
    "    CONSTANTS.titles.remove('hon')\n",
    "    CONSTANTS.titles.remove('wing')\n",
    "    if \" <\" in input_elem:\n",
    "        name_chunk = input_elem.split(\" <\")[0]\n",
    "    elif \"<\" in input_elem:\n",
    "        name_chunk = input_elem.split(\"<\")[0]\n",
    "    else:\n",
    "        name_chunk = input_elem\n",
    "    if \" \" not in name_chunk and \".\" in name_chunk:\n",
    "        # Handle the convention[ish] of names of first.last\n",
    "        name_chunk = name_chunk.replace(\".\", \" \")\n",
    "    parsed = HumanName(name_chunk)\n",
    "    return {\"title\": parsed.title, \"first\": parsed.first}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_name_info_udf = UserDefinedFunction(\n",
    "    parse_name_info,\n",
    "    StructType([StructField(\"title\", StringType()), StructField(\"first\", StringType())]),\n",
    "    \"parse_name_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_with_name = authors_grouped_by_id_saved.select(\n",
    "    \"*\", parse_name_info_udf(\"Author\").alias(\"parsed_info\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_with_name.select(\"parsed_info.first\", \"parsed_info.title\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_count = authors_with_name.groupBy(\"parsed_info.first\").agg(F.count(\"*\").alias(\"names_count\"))\n",
    "names_count.sort(names_count.names_count.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_with_name.filter(authors_with_name.parsed_info.title != \"\").select(\"parsed_info.first\", \"parsed_info.title\", \"Author\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(StringType())\n",
    "def lookup_gender_from_name(names):\n",
    "    # Uses https://pypi.org/project/gender-guesser/ based on https://autohotkey.com/board/topic/20260-gender-verification-by-forename-cmd-line-tool-db/\n",
    "    import gender_guesser.detector as gender\n",
    "    d = gender.Detector()\n",
    "    def inner_detect_gender(name):\n",
    "        fname = name.split(\" \")[0]\n",
    "        return d.get_gender(fname)\n",
    "    return names.apply(inner_detect_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_gender_from_name_genderize(name):\n",
    "    from genderize import Genderize\n",
    "    result = Genderize(api_key=genderize_key).get([name])[0]\n",
    "    if result['gender'] is not None:\n",
    "        return result\n",
    "    else:\n",
    "        return {\"name\": name, \"gender\": None, \"probability\": None, \"count\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_gender_from_name_genderize_udf = UserDefinedFunction(\n",
    "    lookup_gender_from_name_genderize,\n",
    "    StructType([StructField(\"name\", StringType()), StructField(\"gender\", StringType()),\n",
    "               StructField(\"probability\", DoubleType()), StructField(\"count\", IntegerType())\n",
    "               ]),\n",
    "    \"lookup_gender_from_name_genderize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache to break pipeline and mix UDF types\n",
    "infered_gender_for_authors = authors_with_name.withColumn(\n",
    "    \"infered_gender\",\n",
    "    lookup_gender_from_name(\"parsed_info.first\")) \\\n",
    "    .cache() \\\n",
    "    .withColumn(\n",
    "    \"genderize_results\",\n",
    "    lookup_gender_from_name_genderize_udf(\"parsed_info.first\")).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infered_gender_for_authors_pq_saved = non_blocking_df_save_or_load(\n",
    "    infered_gender_for_authors,\n",
    "    \"{0}/infered_gender_for_authors_pq_3\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infered_gender_for_authors_pq_saved.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infered_relevant_info = infered_gender_for_authors_pq_saved.select(\n",
    "    infered_gender_for_authors_pq_saved.project_name,\n",
    "    infered_gender_for_authors_pq_saved.Author,\n",
    "    infered_gender_for_authors_pq_saved.new_unique_id,\n",
    "    infered_gender_for_authors_pq_saved.latest_commit,\n",
    "    infered_gender_for_authors_pq_saved.parsed_info.title.alias(\"title\"),\n",
    "    infered_gender_for_authors_pq_saved.infered_gender,\n",
    "    infered_gender_for_authors_pq_saved.genderize_results.gender.alias(\"genderize_gender\"),\n",
    "    infered_gender_for_authors_pq_saved.genderize_results.probability.alias(\"genderize_prob\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infered_relevant_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_info.groupBy(relevant_info.genderize_gender).agg(F.count(\"*\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_info.groupBy(relevant_info.infered_gender).agg(F.count(\"*\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_info = infered_relevant_info.withColumn(\n",
    "    \"Input_projects\",\n",
    "    infered_relevant_info.project_name).withColumn(\n",
    "    \"cleaned_gender\",\n",
    "    clean_gender_field_udf(\"genderize_gender\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_info_agg_by_gender_and_proj_df = non_blocking_df_save_or_load_csv(\n",
    "    group_by_project_count_gender(relevant_info).repartition(1),\n",
    "    \"{0}/relevant_info_agg_by_gender_and_proj_3c\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_info_agg_by_gender_and_proj_df.alias(\"infered\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what's correlated - TODO loop this over the different types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_sampled_and_infered = sampled_contirbutors_human_agg_by_gender_and_proj_df.join(\n",
    "    relevant_info_agg_by_gender_and_proj_df,\n",
    "    on=\"project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_with_project_info = sampled_contirbutors_human_agg_by_gender_and_proj_df.join(\n",
    "    project_human_cleaned_df,\n",
    "    on=\"project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_with_project_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def flatmap(f, items):\n",
    "        return chain.from_iterable(map(f, items))\n",
    "\n",
    "def compute_new_columns(df):\n",
    "    columns = df.columns\n",
    "    def compute_numeric_columns(column):\n",
    "        if \"Answer_\" in column and not \"feedback\" in column and not \"_difficulty\" in column:\n",
    "            return [\n",
    "                (~ F.isnull(F.col(\"`{0}`\".format(column)))).cast(\"long\").alias(column+\"_exists\"),\n",
    "                (~ F.isnull(F.col(\"`{0}`\".format(column))) & (F.col(column + \"_difficulty\") == \"easy\")).cast(\"long\").alias(column+\"_easy\")]\n",
    "        return []\n",
    "    my_columns = list(flatmap(compute_numeric_columns, columns))\n",
    "    my_columns.append(\"*\")\n",
    "    return df.select(*my_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = compute_new_columns(joined_with_project_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df.select(\"`nonmale_percentage`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df.agg(\n",
    "    F.corr(\"`nonmale_percentage`\", \"Answer_mentoring_guide_exists\"),\n",
    "    F.corr(\"`nonmale_percentage`\", \"Answer_mentoring_guide_easy\"),\n",
    "    F.corr(\"`nonmale_percentage`\", \"Answer_contributing_guide_exists\"),\n",
    "    F.corr(\"`nonmale_percentage`\", \"Answer_contributing_guide_easy\"),\n",
    "    F.corr(\"`nonmale_percentage`\", \"Answer_committer_guide_exists\"),\n",
    "    F.corr(\"`nonmale_percentage`\", \"Answer_committer_guide_easy\"),\n",
    "    F.corr(\"`nonmale_percentage`\", \"Answer_code_of_conduct_exists\"),\n",
    "    F.corr(\"`nonmale_percentage`\", \"Answer_code_of_conduct_easy\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}