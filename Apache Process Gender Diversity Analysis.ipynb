{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook seeks to explore the gender diversity of the different apache projects & the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import collect_set, explode, from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import json\n",
    "import os\n",
    "import meetup.api\n",
    "from copy import copy\n",
    "import time\n",
    "import logging\n",
    "\n",
    "from watson_developer_cloud import ToneAnalyzerV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "API key configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "meetup_key = os.getenv(\"MEETUP_APIKEY\")\n",
    "tone_bluemix_user = os.getenv(\"TONE_BLUEMIX_USERNAME\")\n",
    "tone_bluemix_password = os.getenv(\"TONE_BLUEMIX_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Less secret configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_meetup_events = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "session = SparkSession.builder.appName(\"whatCanWeLearnFromTheSixties\").getOrCreate()\n",
    "sc = session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The first thing we want to get is the committers and PMC members, this information is stored in LDAP but also available in JSON. Eventually we will want to enrich this with mailing list information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loadFlatJsonFile(path, explodeKey, schema=None):\n",
    "    \"\"\"Load a flat multi-line json file and convert into Spark & explode\"\"\"\n",
    "    rdd = sc.wholeTextFiles(path).values()\n",
    "    df = (session.read.schema(schema)\n",
    "            .json(rdd))\n",
    "    return df.select(explode(explodeKey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "apache_people_schema = StructType([StructField(\"lastCreateTimestamp\", StringType()),\n",
    "                     StructField(\"people\",\n",
    "                                 MapType(StringType(), \n",
    "                                         StructType([StructField('name', StringType()),\n",
    "                                                     StructField('key_fingerprints', ArrayType(StringType())),\n",
    "                                                     StructField('urls', ArrayType(StringType())),\n",
    "                                                    ]))\n",
    "                                )])\n",
    "apache_people_df = loadFlatJsonFile(path=\"http_data_sources/public_ldap_people.json\", # http://people.apache.org/public/public_ldap_people.json\n",
    "                                 explodeKey=\"people\", schema=apache_people_schema)\n",
    "apache_people_df = apache_people_df.select(apache_people_df.key.alias(\"username\"), apache_people_df.value.alias(\"extra\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.LazyPool"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a lazy urllib3 pool - however using this inside of UDFs is hard :(\n",
    "from lazy_helpers import *\n",
    "    \n",
    "bcast_pool = sc.broadcast(LazyPool)\n",
    "bcast_pool.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def project_on_github(project):\n",
    "    \"\"\"Returns if a project is on github\"\"\"\n",
    "    import urllib3\n",
    "    http = \n",
    "    r = http.request('GET', \"https://github.com/apache/{0}\".format(project))\n",
    "    return r.status == 200\n",
    "session.catalog.registerFunction(\"on_github\", project_on_github, BooleanType())\n",
    "# Except I'm a bad person so....\n",
    "from pyspark.sql.catalog import UserDefinedFunction\n",
    "project_on_github_udf = UserDefinedFunction(project_on_github, BooleanType(), \"on_github\")\n",
    "session.catalog._jsparkSession.udf().registerPython(\"on_github\", project_on_github_udf._judf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_committees_schema = StructType([StructField(\"lastCreateTimestamp\", StringType()),\n",
    "                     StructField(\"committees\",\n",
    "                                 MapType(StringType(), StructType([StructField('roster', ArrayType(StringType())),\n",
    "                                                                  StructField('modifyTimestamp', StringType()),\n",
    "                                                                  StructField('createTimestamp', StringType())\n",
    "                                                                  ])))])\n",
    "apache_committees_df = loadFlatJsonFile(path=\"http_data_sources/public_ldap_committees.json\", # http://people.apache.org/public/public_ldap_people.json\n",
    "                                 explodeKey=\"committees\", schema=apache_committees_schema)\n",
    "apache_committees_on_github_df = apache_committees_df.filter(project_on_github_udf(apache_committees_df.key))\n",
    "apache_committees_on_github_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "committee_names_df = apache_committees_on_github_df.select(apache_committees_df.key.alias(\"project\"))\n",
    "committee_names_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "committee_names_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "project_to_user_df = apache_committees_on_github_df.select(\n",
    "    apache_committees_on_github_df.key.alias(\"project\"),\n",
    "    explode(apache_committees_on_github_df.value.roster).alias(\"username\"))\n",
    "\n",
    "\n",
    "user_to_project_df = project_to_user_df.groupBy(project_to_user_df.username).agg(\n",
    "    collect_set(project_to_user_df.project).alias(\"projects\"))\n",
    "apache_people_df = apache_people_df.join(user_to_project_df, on=\"username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(username='a_horuzhenko', extra=Row(name='Artyom Horuzhenko', key_fingerprints=None, urls=None), projects=['openmeetings'])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_people_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Attempt to fetch relevant past & present meetups for each project - idea based on the listing at https://www.apache.org/events/meetups.html but different code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(\"WARN\")\n",
    "def lookup_relevant_meetup(project_name, max_meetup_events=0):\n",
    "    \"\"\"Lookup relevant meetups for a specific project.\"\"\"\n",
    "    import logging\n",
    "    import time\n",
    "    import meetup.api\n",
    "    logger = logging.getLogger()\n",
    "    meetup_delay = 30\n",
    "    meetup_reset_delay = 1800 # 30 minutes\n",
    "    standard_keys = {\"text_format\": \"plain\", \"trending\": \"desc=true\", \"and_text\": \"true\", \"city\": \"san francisco\", \"country\": \"usa\", \"text\": \"apache \" + project_name, \"radius\": 10000}\n",
    "    results = {\"upcoming\": [], \"past\": []}\n",
    "    for status in [\"upcoming\", \"past\"]:\n",
    "        keys = copy(standard_keys)\n",
    "        keys[\"status\"] = status\n",
    "        count = 200\n",
    "        base = 0\n",
    "        while (count == 200 and (max_meetup_events == 0 or base < max_meetup_events)):\n",
    "            logging.debug(\"Fetch {0} meetups for {1} on base {2}\".format(status, project_name, base))\n",
    "            project_name = \"spark\"\n",
    "            client = client = meetup.api.Client(meetup_key)\n",
    "            if base > 0:\n",
    "                keys[\"page\"] = base\n",
    "            # Manually sleep for meetup_reset_delay on failure, the meetup-api package retry logic sometimes breaks :(\n",
    "            response = None\n",
    "            retry_count = 0\n",
    "            while response is None and retry_count < 10:\n",
    "                try:\n",
    "                    response = client.GetOpenEvents(**keys)\n",
    "                except:\n",
    "                    response = None\n",
    "                    retry_count += 1\n",
    "                    time.sleep(meetup_reset_delay)\n",
    "                    try:\n",
    "                        response = client.GetOpenEvents(**keys)\n",
    "                    except:\n",
    "                        response = None\n",
    "            try:\n",
    "                count = response.meta['count']\n",
    "                base = base + count\n",
    "                results[status].append(response.results)\n",
    "                time.sleep(meetup_delay)\n",
    "            except:\n",
    "                count = 0\n",
    "    return (project_name, results)\n",
    "\n",
    "project_meetups_df = committee_names_df.rdd.map(lambda x: x.project).map(lambda name: lookup_relevant_meetup(name, max_meetup_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "project_meetups_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "project_meetups_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the provided projects attempt to lookup their GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lookup_project_git(project):\n",
    "    \"\"\"Returns the project github for a specific project. Assumes project is git hosted\"\"\"\n",
    "    return \"git://git.apache.org/{0}\".format(project)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fetch_project_github_data(project):\n",
    "    from perceval.backends.core.github import GitHub as perceval_github\n",
    "    gh_backend = perceval_github(owner=\"apache\", repository=project)\n",
    "    # The backend return a generator - which is awesome. However since we want to pull this data into Spark \n",
    "    return list(gh_backend.fetch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fetch_project_git_data(project):\n",
    "    project_git = lookup_project_git(project)\n",
    "    from perceval.backends.core.git import Git as perceval_git\n",
    "    git_backend = perceval_git(owner=\"apache\", repository=project)\n",
    "    return list(gh_backend.fetch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lazy_helpers import *\n",
    "\n",
    "lazyDriver = LazyDriver()\n",
    "bcast_driver = sc.broadcast(lazyDriver)\n",
    "\n",
    "def lookup_crunchbase_info(people_and_projects):\n",
    "    \"\"\"Lookup a person a crunch base and see what the gender & company is.\n",
    "    Filter for at least one mention of their projects.\"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    driver = bcast_driver.value.get()\n",
    "    for (username, name, projects) in people_and_projects:\n",
    "        # robots.txt seems to be ok with person for now, double check before re-running this\n",
    "        url = \"https://www.crunchbase.com/person/{0}\".format(name.replace(\" \", \"-\"))\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            text = driver.page_source\n",
    "            lower_text = text.lower()\n",
    "            if any(project.lower() in lower_text for project in projects):\n",
    "                soup = BeautifulSoup(text, \"html.parser\")\n",
    "                stats = soup.findAll(\"div\", { \"class\" : \"info-card-overview-content\"})[0]\n",
    "                # Hacky but I'm lazy\n",
    "                result = {}\n",
    "                result[\"username\"] = username\n",
    "                try:\n",
    "                    m = re.search(\"Gender:\\</dt\\>\\<dd\\>(.+?)\\<\", str(stats))\n",
    "                    result[\"gender\"] = m.group(1)\n",
    "                except:\n",
    "                    # If nothing matches thats ok\n",
    "                    pass\n",
    "                try:\n",
    "                    m = re.search(\"data-name=\\\"(.+?)\\\" data-permalink=\\\"\\/organization\", str(stats))\n",
    "                    result[\"company\"] = m.group(1)\n",
    "                except:\n",
    "                    # No match no foul\n",
    "                    pass\n",
    "                return [result]\n",
    "            else:\n",
    "                return None\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'company': 'IBM', 'gender': 'Female', 'username': 'holden'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = lookup_crunchbase_info([(\"holden\", \"holden karau\", [\"spark\"])])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Augment the committer info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 410, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.4/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 180, in main\n    process()\n  File \"/usr/local/lib/python3.4/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/lib/python3.4/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 285, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.4/dist-packages/pyspark/rdd.py\", line 1338, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-32-05dc9e1b739d>\", line 12, in lookup_crunchbase_info\nValueError: too many values to unpack (expected 3)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1475)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1474)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1474)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1702)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1657)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1646)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2011)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2032)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2051)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.4/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 180, in main\n    process()\n  File \"/usr/local/lib/python3.4/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/lib/python3.4/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 285, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.4/dist-packages/pyspark/rdd.py\", line 1338, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-32-05dc9e1b739d>\", line 12, in lookup_crunchbase_info\nValueError: too many values to unpack (expected 3)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-3d368056d159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# we can't actually use the UDF, instead we convert to an RDD and re-join\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcrunchbase_info_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapache_people_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlookup_crunchbase_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcrunchbase_info_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 410, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.4/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 180, in main\n    process()\n  File \"/usr/local/lib/python3.4/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/lib/python3.4/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 285, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.4/dist-packages/pyspark/rdd.py\", line 1338, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-32-05dc9e1b739d>\", line 12, in lookup_crunchbase_info\nValueError: too many values to unpack (expected 3)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1475)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1474)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1474)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1702)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1657)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1646)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2011)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2032)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2051)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.4/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 180, in main\n    process()\n  File \"/usr/local/lib/python3.4/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/lib/python3.4/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 285, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.4/dist-packages/pyspark/rdd.py\", line 1338, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-32-05dc9e1b739d>\", line 12, in lookup_crunchbase_info\nValueError: too many values to unpack (expected 3)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Note since we use the broadcast variable hack (because launching these drivers is spendy)\n",
    "# we can't actually use the UDF, instead we convert to an RDD and re-join\n",
    "crunchbase_info_rdd = apache_people_df.rdd.map(lambda row: (row.username, row.extra.name, row.projects)).flatMap(lookup_crunchbase_info)\n",
    "crunchbase_info_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "One of the things that is interesting is understanding what the tones of the meetup descriptions & mailing list posts are. We can use https://www.ibm.com/watson/developercloud/tone-analyzer/api/v3/?python#introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lookup_tone(document):\n",
    "    \"\"\"Looks up the tone for a specific document. Returns a json blob.\"\"\"\n",
    "    from watson_developer_cloud import ToneAnalyzerV3\n",
    "    tone_analyzer = ToneAnalyzerV3(\n",
    "        username=tone_bluemix_user,\n",
    "        password=tone_bluemix_password,\n",
    "        version='2016-05-19 ')\n",
    "    return tone_analyzer.tone(text=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "oh_no_you = lookup_tone(\"oh no you didn't girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "oh_no_you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ok its time to find some mailing list info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fetch_mbox_data(project_name):\n",
    "    import tempfile\n",
    "    import shutil\n",
    "    from perceval.backends.core.mbox import MBox as perceval_mbox\n",
    "    mbox_user_url = \"http://mail-archives.apache.org/mod_mbox/{0}-user\".format(project_name)\n",
    "    mbox_dev_url = \"http://mail-archives.apache.org/mod_mbox/{0}-dev\".format(project_name)\n",
    "    def fetch_mbox_from_apache_site(root_url):\n",
    "        \"\"\"Fetches all of the mbox files from a given apache url\"\"\"\n",
    "        # Make a temp directory to hold the mbox files, we will return this directory\n",
    "        tempdir = tempfile.mkdtemp()\n",
    "        \n",
    "        # Fetch the page to parse\n",
    "        pool = bcast_pool.value.get()\n",
    "        result = pool.request('GET', root_url)\n",
    "        \n",
    "        \n",
    "        from bs4 import BeautifulSoup\n",
    "        soup = BeautifulSoup(result.data, \"html.parser\")\n",
    "        mbox_ids = set(map(lambda tag: tag.get('id'), soup.findAll(\"span\", { \"class\" : \"links\"})))\n",
    "        \n",
    "        for mbox_id in mbox_ids:\n",
    "            mbox_url = \"{0}/{1}.mbox\".format(root_url, mbox_id)\n",
    "            filename = \"{0}/{1}.mbox\".format(tempdir, mbox_id)\n",
    "            print(\"fetching {0}\".format(mbox_url))\n",
    "            with pool.request('GET', mbox_url, preload_content=False) as r, open(filename, 'wb') as out_file:       \n",
    "                shutil.copyfileobj(r, out_file)\n",
    "        \n",
    "        return tempdir\n",
    "    def process_mbox_directory(base_url, dir_path):\n",
    "        mbox_backend = perceval_mbox(base_url, dir_path)\n",
    "        return mbox_backend.fetch()\n",
    "\n",
    "    def load_and_process(base_url):\n",
    "        output_dir = fetch_mbox_from_apache_site(base_url)\n",
    "        return list(process_mbox_directory(base_url, output_dir))\n",
    "    \n",
    "    return list(map(load_and_process, [mbox_user_url, mbox_dev_url]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching http://mail-archives.apache.org/mod_mbox/spark-user/201601.mbox\n",
      "fetching http://mail-archives.apache.org/mod_mbox/spark-user/201704.mbox\n",
      "fetching http://mail-archives.apache.org/mod_mbox/spark-dev/201601.mbox\n",
      "fetching http://mail-archives.apache.org/mod_mbox/spark-dev/201704.mbox\n"
     ]
    }
   ],
   "source": [
    "fetched_mbox_data = fetch_mbox_data(\"spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backend_name': 'MBox',\n",
       " 'backend_version': '0.7.3',\n",
       " 'category': 'message',\n",
       " 'data': {'Content-type': 'multipart/alternative;\\n\\tboundary=\"B_3534423661_103550\"',\n",
       "  'Date': 'Thu, 31 Dec 2015 16:20:57 -0800',\n",
       "  'Delivered-To': 'mailing list user@spark.apache.org',\n",
       "  'From': 'Andy Davidson <Andy@SantaCruzIntegration.com>',\n",
       "  'In-Reply-To': '<D2AAA15D.2C933%bang_zippy@yahoo.com>',\n",
       "  'List-Help': '<mailto:user-help@spark.apache.org>',\n",
       "  'List-Id': '<user.spark.apache.org>',\n",
       "  'List-Post': '<mailto:user@spark.apache.org>',\n",
       "  'List-Unsubscribe': '<mailto:user-unsubscribe@spark.apache.org>',\n",
       "  'Mailing-List': 'contact user-help@spark.apache.org; run by ezmlm',\n",
       "  'Message-ID': '<D2AB064B.2C995%bang_zippy@yahoo.com>',\n",
       "  'Mime-version': '1.0',\n",
       "  'Precedence': 'bulk',\n",
       "  'Received': 'from [192.168.1.131] (c-69-181-234-49.hsd1.ca.comcast.net [69.181.234.49])\\n\\t(Authenticated sender: Andy@SantaCruzIntegration.com)\\n\\tby omf01.hostedemail.com (Postfix) with ESMTPA\\n\\tfor <user@spark.apache.org>; Fri,  1 Jan 2016 00:21:01 +0000 (UTC)',\n",
       "  'References': '<D2AAA15D.2C933%bang_zippy@yahoo.com>',\n",
       "  'Return-Path': '<user-return-48848-apmail-spark-user-archive=spark.apache.org@spark.apache.org>',\n",
       "  'Sender': 'Andrew Davidson <bang_zippy@yahoo.com>',\n",
       "  'Subject': 'does HashingTF maintain a inverse index?',\n",
       "  'Thread-Topic': 'does HashingTF maintain a inverse index?',\n",
       "  'To': '\"user @spark\" <user@spark.apache.org>',\n",
       "  'User-Agent': 'Microsoft-MacOutlook/14.5.9.151119',\n",
       "  'X-Filterd-Recvd-Size': '2297',\n",
       "  'X-HE-Tag': 'knee66_603d7b2d22529',\n",
       "  'X-Original-To': 'apmail-spark-user-archive@minotaur.apache.org',\n",
       "  'X-Session-Marker': '416E64794053616E74614372757A496E746567726174696F6E2E636F6D',\n",
       "  'X-Spam-Flag': 'NO',\n",
       "  'X-Spam-Level': '******',\n",
       "  'X-Spam-Score': '6.224',\n",
       "  'X-Spam-Status': 'No, score=6.224 tagged_above=-999 required=6.31\\n\\ttests=[FORGED_MSGID_YAHOO=2.244, HTML_MESSAGE=3,\\n\\tKAM_LAZY_DOMAIN_SECURITY=1, RCVD_IN_MSPIKE_H3=-0.01,\\n\\tRCVD_IN_MSPIKE_WL=-0.01] autolearn=disabled',\n",
       "  'X-Spam-Summary': '2,0,0,,d41d8cd98f00b204,andy@santacruzintegration.com,:,RULES_HIT:41:355:379:962:988:989:1189:1221:1260:1261:1313:1314:1345:1359:1381:1437:1516:1517:1518:1534:1539:1567:1575:1588:1589:1594:1711:1714:1730:1749:1777:1792:2559:2562:3138:3139:3140:3141:3142:3690:3865:3866:3867:3868:3871:4184:4361:5007:7652:7903:8603:10004:10400:10848:11658:11914:12043:12517:12519:12663:13018:13019:13071:13139:13200:13229:13618:14659:21060:21080:30026:30051:30054:30090,0,RBL:none,CacheIP:none,Bayesian:0.5,0.5,0.5,Netcheck:none,DomainCache:0,MSF:not bulk,SPF:fn,MSBL:0,DNSBL:none,Custom_rules:0:0:0,LFtime:1,LUA_SUMMARY:none',\n",
       "  'X-Virus-Scanned': 'Debian amavisd-new at spamd1-us-west.apache.org',\n",
       "  'body': {'html': '<html><head></head><body style=\"word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; color: rgb(0, 0, 0); font-size: 14px; font-family: Courier, sans-serif;\"><div><div>Hi&nbsp;</div><div><br></div><div>I am working on proof of concept. I am trying to use spark to classify some documents. I am using tokenizer and hashingTF to convert the documents into vectors. Is there any easy way to map feature back to words or do I need to maintain the reverse index my self? I realize there is a chance some words map to same buck</div><div><br></div><div>Kind regards</div><div><br></div><div>Andy</div></div><div><br></div></body></html>\\n',\n",
       "   'plain': 'Hi \\n\\nI am working on proof of concept. I am trying to use spark to classify some\\ndocuments. I am using tokenizer and hashingTF to convert the documents into\\nvectors. Is there any easy way to map feature back to words or do I need to\\nmaintain the reverse index my self? I realize there is a chance some words\\nmap to same buck\\n\\nKind regards\\n\\nAndy\\n\\n\\n\\n'},\n",
       "  'unixfrom': 'user-return-48848-apmail-spark-user-archive=spark.apache.org@spark.apache.org  Fri Jan  1 00:21:21 2016'},\n",
       " 'origin': 'http://mail-archives.apache.org/mod_mbox/spark-user',\n",
       " 'perceval_version': '0.7.0.dev2',\n",
       " 'tag': 'http://mail-archives.apache.org/mod_mbox/spark-user',\n",
       " 'timestamp': 1492236310.847561,\n",
       " 'updated_on': 1451607657.0,\n",
       " 'uuid': 'e220af8fdd2b32447e9a0a3d12013a3d4ef5a2d4'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetched_mbox_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
