{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook seeks to explore the gender diversity of the different apache projects & the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import concat, collect_set, explode, from_json, format_string\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import json\n",
    "import os\n",
    "import meetup.api\n",
    "from copy import copy\n",
    "import time\n",
    "import logging\n",
    "\n",
    "from watson_developer_cloud import ToneAnalyzerV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "API key configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "meetup_key = os.getenv(\"MEETUP_APIKEY\")\n",
    "tone_bluemix_user = os.getenv(\"TONE_BLUEMIX_USERNAME\")\n",
    "tone_bluemix_password = os.getenv(\"TONE_BLUEMIX_PASSWORD\")\n",
    "gh_api_token = os.getenv(\"GITHUB_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Less secret configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_meetup_events = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "session = SparkSession.builder.appName(\"whatCanWeLearnFromTheSixties\").getOrCreate()\n",
    "sc = session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The first thing we want to get is the committers and PMC members, this information is stored in LDAP but also available in JSON. Eventually we will want to enrich this with mailing list information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loadFlatJsonFile(path, explodeKey, schema=None):\n",
    "    \"\"\"Load a flat multi-line json file and convert into Spark & explode\"\"\"\n",
    "    rdd = sc.wholeTextFiles(path).values().setName(\"Input file {}\".format(path))\n",
    "    df = (session.read.schema(schema)\n",
    "            .json(rdd))\n",
    "    return df.select(explode(explodeKey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[username: string, extra: struct<name:string,key_fingerprints:array<string>,urls:array<string>>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_people_schema = StructType([StructField(\"lastCreateTimestamp\", StringType()),\n",
    "                     StructField(\"people\",\n",
    "                                 MapType(StringType(), \n",
    "                                         StructType([StructField('name', StringType()),\n",
    "                                                     StructField('key_fingerprints', ArrayType(StringType())),\n",
    "                                                     StructField('urls', ArrayType(StringType())),\n",
    "                                                    ]))\n",
    "                                )])\n",
    "apache_people_df = loadFlatJsonFile(path=\"http_data_sources/public_ldap_people.json\", # http://people.apache.org/public/public_ldap_people.json\n",
    "                                 explodeKey=\"people\", schema=apache_people_schema)\n",
    "apache_people_df = apache_people_df.select(apache_people_df.key.alias(\"username\"), apache_people_df.value.alias(\"extra\")).repartition(100).persist().alias(\"apache_people\")\n",
    "apache_people_df.alias(\"Apache Committers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lazy_helpers.LazyPool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a lazy urllib3 pool\n",
    "from lazy_helpers import *\n",
    "    \n",
    "bcast_pool = sc.broadcast(LazyPool)\n",
    "bcast_pool.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def project_on_github(project):\n",
    "    \"\"\"Returns if a project is on github\"\"\"\n",
    "    import urllib3\n",
    "    http = bcast_pool.value.get()\n",
    "    r = http.request('GET', \"https://github.com/apache/{0}\".format(project))\n",
    "    return r.status == 200\n",
    "session.catalog.registerFunction(\"on_github\", project_on_github, BooleanType())\n",
    "# Except I'm a bad person so....\n",
    "from pyspark.sql.catalog import UserDefinedFunction\n",
    "project_on_github_udf = UserDefinedFunction(project_on_github, BooleanType(), \"on_github\")\n",
    "session.catalog._jsparkSession.udf().registerPython(\"on_github\", project_on_github_udf._judf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_committees_schema = StructType([StructField(\"lastCreateTimestamp\", StringType()),\n",
    "                     StructField(\"committees\",\n",
    "                                 MapType(StringType(), StructType([StructField('roster', ArrayType(StringType())),\n",
    "                                                                  StructField('modifyTimestamp', StringType()),\n",
    "                                                                  StructField('createTimestamp', StringType())\n",
    "                                                                  ])))])\n",
    "apache_committees_df = loadFlatJsonFile(path=\"http_data_sources/public_ldap_committees.json\", # http://people.apache.org/public/public_ldap_people.json\n",
    "                                 explodeKey=\"committees\", schema=apache_committees_schema)\n",
    "apache_committees_on_github_df = apache_committees_df.filter(project_on_github_udf(apache_committees_df.key))\n",
    "apache_committees_on_github_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "committee_names_df = apache_committees_on_github_df.select(apache_committees_df.key.alias(\"project\")).alias(\"apache_committees\").repartition(200)\n",
    "committee_names_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "committee_names_df.alias(\"Apache Committee Names\")\n",
    "committee_names_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[username: string, extra: struct<name:string,key_fingerprints:array<string>,urls:array<string>>, projects: array<string>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_to_user_df = apache_committees_on_github_df.select(\n",
    "    apache_committees_on_github_df.key.alias(\"project\"),\n",
    "    explode(apache_committees_on_github_df.value.roster).alias(\"username\"))\n",
    "\n",
    "\n",
    "user_to_project_df = project_to_user_df.groupBy(project_to_user_df.username).agg(\n",
    "    collect_set(project_to_user_df.project).alias(\"projects\"))\n",
    "apache_people_df = apache_people_df.join(user_to_project_df, on=\"username\")\n",
    "apache_people_df.alias(\"Apache People joined with projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(username='aco', extra=Row(name='Adrian T. Co', key_fingerprints=None, urls=None), projects=['servicemix'])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_people_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Attempt to fetch relevant past & present meetups for each project - idea based on the listing at https://www.apache.org/events/meetups.html but different code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "We want to do a non-blocking count to materialize the meetup RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Some async helpers, in Scala we would use AsyncRDDActions but its not currently available in Python\n",
    "# Support is being considered in https://issues.apache.org/jira/browse/SPARK-20347\n",
    "def non_blocking_rdd_count(rdd):\n",
    "    import threading\n",
    "    def count_magic():\n",
    "        rdd.count()\n",
    "    thread = threading.Thread(target=count_magic)\n",
    "    thread.start()\n",
    "\n",
    "def non_blocking_rdd_save(rdd, target):\n",
    "    import threading\n",
    "    def save_panda():\n",
    "        rdd.saveAsPickleFile(target)\n",
    "    thread = threading.Thread(target=save_panda)\n",
    "    thread.start()\n",
    "\n",
    "def non_blocking_df_save(df, target):\n",
    "    import threading\n",
    "    def save_panda():\n",
    "        df.write.save(target)\n",
    "    thread = threading.Thread(target=save_panda)\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Meetup Data RDD PythonRDD[49] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(\"WARN\")\n",
    "def lookup_relevant_meetup(project_name, max_meetup_events=0):\n",
    "    \"\"\"Lookup relevant meetups for a specific project.\"\"\"\n",
    "    import logging\n",
    "    import time\n",
    "    import meetup.api\n",
    "    logger = logging.getLogger()\n",
    "    meetup_delay = 30\n",
    "    meetup_reset_delay = 3600 # 1 hour\n",
    "    standard_keys = {\"text_format\": \"plain\", \"trending\": \"desc=true\", \"and_text\": \"true\", \"city\": \"san francisco\", \"country\": \"usa\", \"text\": \"apache \" + project_name, \"radius\": 10000}\n",
    "    results = {\"upcoming\": [], \"past\": []}\n",
    "    for status in [\"upcoming\", \"past\"]:\n",
    "        keys = copy(standard_keys)\n",
    "        keys[\"status\"] = status\n",
    "        count = 200\n",
    "        base = 0\n",
    "        while (count == 200 and (max_meetup_events == 0 or base < max_meetup_events)):\n",
    "            logging.debug(\"Fetch {0} meetups for {1} on base {2}\".format(status, project_name, base))\n",
    "            project_name = \"spark\"\n",
    "            client = client = meetup.api.Client(meetup_key)\n",
    "            if base > 0:\n",
    "                keys[\"page\"] = base\n",
    "            # Manually sleep for meetup_reset_delay on failure, the meetup-api package retry logic sometimes breaks :(\n",
    "            response = None\n",
    "            retry_count = 0\n",
    "            while response is None and retry_count < 10:\n",
    "                try:\n",
    "                    response = client.GetOpenEvents(**keys)\n",
    "                except:\n",
    "                    response = None\n",
    "                    retry_count += 1\n",
    "                    time.sleep(meetup_reset_delay)\n",
    "                    try:\n",
    "                        response = client.GetOpenEvents(**keys)\n",
    "                    except:\n",
    "                        response = None\n",
    "            try:\n",
    "                count = response.meta['count']\n",
    "                base = base + count\n",
    "                results[status].append(response.results)\n",
    "                time.sleep(meetup_delay)\n",
    "            except:\n",
    "                count = 0\n",
    "    return (project_name, results)\n",
    "\n",
    "project_meetups_rdd = committee_names_df.repartition(500).rdd.map(lambda x: x.project).map(lambda name: lookup_relevant_meetup(name, max_meetup_events))\n",
    "project_meetups_rdd.setName(\"Meetup Data RDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: map<string,array<array<map<string,bigint>>>>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_meetups_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "project_meetups_df = project_meetups_rdd.toDF() \n",
    "project_meetups_df.alias(\"Project -> meetup dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "non_blocking_df_save(project_meetups_df, \"mini_meetup_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1910.showString.\n: org.apache.spark.SparkException: Job 93 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1427)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1674)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1657)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1646)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2011)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2032)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2051)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2806)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2112)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2112)\n\tat org.apache.spark.sql.Dataset$$anonfun$57.apply(Dataset.scala:2790)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2789)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2112)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2325)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:251)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-9f72d6a44e1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproject_meetups_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1910.showString.\n: org.apache.spark.SparkException: Job 93 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1427)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1674)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1657)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1646)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2011)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2032)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2051)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2806)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2112)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2112)\n\tat org.apache.spark.sql.Dataset$$anonfun$57.apply(Dataset.scala:2790)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2789)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2112)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2325)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:251)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "project_meetups_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "project_meetups_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the provided projects attempt to lookup their GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lookup_project_git(project):\n",
    "    \"\"\"Returns the project github for a specific project. Assumes project is git hosted\"\"\"\n",
    "    return \"https://github.com/apache/{0}.git\".format(project)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fetch_project_github_data(project):\n",
    "    \"\"\"Fetch the project github data, note this only gets github issues so likely not super useful\"\"\"\n",
    "    from perceval.backends.core.github import GitHub as perceval_github\n",
    "    gh_backend = perceval_github(owner=\"apache\", repository=project, api_token=gh_api_token)\n",
    "    # The backend return a generator - which is awesome. However since we want to pull this data into Spark \n",
    "    def append_project_info(result):\n",
    "        \"\"\"Add the project information to the return from perceval\"\"\"\n",
    "        result[\"project_name\"] = project\n",
    "        return result\n",
    "\n",
    "    return list(map(append_project_info, gh_backend.fetch()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fetch_project_git_data(project):\n",
    "    project_git = lookup_project_git(project)\n",
    "    from perceval.backends.core.git import Git as perceval_git\n",
    "\n",
    "    git_uri = lookup_project_git(project)\n",
    "    import tempfile\n",
    "    import shutil\n",
    "    tempdir = tempfile.mkdtemp()\n",
    "\n",
    "    def append_project_info(result):\n",
    "        \"\"\"Add the project information to the return from perceval\"\"\"\n",
    "        result[\"project_name\"] = project\n",
    "        return result\n",
    "\n",
    "    try:\n",
    "        git_backend = perceval_git(uri=git_uri, gitpath=tempdir + \"/repo\")\n",
    "        return list(map(append_project_info, git_backend.fetch()))\n",
    "    finally:\n",
    "        shutil.rmtree(tempdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fetch the git history info using perceval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceival GIT dat PythonRDD[66] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_project_data_rdd = committee_names_df.repartition(400).rdd.flatMap(lambda row: fetch_project_git_data(row.project))\n",
    "git_project_data_rdd.persist()\n",
    "git_project_data_rdd.setName(\"Perceival GIT dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mini_list = git_project_data_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'backend_name': 'Git',\n",
       "  'backend_version': '0.7.3',\n",
       "  'category': 'commit',\n",
       "  'data': {'Author': 'James. M. Snell <jmsnell@apache.org>',\n",
       "   'AuthorDate': 'Fri Jun 16 20:05:13 2006 +0000',\n",
       "   'Commit': 'James. M. Snell <jmsnell@apache.org>',\n",
       "   'CommitDate': 'Fri Jun 16 20:05:13 2006 +0000',\n",
       "   'commit': '7669e92657eaef2b73e8f0e97de9171ee123e41e',\n",
       "   'files': [],\n",
       "   'message': 'git-svn-id: https://svn.apache.org/repos/asf/incubator/abdera/java/trunk@414892 13f79535-47bb-0310-9956-ffa450edef68',\n",
       "   'parents': [],\n",
       "   'refs': []},\n",
       "  'origin': 'https://github.com/apache/abdera.git',\n",
       "  'perceval_version': '0.7.0.dev2',\n",
       "  'tag': 'https://github.com/apache/abdera.git',\n",
       "  'timestamp': 1492339308.556235,\n",
       "  'updated_on': 1150488313.0,\n",
       "  'uuid': '72458588d4a1c804e17455ed58304b05c2f2c3ff'}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lookup info from crunchbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lazy_helpers import *\n",
    "\n",
    "bcast_driver = sc.broadcast(LazyDriver)\n",
    "\n",
    "def lookup_crunchbase_info(people_and_projects):\n",
    "    \"\"\"Lookup a person a crunch base and see what the gender & company is.\n",
    "    Filter for at least one mention of their projects.\"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    driver = bcast_driver.value.get()\n",
    "    import time\n",
    "    import random\n",
    "    for (username, name, projects, urls) in people_and_projects:\n",
    "        time.sleep(random.randint(9, 18))\n",
    "        # robots.txt seems to be ok with person for now, double check before re-running this\n",
    "        url = \"https://www.crunchbase.com/person/{0}\".format(name.replace(\" \", \"-\"))\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            text = driver.page_source\n",
    "            lower_text = text.lower()\n",
    "            if any(project.lower() in lower_text for project in projects) or any(url.lower in lower_text for url in urls):\n",
    "                soup = BeautifulSoup(text, \"html.parser\")\n",
    "                stats = soup.findAll(\"div\", { \"class\" : \"info-card-overview-content\"})[0]\n",
    "                # Hacky but I'm lazy\n",
    "                result = {}\n",
    "                result[\"username\"] = username\n",
    "                try:\n",
    "                    m = re.search(\"Gender:\\</dt\\>\\<dd\\>(.+?)\\<\", str(stats))\n",
    "                    result[\"gender\"] = m.group(1)\n",
    "                except:\n",
    "                    # If nothing matches thats ok\n",
    "                    pass\n",
    "                try:\n",
    "                    m = re.search(\"data-name=\\\"(.+?)\\\" data-permalink=\\\"\\/organization\", str(stats))\n",
    "                    result[\"company\"] = m.group(1)\n",
    "                except:\n",
    "                    # No match no foul\n",
    "                    pass\n",
    "                yield result\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'company': 'IBM', 'gender': 'Female', 'username': 'holden'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = lookup_crunchbase_info([(\"holden\", \"holden karau\", [\"spark\"], [\"http://www.holdenkarau.com\"])])\n",
    "list(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Augment the committer info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We do this as an RDD transformation since the cost of the transformation dominates\n",
    "relevant_info = apache_people_df.select(\n",
    "    apache_people_df.username,\n",
    "    apache_people_df.extra.getField(\"name\").alias(\"name\"),\n",
    "    apache_people_df.projects,\n",
    "    apache_people_df.extra.getField(\"urls\").alias(\"urls\"))\n",
    "crunchbase_info_rdd = relevant_info.rdd.map(lambda row: (row.username, row.name, row.projects, row.urls)).mapPartitions(lookup_crunchbase_info)\n",
    "crunchbase_info_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "schema = StructType([\n",
    "    StructField(\"username\", StringType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"company\", StringType())])\n",
    "crunchbase_info_df = crunchbase_info_rdd.toDF(schema = schema)\n",
    "crunchbase_info_df.alias(\"Crunchbase user information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "non_blocking_df_save(crunchbase_info_df, \"crunchbase_out_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crunchbase_info_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2164"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_people_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Export to Mechnical turk format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def mini_concat_udf(array_strs):\n",
    "    \"\"\"Concat the array of strs\"\"\"\n",
    "    if array_strs == None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return ' '.join(array_strs)\n",
    "\n",
    "# Except I'm a bad person so....\n",
    "from pyspark.sql.catalog import UserDefinedFunction\n",
    "mini_concat_udf = UserDefinedFunction(mini_concat_udf, StringType(), \"mini_concat_udf\")\n",
    "session.catalog._jsparkSession.udf().registerPython(\"mini_concat_udf\", mini_concat_udf._judf)\n",
    "\n",
    "apache_people_df.select(\n",
    "    apache_people_df.username,\n",
    "    apache_people_df.extra.getField(\"name\").alias(\"name\"),\n",
    "    mini_concat_udf(apache_people_df.extra.getField(\"urls\")).alias(\"personal_websites\"),\n",
    "    mini_concat_udf(apache_people_df.projects).alias(\"projects\")\n",
    "    ).coalesce(1).write.csv(\"./apache_people.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "crunchbase_info_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "One of the things that is interesting is understanding what the tones of the meetup descriptions & mailing list posts are. We can use https://www.ibm.com/watson/developercloud/tone-analyzer/api/v3/?python#introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lookup_tone(document):\n",
    "    \"\"\"Looks up the tone for a specific document. Returns a json blob.\"\"\"\n",
    "    from watson_developer_cloud import ToneAnalyzerV3\n",
    "    tone_analyzer = ToneAnalyzerV3(\n",
    "        username=tone_bluemix_user,\n",
    "        password=tone_bluemix_password,\n",
    "        version='2016-05-19 ')\n",
    "    return tone_analyzer.tone(text=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "oh_no_you = lookup_tone(\"oh no you didn't girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "oh_no_you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ok its time to find some mailing list info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mbox_failures = sc.accumulator(0)\n",
    "\n",
    "def fetch_mbox_ids(project_name):\n",
    "    \"\"\"Return the mbox ids\"\"\"\n",
    "    import itertools\n",
    "\n",
    "    def fetch_mbox_ids_apache_site(box_type):\n",
    "        \"\"\"Fetches all of the mbox ids from a given apache project and box type (dev or user)\"\"\"\n",
    "        root_url = \"http://mail-archives.apache.org/mod_mbox/{0}-{1}\".format(project_name, box_type)\n",
    "        \n",
    "        # Fetch the page to parse\n",
    "        pool = bcast_pool.value.get()\n",
    "        result = pool.request('GET', root_url)\n",
    "        \n",
    "        \n",
    "        from bs4 import BeautifulSoup\n",
    "        soup = BeautifulSoup(result.data, \"html.parser\")\n",
    "        mbox_ids = set(map(lambda tag: tag.get('id'), soup.findAll(\"span\", { \"class\" : \"links\"})))\n",
    "        return map(lambda box_id: (project_name, box_type, box_id), mbox_ids)\n",
    "    # We have to return a list here because PySpark doesn't handle generators (TODO: holden)\n",
    "    return list(itertools.chain.from_iterable(map(fetch_mbox_ids_apache_site, [\"dev\", \"user\"])))\n",
    "        \n",
    "        \n",
    "def fetch_and_process_mbox_records(project_name, box_type, mbox_id):\n",
    "        import tempfile\n",
    "        import shutil\n",
    "        from perceval.backends.core.mbox import MBox as perceval_mbox\n",
    "\n",
    "        def process_mbox_directory(base_url, dir_path):\n",
    "            mbox_backend = perceval_mbox(base_url, dir_path)\n",
    "            return mbox_backend.fetch()\n",
    "        \n",
    "        def append_project_info(result):\n",
    "            \"\"\"Add the project information to the return from perceval\"\"\"\n",
    "            result[\"project_name\"] = project_name\n",
    "            result[\"box_type\"] = box_type\n",
    "            result[\"mbox_id\"] = mbox_id\n",
    "            return result\n",
    "\n",
    "        # Make a temp directory to hold the mbox files\n",
    "        tempdir = tempfile.mkdtemp()\n",
    "\n",
    "        try:\n",
    "            root_url = \"http://mail-archives.apache.org/mod_mbox/{0}-{1}\".format(project_name, box_type)\n",
    "            mbox_url = \"{0}/{1}.mbox\".format(root_url, mbox_id)\n",
    "            filename = \"{0}/{1}.mbox\".format(tempdir, mbox_id)\n",
    "        \n",
    "            print(\"fetching {0}\".format(mbox_url))\n",
    "\n",
    "            pool = bcast_pool.value.get()\n",
    "            with pool.request('GET', mbox_url, preload_content=False) as r, open(filename, 'wb') as out_file:       \n",
    "                try:\n",
    "                    shutil.copyfileobj(r, out_file)\n",
    "                    return list(map(append_project_info, process_mbox_directory(root_url, tempdir)))\n",
    "                except:\n",
    "                    mbox_failures.add(1)\n",
    "                    return []\n",
    "        finally:\n",
    "            shutil.rmtree(tempdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching http://mail-archives.apache.org/mod_mbox/spark-dev/201308.mbox\n"
     ]
    }
   ],
   "source": [
    "fetched_mbox_ids = fetch_mbox_ids(\"spark\")\n",
    "list(fetched_mbox_ids)[0]\n",
    "fetched_mbox_data = fetch_and_process_mbox_records('spark', 'dev', '201308')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backend_name': 'MBox',\n",
       " 'backend_version': '0.7.3',\n",
       " 'box_type': 'dev',\n",
       " 'category': 'message',\n",
       " 'data': {'Content-Disposition': 'inline',\n",
       "  'Content-Type': 'text/plain; charset=us-ascii',\n",
       "  'Date': 'Thu, 1 Aug 2013 19:00:52 +0000',\n",
       "  'Delivered-To': 'moderator for dev@spark.incubator.apache.org',\n",
       "  'From': 'Mike <spark@good-with-numbers.com>',\n",
       "  'In-Reply-To': '<48E7FDAD-E0F7-4317-BBAD-2AD45188C0CF@gmail.com>',\n",
       "  'List-Help': '<mailto:dev-help@spark.incubator.apache.org>',\n",
       "  'List-Id': '<dev.spark.incubator.apache.org>',\n",
       "  'List-Post': '<mailto:dev@spark.incubator.apache.org>',\n",
       "  'List-Unsubscribe': '<mailto:dev-unsubscribe@spark.incubator.apache.org>',\n",
       "  'MIME-Version': '1.0',\n",
       "  'Mailing-List': 'contact dev-help@spark.incubator.apache.org; run by ezmlm',\n",
       "  'Message-ID': '<20130801190052.GA30564@64-142-29-25.dsl.static.sonic.net>',\n",
       "  'Precedence': 'bulk',\n",
       "  'Received': '(qmail 42870 invoked by uid 99); 1 Aug 2013 19:01:19 -0000',\n",
       "  'Received-SPF': 'pass (athena.apache.org: local policy)',\n",
       "  'Reply-To': 'dev@spark.incubator.apache.org',\n",
       "  'Return-Path': '<dev-return-121-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>',\n",
       "  'Subject': 'Re: Saying hello and helping out',\n",
       "  'To': 'dev@spark.incubator.apache.org',\n",
       "  'X-ASF-Spam-Status': 'No, hits=-0.0 required=5.0\\n\\ttests=SPF_PASS',\n",
       "  'X-Original-To': 'apmail-spark-dev-archive@minotaur.apache.org',\n",
       "  'X-Spam-Check-By': 'apache.org',\n",
       "  'X-Virus-Checked': 'Checked by ClamAV on apache.org',\n",
       "  'body': {'plain': \"Matei,\\n\\nOn Mon, Jul 29, 2013 at 04:17:49PM -0700, Matei Zaharia wrote:\\n> if you prefer to work on the Java VM, there are a bunch of internal \\n> things to do there too -- I can give an overview of what I'd consider \\n> easy to jump into there.\\n\\nI'd be interesting in hearing about this part.\\n\\n--\\nMike\\n\"},\n",
       "  'unixfrom': 'dev-return-121-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Aug  1 19:03:23 2013'},\n",
       " 'mbox_id': '201308',\n",
       " 'origin': 'http://mail-archives.apache.org/mod_mbox/spark-dev',\n",
       " 'perceval_version': '0.7.0.dev2',\n",
       " 'project_name': 'spark',\n",
       " 'tag': 'http://mail-archives.apache.org/mod_mbox/spark-dev',\n",
       " 'timestamp': 1492332622.542332,\n",
       " 'updated_on': 1375383652.0,\n",
       " 'uuid': '82e0f3bdbbce4324ac365bc564d1221947eb5405'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetched_mbox_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[238] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_key(x):\n",
    "    import random\n",
    "    return (random.randint(0, 40000), x)\n",
    "\n",
    "def de_key(x):\n",
    "    return x[1]\n",
    "\n",
    "mailing_list_posts_mbox_ids = committee_names_df.repartition(400).rdd.flatMap(lambda row: fetch_mbox_ids(row.project))\n",
    "# mbox's can be big, so break up how many partitions we have\n",
    "mailing_list_posts_mbox_ids = mailing_list_posts_mbox_ids.map(random_key).repartition(2000).map(de_key)\n",
    "mailing_list_posts_rdd = mailing_list_posts_mbox_ids.flatMap(lambda args: fetch_and_process_mbox_records(*args))\n",
    "mailing_list_posts_rdd.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"project_name\",StringType()),\n",
    "    StructField(\"box_type\",StringType()), # dev or user\n",
    "    StructField(\"mbox_id\",StringType()),\n",
    "    StructField(\"backend_name\",StringType()),\n",
    "    StructField(\"backend_version\",StringType()),\n",
    "    StructField(\"category\",StringType()),\n",
    "    StructField(\"data\", MapType(StringType(),StringType())), # The \"important\" bits\n",
    "    StructField(\"origin\",StringType()),\n",
    "    StructField(\"perceval_version\",StringType()),\n",
    "    StructField(\"tag\",StringType()),\n",
    "    StructField(\"timestamp\",DoubleType()),\n",
    "    StructField(\"updated_on\",DoubleType()),\n",
    "    StructField(\"uuid\",StringType())])\n",
    "mailing_list_posts_mbox_df_raw = mailing_list_posts_rdd.toDF(schema=schema)\n",
    "mailing_list_posts_mbox_df_raw.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "mailing_list_posts_mbox_df_raw.alias(\"Mailing list perceival information - no post processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "non_blocking_df_save(mailing_list_posts_mbox_df_raw, \"mailing_list_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mailing_list_posts_mbox_df = mailing_list_posts_mbox_df_raw.select(\"*\",\n",
    "                                                               mailing_list_posts_mbox_df_raw.data.getField(\"From\").alias(\"from\"),\n",
    "                                                               mailing_list_posts_mbox_df_raw.data.getField(\"body\").alias(\"body\")\n",
    "                                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to infer Gender off of name. This should be used as a last-ditch fall back, see https://ironholds.org/names-gender/ for a discussion on why this is problematic. However without doing this it's difficult to get much of a picture (see above where we attempt to gender from other sources, the hit rate leaves something to be desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
