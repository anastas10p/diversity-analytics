{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook seeks to explore the gender diversity of the different apache projects & the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "blockmgr-1f117d91-c674-45a3-b47c-ca7284bcc685\n",
      "blockmgr-6ed4b2da-f064-417b-b9d1-0778183fb623\n",
      "spark-6207646a-f0a7-42ce-8f11-3babed4e79aa\n",
      "spark-b9a4f48d-cf45-4c59-a802-97f0003ebd85\n",
      "/\n",
      "--2018-07-13 00:34:33--  https://raw.githubusercontent.com/holdenk/diversity-analytics/master/lazy_helpers.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1397 (1.4K) [text/plain]\n",
      "Saving to: ‘lazy_helpers.py’\n",
      "\n",
      "lazy_helpers.py     100%[=====================>]   1.36K  --.-KB/s   in 0s     \n",
      "\n",
      "2018-07-13 00:34:33 (40.3 MB/s) - ‘lazy_helpers.py’ saved [1397/1397]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls /hadoop/spark/tmp/\n",
    "!cd /hadoop/spark/tmp/spark-*/userFiles-*/\n",
    "!pwd\n",
    "!rm lazy_helpers.py\n",
    "!wget https://raw.githubusercontent.com/holdenk/diversity-analytics/master/lazy_helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Already up-to-date.\\ncommit 621a5736855a69a9684b2c926f778c2da3a61c95\\nAuthor: Holden Karau <holden@pigscanfly.ca>\\nDate:   Thu Jul 12 17:11:53 2018 -0700\\n\\n    Works in Python 2 and 3 now\\n\\ncommit 91ff6b839c1bedf6b376e496386c63e35e85a330\\nAuthor: Holden Karau <holden@pigscanfly.ca>\\nDate:   Thu Jul 12 17:09:38 2018 -0700\\n\\n    Working in Python 3\\n\\ncommit 358458dfa152b8255d703a0e91b4f39b6012f241\\nAuthor: Holden Karau <holden@pigscanfly.ca>\\nDate:   Thu Jul 12 16:51:16 2018 -0700\\n\\n    Put pandas back in with a noqa skip for flake8 since we use it in the doc tests.\\n\\ncommit 388118867fb818459184b67a07203e41111ad956\\nAuthor: Holden Karau <holden@pigscanfly.ca>\\nDate:   Thu Jul 12 16:26:03 2018 -0700\\n\\n    Remove home/build from the cache\\n\\ncommit 81ea25aa31173738d35d3a4e98f9648042c3ac7c\\nAuthor: Holden Karau <holden@pigscanfly.ca>\\nDate:   Thu Jul 12 16:22:52 2018 -0700\\n\\n    Fix flake8s\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hack to update sparklingml on a running cluster\n",
    "memory_status_count = sc._jsc.sc().getExecutorMemoryStatus().size()\n",
    "estimated_executors = max(sc.defaultParallelism, memory_status_count)\n",
    "rdd = sc.parallelize(range(estimated_executors))\n",
    "def do_update(x):\n",
    "    import os\n",
    "    return str(os.popen(\"cd /sparklingml && git pull && git log -n 5\").read())\n",
    "result = rdd.map(do_update)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = os.environ['PATH'] + \":/usr/lib/chromium/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import concat, collect_set, explode, from_json, format_string\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.session import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import json\n",
    "import os\n",
    "import meetup.api\n",
    "from copy import copy\n",
    "import time\n",
    "import logging\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API key configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open(\"./secrets.literal\").read()) # Load gh_api_token & meetup_key\n",
    "gh_user = \"holdenk\"\n",
    "fs_prefix = \"gs://boo-stuff/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less secret configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_meetup_events = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = (SparkSession.builder\n",
    "           .appName(\"whatCanWeLearnFromTheSixties\")\n",
    "           .config(\"spark.executor.instances\", \"40\")\n",
    "           .config(\"spark.driver.memoryOverhead\", \"0.25\")\n",
    "           .config(\"spark.executor.memory\", \"16g\")\n",
    "           .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "           .config(\"spark.ui.enabled\", \"true\")\n",
    "          ).getOrCreate()\n",
    "sc = session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'holden-magic-m:18080'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In _theory_ in preview dataproc Spark UI is force disabled but history fills the gap, except history server isn't started by default :(\n",
    "sc.getConf().get(\"spark.yarn.historyServer.address\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to get is the committers and PMC members, this information is stored in LDAP but also available in JSON. Eventually we will want to enrich this with mailing list information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFlatJsonFile(path, explodeKey, schema=None):\n",
    "    \"\"\"Load a flat multi-line json file and convert into Spark & explode\"\"\"\n",
    "    rdd = sc.wholeTextFiles(path).values().setName(\"Input file {}\".format(path))\n",
    "    df = (session.read.schema(schema)\n",
    "            .json(rdd))\n",
    "    return df.select(explode(explodeKey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[username: string, extra: struct<name:string,key_fingerprints:array<string>,urls:array<string>>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_people_schema = StructType([StructField(\"lastCreateTimestamp\", StringType()),\n",
    "                     StructField(\"people\",\n",
    "                                 MapType(StringType(), \n",
    "                                         StructType([StructField('name', StringType()),\n",
    "                                                     StructField('key_fingerprints', ArrayType(StringType())),\n",
    "                                                     StructField('urls', ArrayType(StringType())),\n",
    "                                                    ]))\n",
    "                                )])\n",
    "apache_poeple_df_file = \"{0}{1}\".format(fs_prefix, \"http_data_sources/public_ldap_people.json\") # http://people.apache.org/public/public_ldap_people.json\n",
    "apache_people_df = loadFlatJsonFile(path=apache_poeple_df_file, \n",
    "                                 explodeKey=\"people\", schema=apache_people_schema)\n",
    "apache_people_df = apache_people_df.select(apache_people_df.key.alias(\"username\"), apache_people_df.value.alias(\"extra\")).repartition(100).persist().alias(\"apache_people\")\n",
    "apache_people_df.alias(\"Apache Committers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile(\"lazy_helpers.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lazy_helpers.LazyPool"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a lazy urllib3 pool\n",
    "from lazy_helpers import *\n",
    "    \n",
    "bcast_pool = sc.broadcast(LazyPool)\n",
    "bcast_pool.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_on_github(project):\n",
    "    \"\"\"Returns if a project is on github\"\"\"\n",
    "    import urllib3\n",
    "    http = bcast_pool.value.get()\n",
    "    r = http.request('GET', \"https://github.com/apache/{0}\".format(project))\n",
    "    return r.status == 200\n",
    "session.catalog.registerFunction(\"on_github\", project_on_github, BooleanType())\n",
    "# Except I'm a bad person so....\n",
    "from pyspark.sql.catalog import UserDefinedFunction\n",
    "project_on_github_udf = UserDefinedFunction(project_on_github, BooleanType(), \"on_github\")\n",
    "session.catalog._jsparkSession.udf().registerPython(\"on_github\", project_on_github_udf._judf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_committees_schema = StructType([StructField(\"lastCreateTimestamp\", StringType()),\n",
    "                     StructField(\"committees\",\n",
    "                                 MapType(StringType(), StructType([StructField('roster', ArrayType(StringType())),\n",
    "                                                                  StructField('modifyTimestamp', StringType()),\n",
    "                                                                  StructField('createTimestamp', StringType())\n",
    "                                                                  ])))])\n",
    "apache_committees_df_file = \"{0}{1}\".format(fs_prefix, \"http_data_sources/public_ldap_committees.json\") # http://people.apache.org/public/public_ldap_committees.json\n",
    "apache_committees_df = loadFlatJsonFile(path=apache_committees_df_file,\n",
    "                                 explodeKey=\"committees\", schema=apache_committees_schema)\n",
    "apache_committees_on_github_df = apache_committees_df.filter(project_on_github_udf(apache_committees_df.key))\n",
    "apache_committees_on_github_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "committee_names_df = apache_committees_on_github_df.select(apache_committees_df.key.alias(\"project\")).alias(\"apache_committees\").repartition(200)\n",
    "committee_names_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "committee_names_df.alias(\"Apache Committee Names\")\n",
    "committee_names_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[username: string, extra: struct<name:string,key_fingerprints:array<string>,urls:array<string>>, projects: array<string>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_to_user_df = apache_committees_on_github_df.select(\n",
    "    apache_committees_on_github_df.key.alias(\"project\"),\n",
    "    explode(apache_committees_on_github_df.value.roster).alias(\"username\"))\n",
    "\n",
    "\n",
    "user_to_project_df = project_to_user_df.groupBy(project_to_user_df.username).agg(\n",
    "    collect_set(project_to_user_df.project).alias(\"projects\"))\n",
    "apache_people_df = apache_people_df.join(user_to_project_df, on=\"username\")\n",
    "apache_people_df.alias(\"Apache People joined with projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(username='macdonst', extra=Row(name='Simon MacDonald', key_fingerprints=None, urls=None), projects=['cordova'])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_people_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to fetch relevant past & present meetups for each project - idea based on the listing at https://www.apache.org/events/meetups.html but different code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do a non-blocking count to materialize the meetup RDD because this is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some async helpers, in Scala we would use AsyncRDDActions but its not currently available in Python\n",
    "# Support is being considered in https://issues.apache.org/jira/browse/SPARK-20347\n",
    "def non_blocking_rdd_count(rdd):\n",
    "    import threading\n",
    "    def count_magic():\n",
    "        rdd.count()\n",
    "    thread = threading.Thread(target=count_magic)\n",
    "    thread.start()\n",
    "\n",
    "def non_blocking_rdd_save(rdd, target):\n",
    "    import threading\n",
    "    def save_panda():\n",
    "        rdd.saveAsPickleFile(target)\n",
    "    thread = threading.Thread(target=save_panda)\n",
    "    thread.start()\n",
    "\n",
    "def non_blocking_df_save(df, target):\n",
    "    import threading\n",
    "    def save_panda():\n",
    "        df.write.mode(\"overwrite\").save(target)\n",
    "    thread = threading.Thread(target=save_panda)\n",
    "    thread.start()\n",
    "\n",
    "def non_blocking_df_save_csv(df, target):\n",
    "    import threading\n",
    "    def save_panda():\n",
    "        df.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(target)\n",
    "    thread = threading.Thread(target=save_panda)\n",
    "    thread.start()\n",
    "\n",
    "def non_blocking_df_save_or_load(df, target):\n",
    "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jvm.java.net.URI(fs_prefix), sc._jsc.hadoopConfiguration())\n",
    "    success_files = [\"{0}/SUCCESS.txt\", \"{0}/_SUCCESS\"]\n",
    "    if any(fs.exists(sc._jvm.org.apache.hadoop.fs.Path(t.format(target))) for t in success_files):\n",
    "        print(\"Reusing\")\n",
    "        return session.read.load(target).persist()\n",
    "    else:\n",
    "        print(\"Saving\")\n",
    "        non_blocking_df_save(df, target)\n",
    "        return df\n",
    "\n",
    "def non_blocking_df_save_or_load_csv(df, target):\n",
    "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jvm.java.net.URI(fs_prefix), sc._jsc.hadoopConfiguration())\n",
    "    success_files = [\"{0}/SUCCESS.txt\", \"{0}/_SUCCESS\"]\n",
    "    if any(fs.exists(sc._jvm.org.apache.hadoop.fs.Path(t.format(target))) for t in success_files):\n",
    "        print(\"Reusing\")\n",
    "        return session.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(target).persist()\n",
    "    else:\n",
    "        print(\"Saving\")\n",
    "        non_blocking_df_save_csv(df, target)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(\"WARN\")\n",
    "# For now, this is an avenue of future exploration, AKA holden's doesn't want her meetup API keys banned\n",
    "def lookup_relevant_meetup(project_name, max_meetup_events=0):\n",
    "    \"\"\"Lookup relevant meetups for a specific project.\"\"\"\n",
    "    import logging\n",
    "    import time\n",
    "    import meetup.api\n",
    "    logger = logging.getLogger()\n",
    "    meetup_delay = 30\n",
    "    meetup_reset_delay = 3600 # 1 hour\n",
    "    standard_keys = {\"text_format\": \"plain\", \"trending\": \"desc=true\", \"and_text\": \"true\", \"city\": \"san francisco\", \"country\": \"usa\", \"text\": \"apache \" + project_name, \"radius\": 10000}\n",
    "    results = {\"upcoming\": [], \"past\": []}\n",
    "    for status in [\"upcoming\", \"past\"]:\n",
    "        keys = copy(standard_keys)\n",
    "        keys[\"status\"] = status\n",
    "        count = 200\n",
    "        base = 0\n",
    "        while (count == 200 and (max_meetup_events == 0 or base < max_meetup_events)):\n",
    "            logging.debug(\"Fetch {0} meetups for {1} on base {2}\".format(status, project_name, base))\n",
    "            project_name = \"spark\"\n",
    "            client = client = meetup.api.Client(meetup_key)\n",
    "            if base > 0:\n",
    "                keys[\"page\"] = base\n",
    "            # Manually sleep for meetup_reset_delay on failure, the meetup-api package retry logic sometimes breaks :(\n",
    "            response = None\n",
    "            retry_count = 0\n",
    "            while response is None and retry_count < 10:\n",
    "                try:\n",
    "                    response = client.GetOpenEvents(**keys)\n",
    "                except:\n",
    "                    response = None\n",
    "                    retry_count += 1\n",
    "                    time.sleep(meetup_reset_delay)\n",
    "                    try:\n",
    "                        response = client.GetOpenEvents(**keys)\n",
    "                    except:\n",
    "                        response = None\n",
    "            try:\n",
    "                count = response.meta['count']\n",
    "                base = base + count\n",
    "                results[status].append(response.results)\n",
    "                time.sleep(meetup_delay)\n",
    "            except:\n",
    "                count = 0\n",
    "    return (project_name, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Meetup Data RDD PythonRDD[74] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_meetups_rdd = committee_names_df.repartition(500).rdd.map(lambda x: x.project).map(lambda name: lookup_relevant_meetup(name, max_meetup_events))\n",
    "project_meetups_rdd.setName(\"Meetup Data RDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "#raw_project_meetups_df = project_meetups_rdd.toDF() \n",
    "#raw_project_meetups_df.alias(\"Project -> meetup dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_df = non_blocking_df_save(raw_project_meetups_df, \"mini_meetup_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_meetups_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the provided projects attempt to lookup their GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_project_git(org, project):\n",
    "    \"\"\"Returns the project github for a specific project. Assumes project is git hosted\"\"\"\n",
    "    return \"https://github.com/{0}/{1}.git\".format(org, project)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_project_github_data(org, project):\n",
    "    \"\"\"Fetch the project github data, note this only gets github issues so likely not super useful\"\"\"\n",
    "    from perceval.backends.core.github import GitHub as perceval_github\n",
    "    gh_backend = perceval_github(owner=org, repository=project, api_token=gh_api_token)\n",
    "    # The backend return a generator - which is awesome. However since we want to pull this data into Spark \n",
    "    def append_project_info(result):\n",
    "        \"\"\"Add the project information to the return from perceval\"\"\"\n",
    "        result[\"project_name\"] = project\n",
    "        return result\n",
    "\n",
    "    return list(map(append_project_info, gh_backend.fetch()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_project_git_data(org, project):\n",
    "    from perceval.backends.core.git import Git as perceval_git\n",
    "\n",
    "    git_uri = lookup_project_git(org, project)\n",
    "    import tempfile\n",
    "    import shutil\n",
    "    tempdir = tempfile.mkdtemp()\n",
    "\n",
    "    def append_project_info(result):\n",
    "        \"\"\"Add the project information to the return from perceval\"\"\"\n",
    "        result[\"project_name\"] = project\n",
    "        return result\n",
    "\n",
    "    try:\n",
    "        git_backend = perceval_git(uri=git_uri, gitpath=tempdir + \"/repo\")\n",
    "        return list(map(append_project_info, git_backend.fetch()))\n",
    "    finally:\n",
    "        shutil.rmtree(tempdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the git history info using perceval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceival GIT dat UnionRDD[86] at union at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_git_project_data_rdd = committee_names_df.repartition(400).rdd.flatMap(lambda row: fetch_project_git_data(\"apache\", row.project))\n",
    "jupyter_git_project_data_rdd = sc.parallelize([(\"jupyter\", \"notebook\"), (\"nteract\", \"nteract\")]).flatMap(lambda elem: fetch_project_git_data(elem[0], elem[1]))\n",
    "git_project_data_rdd = apache_git_project_data_rdd.union(jupyter_git_project_data_rdd)\n",
    "git_project_data_rdd.setName(\"Perceival GIT dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing\n"
     ]
    }
   ],
   "source": [
    "git_project_data_df_raw = git_project_data_rdd.map(lambda row: Row(**row)).toDF().persist()\n",
    "git_project_data_df = non_blocking_df_save_or_load(git_project_data_df_raw, \"{0}/raw_git_data\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(backend_name,StringType,true),StructField(backend_version,StringType,true),StructField(category,StringType,true),StructField(data,MapType(StringType,StringType,true),true),StructField(origin,StringType,true),StructField(perceval_version,StringType,true),StructField(project_name,StringType,true),StructField(tag,StringType,true),StructField(timestamp,DoubleType,true),StructField(updated_on,DoubleType,true),StructField(uuid,StringType,true)))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_project_data_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_authors_by_project_and_commit_df = git_project_data_df.select(\"project_name\", \"data.Author\", \"data.CommitDate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+\n",
      "|project_name|              Author|          CommitDate|\n",
      "+------------+--------------------+--------------------+\n",
      "|  manifoldcf|Grant Ingersoll <...|Mon Jan 11 22:36:...|\n",
      "|  manifoldcf|Grant Ingersoll <...|Tue Jan 12 15:04:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Sat Feb 13 19:06:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Sat Feb 13 20:20:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Sat Feb 13 21:20:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Sat Feb 13 21:42:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Sat Feb 13 21:59:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Sun Feb 14 01:04:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Sun Feb 14 12:08:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Sun Feb 14 13:40:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Sun Feb 14 14:15:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Sun Feb 14 20:37:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Mon Feb 15 15:51:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Mon Feb 15 20:16:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Tue Feb 16 13:11:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Tue Feb 16 13:51:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Tue Feb 16 13:53:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Tue Feb 16 17:48:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Tue Feb 16 18:00:...|\n",
      "|  manifoldcf|Karl Wright <kwri...|Tue Feb 16 19:26:...|\n",
      "+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(project_name='manifoldcf', Author='Grant Ingersoll <gsingers@apache.org>', CommitDate='Mon Jan 11 22:36:06 2010 +0000')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_authors_by_project_and_commit_df.show()\n",
    "raw_authors_by_project_and_commit_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(StringType())\n",
    "def strip_junk(inputSeries):\n",
    "    \"\"\"Discard timezone information, who needs that anyways.\n",
    "    More accurately we don't care about that here since we're looking at a year long window.\"\"\"\n",
    "    return inputSeries.apply(lambda x: x.split(\"+\")[0])\n",
    "\n",
    "@F.pandas_udf(StringType())\n",
    "def extract_email(inputSeries):\n",
    "    \"\"\"Take e-mails of the form Foo Baz<foobaz@baz.com> and turn it into foobaz@baz.com\"\"\"\n",
    "    import re\n",
    "    def extract_email_record(record):\n",
    "        try:\n",
    "            emails = re.findall('<\\S+>$', record)\n",
    "            return emails[0]\n",
    "        except:\n",
    "            return record\n",
    "    \n",
    "    return inputSeries.apply(extract_email_record)\n",
    "\n",
    "@F.pandas_udf(StringType())\n",
    "def extract_name(inputSeries):\n",
    "    \"\"\"Take e-mails of the form Foo Baz<foobaz@baz.com> and turn it into the probable name e.g. Foo Baz\"\"\"\n",
    "    import re\n",
    "    def extract_name_record(record):\n",
    "        try:\n",
    "            emails = re.findall('([^<]+)<\\S+>$', record)\n",
    "            return emails[0]\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    return inputSeries.apply(extract_name_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_by_project_and_commit_df = raw_authors_by_project_and_commit_df.select(\n",
    "    \"project_name\", \"Author\", extract_email(\"Author\").alias(\"email\"), extract_name(\"Author\").alias(\"name\"),\n",
    "    F.to_date(strip_junk(\"CommitDate\"), format=\"EEE MMM d H:mm:ss YYYY \").alias(\"CommitDate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(project_name,StringType,true),StructField(Author,StringType,true),StructField(email,StringType,true),StructField(name,StringType,true),StructField(CommitDate,DateType,true)))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_by_project_and_commit_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[project_name: string, email: string, Author: string, latest_commit: date]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_distinct_authors_latest_commit = authors_by_project_and_commit_df.groupBy(\n",
    "    \"project_name\", \"email\").agg(\n",
    "    F.last(\"Author\").alias(\"Author\"), F.max(\"CommitDate\").alias(\"latest_commit\"))\n",
    "raw_distinct_authors_latest_commit.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing\n"
     ]
    }
   ],
   "source": [
    "distinct_authors_latest_commit = non_blocking_df_save_or_load(\n",
    "    raw_distinct_authors_latest_commit,\n",
    "    \"{0}distinct_authors_latest_commit_4\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(StringType(), functionType=F.PandasUDFType.SCALAR)\n",
    "def lookup_github_user_by_email(emails):\n",
    "    \n",
    "    import time\n",
    "    from github import Github\n",
    "    import backoff\n",
    "    github_client = Github(gh_user, gh_api_token)\n",
    "    # In theory PyGithub handles backoff but we have multiple instances/machines.\n",
    "    @backoff.on_exception(backoff.expo, Exception)\n",
    "    def inner_lookup_github_user_by_email(email):\n",
    "        \"\"\"Lookup github user by e-mail address and returns the github username. Returns None if no user or more than 1 user is found.\"\"\"\n",
    "        users = github_client.search_users(\"{0}\".format(email))\n",
    "        def process_result(users):\n",
    "            if users.totalCount == 1:\n",
    "                return list(users).pop().login\n",
    "            else:\n",
    "                return \"\"\n",
    "\n",
    "        return process_result(users)\n",
    "\n",
    "    return emails.apply(inner_lookup_github_user_by_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_to_github_username = distinct_authors_latest_commit.withColumn(\n",
    "    \"github_username\",\n",
    "    lookup_github_user_by_email(\"email\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(returnType=StringType(), functionType=F.PandasUDFType.SCALAR)\n",
    "def fetch_github_user_bio(logins):\n",
    "    from github import Github\n",
    "    import time\n",
    "    github_client = Github(gh_user, gh_api_token)\n",
    "    import backoff\n",
    "\n",
    "    @backoff.on_exception(backoff.expo, Exception)\n",
    "    def individual_fetch_github_user_bio(login):\n",
    "        if login == None or login == \"\":\n",
    "            return \"\"\n",
    "\n",
    "        result = github_client.get_user(login=login)\n",
    "        try:\n",
    "            return result.bio\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    return logins.apply(individual_fetch_github_user_bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving\n"
     ]
    }
   ],
   "source": [
    "authors_to_github_username.persist()\n",
    "authors_to_github_username_saved = non_blocking_df_save_or_load(\n",
    "    authors_to_github_username,\n",
    "    \"{0}/authors_to_github-10\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(project_name,StringType,true),StructField(email,StringType,true),StructField(Author,StringType,true),StructField(latest_commit,DateType,true)))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_authors_latest_commit.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(project_name,StringType,true),StructField(email,StringType,true),StructField(Author,StringType,true),StructField(latest_commit,DateType,true),StructField(github_username,StringType,true)))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_to_github_username_saved.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_authors_with_gh = authors_to_github_username_saved.withColumn(\n",
    "    \"new_unique_id\",\n",
    "    F.when(F.col(\"github_username\") != \"\",\n",
    "         F.col(\"github_username\")).otherwise(\n",
    "        F.col(\"email\")))\n",
    "                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_grouped_by_id = distinct_authors_with_gh.groupBy(\"project_name\", \"new_unique_id\").agg(\n",
    "    collect_set(F.col(\"email\")).alias(\"emails\"),\n",
    "    F.last(F.col(\"Author\")).alias(\"Author\"),\n",
    "    F.first(\"github_username\").alias(\"github_username\"),\n",
    "    F.max(\"latest_commit\").alias(\"latest_commit\"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(project_name,StringType,true),StructField(new_unique_id,StringType,true),StructField(emails,ArrayType(StringType,true),true),StructField(Author,StringType,true),StructField(github_username,StringType,true),StructField(latest_commit,DateType,true)))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_grouped_by_id.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing\n"
     ]
    }
   ],
   "source": [
    "authors_grouped_by_id.persist()\n",
    "authors_grouped_by_id_saved = non_blocking_df_save_or_load(\n",
    "    authors_grouped_by_id,\n",
    "    \"{0}/authors_grouped_by_id-3\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookup info from crunchbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/conda/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/chromium/'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazy_helpers import *\n",
    "\n",
    "bcast_driver = sc.broadcast(LazyDriver)\n",
    "\n",
    "# TBD if we should see this, see comments on robots.txt in function, also consider overhead of firefox req\n",
    "def lookup_crunchbase_info(people_and_projects):\n",
    "    \"\"\"Lookup a person a crunch base and see what the gender & company is.\n",
    "    Filter for at least one mention of their projects.\"\"\"\n",
    "    # Path hack\n",
    "    if not \"chromium\" in os.environ['PATH']:\n",
    "        os.environ['PATH'] = os.environ['PATH'] + \":/usr/lib/chromium/\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    driver = bcast_driver.value.get()\n",
    "    import time\n",
    "    import random\n",
    "    for (username, name, projects, urls) in people_and_projects:\n",
    "        time.sleep(random.randint(60, 2*60))\n",
    "        # robots.txt seems to be ok with person for now as of April 4 2018, double check before re-running this\n",
    "        url = \"https://www.crunchbase.com/person/{0}\".format(name.replace(\" \", \"-\"))\n",
    "        try:\n",
    "            if driver.current_url != url:\n",
    "                driver.get(url)\n",
    "            text = driver.page_source\n",
    "            lower_text = text.lower()\n",
    "            yield[lower_text]\n",
    "            if \"the quick brown fox jumps over the lazy dog\" in lower_text or \"pardon our interruption...\" in lower_text:\n",
    "                time.sleep(random.randint(30*60, 2*60*60))\n",
    "                bcast_driver.value.reset()\n",
    "            if any(project.lower() in lower_text for project in projects) or any(url.lower in lower_text for url in urls):\n",
    "                soup = BeautifulSoup(text, \"html.parser\")\n",
    "                stats = soup.findAll(\"div\", { \"class\" : \"component--fields-card\"})[0]\n",
    "                # Hacky but I'm lazy\n",
    "                result = {}\n",
    "                result[\"crunchbase-url\"] = url\n",
    "                result[\"username\"] = username\n",
    "                if \"Female\" in str(stats):\n",
    "                    result[\"gender\"] = \"Female\"\n",
    "                if \"Male\" in str(stats):\n",
    "                    result[\"gender\"] = \"Male\"\n",
    "                try:\n",
    "                    m = re.search(\"\\\" title=\\\"(.+?)\\\" href=\\\"\\/organization\", lower_text)\n",
    "                    result[\"company\"] = m.group(1)\n",
    "                except:\n",
    "                    # No match no foul\n",
    "                    pass\n",
    "                yield result\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = lookup_crunchbase_info([(\"holden\", \"holden karau\", [\"spark\"], [\"http://www.holdenkarau.com\"])])\n",
    "#list(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment the committer info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[username: string, gender: string, company: string, crunchbase-url: string]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We do this as an RDD transformation since the cost of the transformation dominates\n",
    "relevant_info = apache_people_df.select(\n",
    "    apache_people_df.username,\n",
    "    apache_people_df.extra.getField(\"name\").alias(\"name\"),\n",
    "    apache_people_df.projects,\n",
    "    apache_people_df.extra.getField(\"urls\").alias(\"urls\"))\n",
    "crunchbase_info_rdd = relevant_info.rdd.map(lambda row: (row.username, row.name, row.projects, row.urls)).mapPartitions(lookup_crunchbase_info)\n",
    "crunchbase_info_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "schema = StructType([\n",
    "    StructField(\"username\", StringType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"company\", StringType()),\n",
    "    StructField(\"crunchbase-url\", StringType())])\n",
    "crunchbase_info_df = crunchbase_info_rdd.toDF(schema = schema)\n",
    "crunchbase_info_df.alias(\"Crunchbase user information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving\n"
     ]
    }
   ],
   "source": [
    "crunchbase_info_df = non_blocking_df_save_or_load(\n",
    "    crunchbase_info_df,\n",
    "    \"{0}crunchbase_out_11\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crunchbase_info_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2565"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_people_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to Mechnical turk format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing\n"
     ]
    }
   ],
   "source": [
    "def mini_concat_udf(array_strs):\n",
    "    \"\"\"Concat the array of strs\"\"\"\n",
    "    if array_strs == None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return ' '.join(array_strs)\n",
    "\n",
    "# Except I'm a bad person so....\n",
    "from pyspark.sql.catalog import UserDefinedFunction\n",
    "mini_concat_udf = UserDefinedFunction(mini_concat_udf, StringType(), \"mini_concat_udf\")\n",
    "session.catalog._jsparkSession.udf().registerPython(\"mini_concat_udf\", mini_concat_udf._judf)\n",
    "\n",
    "mini_csv_data_df = apache_people_df.select(\n",
    "    apache_people_df.username,\n",
    "    apache_people_df.extra.getField(\"name\").alias(\"name\"),\n",
    "    mini_concat_udf(apache_people_df.extra.getField(\"urls\")).alias(\"personal_websites\"),\n",
    "    mini_concat_udf(apache_people_df.projects).alias(\"projects\")\n",
    "    ).coalesce(1)\n",
    "mini_csv_data_df = non_blocking_df_save_or_load_csv(mini_csv_data_df, \"{0}/apache_people.csv\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crunchbase_info_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One of the things that is interesting is understanding what the tones of the meetup descriptions & mailing list posts are. We can use https://www.ibm.com/watson/developercloud/tone-analyzer/api/v3/?python#introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pandas UDF accelerate (but multiple pieces of informaiton returned at the same time)\n",
    "def lookup_sentiment(document):\n",
    "    \"\"\"Looks up the sentiment for a specific document.\"\"\"\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    # TODO(holden): Consider broadcast variable?\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    return sid.polarity_scores(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_no_you = lookup_sentiment(\"oh no you didn't girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.355, 'neu': 0.645, 'pos': 0.0, 'compound': -0.296}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_no_you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ok its time to find some mailing list info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbox_failures = sc.accumulator(0)\n",
    "\n",
    "def fetch_mbox_ids(project_name):\n",
    "    \"\"\"Return the mbox ids\"\"\"\n",
    "    import itertools\n",
    "\n",
    "    def fetch_mbox_ids_apache_site(box_type):\n",
    "        \"\"\"Fetches all of the mbox ids from a given apache project and box type (dev or user)\"\"\"\n",
    "        root_url = \"http://mail-archives.apache.org/mod_mbox/{0}-{1}\".format(project_name, box_type)\n",
    "        \n",
    "        # Fetch the page to parse\n",
    "        pool = bcast_pool.value.get()\n",
    "        result = pool.request('GET', root_url)\n",
    "        \n",
    "        \n",
    "        from bs4 import BeautifulSoup\n",
    "        soup = BeautifulSoup(result.data, \"html.parser\")\n",
    "        mbox_ids = set(map(lambda tag: tag.get('id'), soup.findAll(\"span\", { \"class\" : \"links\"})))\n",
    "        return map(lambda box_id: (project_name, box_type, box_id), mbox_ids)\n",
    "    # We have to return a list here because PySpark doesn't handle generators (TODO: holden)\n",
    "    return list(itertools.chain.from_iterable(map(fetch_mbox_ids_apache_site, [\"dev\", \"user\"])))\n",
    "        \n",
    "        \n",
    "def fetch_and_process_mbox_records(project_name, box_type, mbox_id):\n",
    "        import tempfile\n",
    "        import shutil\n",
    "        from perceval.backends.core.mbox import MBox as perceval_mbox\n",
    "\n",
    "        def process_mbox_directory(base_url, dir_path):\n",
    "            mbox_backend = perceval_mbox(base_url, dir_path)\n",
    "            return mbox_backend.fetch()\n",
    "        \n",
    "        def append_project_info(result):\n",
    "            \"\"\"Add the project information to the return from perceval\"\"\"\n",
    "            result[\"project_name\"] = project_name\n",
    "            result[\"box_type\"] = box_type\n",
    "            result[\"mbox_id\"] = mbox_id\n",
    "            return result\n",
    "\n",
    "        # Make a temp directory to hold the mbox files\n",
    "        tempdir = tempfile.mkdtemp()\n",
    "\n",
    "        try:\n",
    "            root_url = \"http://mail-archives.apache.org/mod_mbox/{0}-{1}\".format(project_name, box_type)\n",
    "            mbox_url = \"{0}/{1}.mbox\".format(root_url, mbox_id)\n",
    "            filename = \"{0}/{1}.mbox\".format(tempdir, mbox_id)\n",
    "        \n",
    "            print(\"fetching {0}\".format(mbox_url))\n",
    "\n",
    "            pool = bcast_pool.value.get()\n",
    "            with pool.request('GET', mbox_url, preload_content=False) as r, open(filename, 'wb') as out_file:       \n",
    "                try:\n",
    "                    shutil.copyfileobj(r, out_file)\n",
    "                    return list(map(append_project_info, process_mbox_directory(root_url, tempdir)))\n",
    "                except:\n",
    "                    mbox_failures.add(1)\n",
    "                    return []\n",
    "        finally:\n",
    "            shutil.rmtree(tempdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[216] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_key(x):\n",
    "    import random\n",
    "    return (random.randint(0, 40000), x)\n",
    "\n",
    "def de_key(x):\n",
    "    return x[1]\n",
    "\n",
    "mailing_list_posts_mbox_ids = committee_names_df.repartition(400).rdd.flatMap(lambda row: fetch_mbox_ids(row.project))\n",
    "# mbox's can be big, so break up how many partitions we have\n",
    "mailing_list_posts_mbox_ids = mailing_list_posts_mbox_ids.map(random_key).repartition(2000).map(de_key)\n",
    "mailing_list_posts_rdd = mailing_list_posts_mbox_ids.flatMap(lambda args: fetch_and_process_mbox_records(*args))\n",
    "mailing_list_posts_rdd.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[project_name: string, box_type: string, mbox_id: string, backend_name: string, backend_version: string, category: string, data: map<string,string>, origin: string, perceval_version: string, tag: string, timestamp: double, updated_on: double, uuid: string]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"project_name\",StringType()),\n",
    "    StructField(\"box_type\",StringType()), # dev or user\n",
    "    StructField(\"mbox_id\",StringType()),\n",
    "    StructField(\"backend_name\",StringType()),\n",
    "    StructField(\"backend_version\",StringType()),\n",
    "    StructField(\"category\",StringType()),\n",
    "    StructField(\"data\", MapType(StringType(),StringType())), # The \"important\" bits\n",
    "    StructField(\"origin\",StringType()),\n",
    "    StructField(\"perceval_version\",StringType()),\n",
    "    StructField(\"tag\",StringType()),\n",
    "    StructField(\"timestamp\",DoubleType()),\n",
    "    StructField(\"updated_on\",DoubleType()),\n",
    "    StructField(\"uuid\",StringType())])\n",
    "mailing_list_posts_mbox_df_raw = mailing_list_posts_rdd.toDF(schema=schema)\n",
    "mailing_list_posts_mbox_df_raw.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "mailing_list_posts_mbox_df_raw.alias(\"Mailing list perceival information - no post processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing\n"
     ]
    }
   ],
   "source": [
    "mailing_list_posts_mbox_df_raw = non_blocking_df_save_or_load(\n",
    "    mailing_list_posts_mbox_df_raw,\n",
    "    \"{0}mailing_list_info_4\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = mailing_list_posts_mbox_df_raw.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(project_name='juddi', box_type='dev', mbox_id='201302', backend_name='MBox', backend_version='0.10.2', category='message', data={'Reply-To': 'dev@juddi.apache.org', 'X-Original-To': 'apmail-juddi-dev-archive@www.apache.org', 'Return-Path': '<dev-return-5466-apmail-juddi-dev-archive=juddi.apache.org@juddi.apache.org>', 'List-Post': '<mailto:dev@juddi.apache.org>', 'Received': 'from arcas.apache.org (HELO arcas.apache.org) (140.211.11.28)\\n    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Feb 2013 13:38:13 +0000', 'Message-ID': '<JIRA.12632374.1360849017274.288795.1360849093042@arcas>', 'Content-Transfer-Encoding': '7bit', 'From': '\"Steve Webb (JIRA)\" <juddi-dev@ws.apache.org>', 'body': '{plain=Steve Webb created JUDDI-553:\\n--------------------------------\\n\\n             Summary: TModelDetail not deleted when TModel is deleted\\n                 Key: JUDDI-553\\n                 URL: https://issues.apache.org/jira/browse/JUDDI-553\\n             Project: jUDDI\\n          Issue Type: Bug\\n          Components: uddi-client\\n    Affects Versions: 3.1.3\\n            Reporter: Steve Webb\\n            Assignee: Kurt T Stam\\n\\n\\nI use the client to create a TModel (saveTModel) then a BusinessService (saveService) and finally a binding (saveBindings).\\n\\nI use getTModelDetail within SoapUI to get the details.\\n\\nFinally I deleteBinding, deleteTModel and then deleteService. \\n\\nI use gTModelDetail with soapUI using previous tModelKey which was deleted and I still get the tModelDetails back. The tmodel, binding and service are deleted when requested from soapUI.\\n\\nIt looks to me that the tModelDetails in the database are not being deleted.\\n\\n--\\nThis message is automatically generated by JIRA.\\nIf you think it was sent incorrectly, please contact your JIRA administrators\\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\\n}', 'In-Reply-To': '<JIRA.12632374.1360849017274@arcas>', 'Date': 'Thu, 14 Feb 2013 13:38:13 +0000 (UTC)', 'MIME-Version': '1.0', 'Subject': '[jira] [Created] (JUDDI-553) TModelDetail not deleted when TModel\\n is deleted', 'X-JIRA-FingerPrint': '30527f35849b9dde25b450d4833f0394', 'List-Unsubscribe': '<mailto:dev-unsubscribe@juddi.apache.org>', 'References': '<JIRA.12632374.1360849017274@arcas>', 'List-Help': '<mailto:dev-help@juddi.apache.org>', 'Delivered-To': 'mailing list dev@juddi.apache.org', 'List-Id': '<dev.juddi.apache.org>', 'To': 'dev@juddi.apache.org', 'Precedence': 'bulk', 'unixfrom': 'dev-return-5466-apmail-juddi-dev-archive=juddi.apache.org@juddi.apache.org  Thu Feb 14 13:38:19 2013', 'Mailing-List': 'contact dev-help@juddi.apache.org; run by ezmlm', 'Content-Type': 'text/plain; charset=utf-8'}, origin='http://mail-archives.apache.org/mod_mbox/juddi-dev', perceval_version='0.11.2', tag='http://mail-archives.apache.org/mod_mbox/juddi-dev', timestamp=1531262144.392772, updated_on=1360849093.0, uuid='cda9c36d8bf0ea946c997bb979fd8bcbfb747c26')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailing_list_posts_mbox_df = mailing_list_posts_mbox_df_raw.select(\n",
    "    \"*\",\n",
    "    mailing_list_posts_mbox_df_raw.data.getField(\"From\").alias(\"from\"),\n",
    "    extract_email(mailing_list_posts_mbox_df_raw.data.getField(\"From\")).alias(\"from_processed_email\"),\n",
    "    mailing_list_posts_mbox_df_raw.data.getField(\"body\").alias(\"body\"),\n",
    "    mailing_list_posts_mbox_df_raw.data.getField(\"Message-ID\").alias(\"message_id\"),\n",
    "    mailing_list_posts_mbox_df_raw.data.getField(\"In-Reply-To\").alias(\"in_reply_to\"),\n",
    "    mailing_list_posts_mbox_df_raw.data.getField(\"Content-Language\").alias(\"content_language\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing\n"
     ]
    }
   ],
   "source": [
    "mailing_list_posts_mbox_df_saved = non_blocking_df_save_or_load(\n",
    "    mailing_list_posts_mbox_df,\n",
    "    \"{0}/processed_mbox_data_3\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start using some of the lazily created DFs to compute the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15440"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_grouped_by_id_saved.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+---------------+-------------+\n",
      "|project_name|       new_unique_id|              emails|              Author|github_username|latest_commit|\n",
      "+------------+--------------------+--------------------+--------------------+---------------+-------------+\n",
      "|    accumulo|  <dhutchis@mit.edu>|[<dhutchis@mit.edu>]|Dylan Hutchison <...|               |   2015-01-02|\n",
      "|    accumulo|<mario.pastorelli...|[<mario.pastorell...|Mario Pastorelli ...|               |   2015-12-30|\n",
      "|    activemq|<czobrisky@gmail....|[<czobrisky@gmail...|Chad Zobrisky <cz...|               |   2014-12-31|\n",
      "|       arrow|<adeneche@dremio....|[<adeneche@dremio...|adeneche <adenech...|               |   2017-01-06|\n",
      "|       atlas|<amestry@apache.org>|[<amestry@apache....|Ashutosh Mestry <...|               |   2017-01-06|\n",
      "|       atlas|<venkat@hortonwor...|[<venkat@hortonwo...|Venkat Ranganatha...|               |   2015-01-02|\n",
      "|      aurora|<ndonatucci@medal...|[<ndonatucci@meda...|Nicolás Donatucci...|               |   2017-01-03|\n",
      "|      aurora|<sgeorge@twitter....|[<sgeorge@twitter...|Selvin George <sg...|               |   2013-01-05|\n",
      "|        beam|<cpovirk@google.com>|[<cpovirk@google....|cpovirk <cpovirk@...|               |   2016-01-01|\n",
      "|        beam|<p.kaczmarczyk@oc...|[<p.kaczmarczyk@o...|Pawel Kaczmarczyk...|               |   2018-01-02|\n",
      "|      bigtop|<pengwenwu2008@16...|[<pengwenwu2008@1...|Wenwu Peng <pengw...|               |   2014-01-02|\n",
      "|    brooklyn|  <bostko@gmail.com>|[<bostko@gmail.com>]|Valentin Aitken <...|               |   2014-12-30|\n",
      "|     calcite|<serhii.harnyk@gm...|[<serhii.harnyk@g...|Serhii-Harnyk <se...|               |   2015-12-29|\n",
      "|       camel|<Thomas.papke@icw...|[<Thomas.papke@ic...|Thopap <Thomas.pa...|               |   2017-01-03|\n",
      "|       camel|<jvazquez@tecsisa...|[<jvazquez@tecsis...|juanjovazquez <jv...|               |   2014-01-01|\n",
      "|       camel|             nkukhar|[<kukhar.n@gmail....|nkukhar <kukhar.n...|        nkukhar|   2015-01-02|\n",
      "|       camel|        softwaredoug|[<dturnbull@o19s....|Doug Turnbull <dt...|   softwaredoug|   2014-01-04|\n",
      "|  carbondata|<SRIGOPALMOHANTY@...|[<SRIGOPALMOHANTY...|SRIGOPALMOHANTY <...|               |   2017-01-03|\n",
      "|  carbondata|<vincent.chenfei@...|[<vincent.chenfei...|vincentchenfei <v...|               |   2017-01-03|\n",
      "|   cassandra|<MWeiser@ardmoref...|[<MWeiser@ardmore...|Matthias Weiser <...|               |   2018-01-02|\n",
      "+------------+--------------------+--------------------+--------------------+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authors_grouped_by_id_saved.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "| project_name|author_count|\n",
      "+-------------+------------+\n",
      "|         lucy|          14|\n",
      "|      vxquery|          16|\n",
      "|    chemistry|          12|\n",
      "|       roller|          19|\n",
      "|        geode|         136|\n",
      "|       falcon|          55|\n",
      "|trafficserver|         307|\n",
      "|          tez|          26|\n",
      "|       pdfbox|          21|\n",
      "|        httpd|         113|\n",
      "|   carbondata|         135|\n",
      "|        celix|          13|\n",
      "|       wicket|          98|\n",
      "|     accumulo|          95|\n",
      "|        twill|          38|\n",
      "|   servicemix|          19|\n",
      "|     clerezza|          17|\n",
      "|      couchdb|         169|\n",
      "|       bigtop|         139|\n",
      "|     marmotta|          25|\n",
      "+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_authors_by_project = authors_grouped_by_id_saved.groupBy(\"project_name\").agg(F.count(\"Author\").alias(\"author_count\"))\n",
    "num_authors_by_project.cache()\n",
    "num_authors_by_project.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the sample %s for each project so we can get reasonable confidence bounds for sampling.\n",
    "Looking at http://veekaybee.github.io/2015/08/04/how-big-of-a-sample-size-do-you-need/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think is Cochran's formula scaled for small datasets\n",
    "@F.udf(IntegerType())\n",
    "def compute_num_required_sample_1(pop_size):\n",
    "    import numpy as np\n",
    "    import scipy.stats\n",
    "    import math\n",
    "    e = 0.05\n",
    "    Z = 1.64 # 90%, 95%: 1.96\n",
    "    p = 0.5\n",
    "    N = pop_size\n",
    "    # CALC SAMPLE SIZE\n",
    "    n_0 = ((Z**2) * p * (1-p)) / (e**2)\n",
    "    # ADJUST SAMPLE SIZE FOR FINITE POPULATION\n",
    "    n = n_0 / (1 + ((n_0 - 1) / float(N)) )\n",
    "    target = int(math.ceil(n))\n",
    "    # Compute a fall back size\n",
    "    fall_back_size = min(3, pop_size)\n",
    "    return max(fall_back_size, target) # THE SAMPLE SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399.99999999999994"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number 2: https://en.wikipedia.org/wiki/Sample_size_determination#Estimation\n",
    "def walds_method():\n",
    "    return 1/(0.05**2) # +- 5%\n",
    "walds_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sample_sizes = num_authors_by_project.withColumn(\n",
    "    \"sample_size_1\",\n",
    "    compute_num_required_sample_1(\"author_count\")).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing\n"
     ]
    }
   ],
   "source": [
    "sample_sizes = non_blocking_df_save_or_load(raw_sample_sizes, \"{0}/sample_sizes_10\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------------+\n",
      "|project_name|author_count|sample_size_1|\n",
      "+------------+------------+-------------+\n",
      "|spamassassin|          39|           35|\n",
      "|        oodt|          41|           36|\n",
      "|     cayenne|          45|           39|\n",
      "|     streams|          25|           23|\n",
      "|  jackrabbit|          37|           33|\n",
      "|  manifoldcf|          19|           18|\n",
      "|        drat|          29|           27|\n",
      "|      giraph|          37|           33|\n",
      "|     cordova|           1|            1|\n",
      "|      thrift|         237|          127|\n",
      "|      tomcat|          34|           31|\n",
      "|     syncope|          31|           28|\n",
      "|     calcite|         159|          101|\n",
      "|      impala|         131|           89|\n",
      "|       nutch|          72|           57|\n",
      "|    activemq|          90|           68|\n",
      "|  deltaspike|          56|           47|\n",
      "|      allura|          84|           65|\n",
      "|openmeetings|           7|            7|\n",
      "|       tomee|          47|           41|\n",
      "+------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_sizes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|sum(sample_size_1)|\n",
      "+------------------+\n",
      "|              9117|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_sizes.groupby().agg(F.sum(\"sample_size_1\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is a bit high to do on a shoestring budget with sampling, but what about if we limit to folks who have recently participated & got rid of projects with limited or no recent participation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(project_name,StringType,true),StructField(new_unique_id,StringType,true),StructField(emails,ArrayType(StringType,true),true),StructField(Author,StringType,true),StructField(github_username,StringType,true),StructField(latest_commit,DateType,true)))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_grouped_by_id_saved.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_distinct_authors_latest_commit = authors_grouped_by_id_saved.filter(\n",
    "    (F.date_sub(F.current_date(), 365)) < authors_grouped_by_id_saved.latest_commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(project_name,StringType,true),StructField(new_unique_id,StringType,true),StructField(emails,ArrayType(StringType,true),true),StructField(Author,StringType,true),StructField(github_username,StringType,true),StructField(latest_commit,DateType,true)))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_distinct_authors_latest_commit.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+---------------+-------------+\n",
      "|project_name|       new_unique_id|              emails|              Author|github_username|latest_commit|\n",
      "+------------+--------------------+--------------------+--------------------+---------------+-------------+\n",
      "|        beam|<p.kaczmarczyk@oc...|[<p.kaczmarczyk@o...|Pawel Kaczmarczyk...|               |   2018-01-02|\n",
      "|   cassandra|<MWeiser@ardmoref...|[<MWeiser@ardmore...|Matthias Weiser <...|               |   2018-01-02|\n",
      "|         cxf|<butkovic@gmail.com>|[<butkovic@gmail....|Peter Butkovic <b...|               |   2017-12-31|\n",
      "|         cxf|<simon.marti@inve...|[<simon.marti@inv...|Simon Marti <simo...|               |   2017-12-31|\n",
      "|       eagle| <yonzhang@ebay.com>|[<yonzhang@ebay.c...|yonzhang <yonzhan...|               |   2018-01-02|\n",
      "|        hive| <ganu.ec@gmail.com>|[<ganu.ec@gmail.c...|Ganesha Shreedhar...|               |   2018-01-05|\n",
      "|       karaf| <ancosen@gmail.com>|[<ancosen@gmail.c...|Andrea Cosentino ...|               |   2018-01-04|\n",
      "|        kudu|<wdberkeley@apach...|[<wdberkeley@apac...|Will Berkeley <wd...|               |   2018-01-06|\n",
      "|       nutch|<bvachon@attivio....|[<bvachon@attivio...|Ben Vachon <bvach...|               |   2018-01-05|\n",
      "|    systemml|             mboehm7|[<mboehm7@gmail.c...|Matthias Boehm <m...|        mboehm7|   2018-01-06|\n",
      "|       tomee|             AndyGee|[<agumbrecht@tomi...|Andy Gumbrecht <a...|        AndyGee|   2018-01-01|\n",
      "|    zeppelin|<simonmueller@liv...|[<simonmueller@li...|skymon <simonmuel...|               |   2018-01-05|\n",
      "|       karaf|              MrEasy|[<r.neubauer@seeb...|Rico Neubauer <r....|         MrEasy|   2017-12-31|\n",
      "|       arrow|<brian.hulette@cc...|[<brian.hulette@c...|Brian Hulette <br...|               |   2018-01-05|\n",
      "|       camel|           davsclaus|[<claus.ibsen@gma...|Claus Ibsen <clau...|      davsclaus|   2018-01-06|\n",
      "|  cloudstack|<the.evergreen@gm...|[<the.evergreen@g...|Frank Maximus <th...|               |   2018-01-05|\n",
      "|        fluo|<furkankamaci@gma...|[<furkankamaci@gm...|Furkan KAMACI <fu...|               |   2018-01-02|\n",
      "|      hadoop| <epayne@apache.org>|[<epayne@apache.o...|Eric Payne <epayn...|               |   2018-01-05|\n",
      "|        kudu|<anjuwong@g.ucla....|[<anjuwong@g.ucla...|Andrew Wong <anju...|               |   2018-01-04|\n",
      "|        kudu|<sailesh@apache.org>|[<sailesh@apache....|Sailesh Mukil <sa...|               |   2018-01-04|\n",
      "+------------+--------------------+--------------------+--------------------+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "2240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(project_name='beam', new_unique_id='<p.kaczmarczyk@ocado.com>', emails=['<p.kaczmarczyk@ocado.com>'], Author='Pawel Kaczmarczyk <p.kaczmarczyk@ocado.com>', github_username='', latest_commit=datetime.date(2018, 1, 2)),\n",
       " Row(project_name='cassandra', new_unique_id='<MWeiser@ardmorefinancial.com>', emails=['<MWeiser@ardmorefinancial.com>'], Author='Matthias Weiser <MWeiser@ardmorefinancial.com>', github_username='', latest_commit=datetime.date(2018, 1, 2)),\n",
       " Row(project_name='cxf', new_unique_id='<butkovic@gmail.com>', emails=['<butkovic@gmail.com>'], Author='Peter Butkovic <butkovic@gmail.com>', github_username='', latest_commit=datetime.date(2017, 12, 31)),\n",
       " Row(project_name='cxf', new_unique_id='<simon.marti@inventage.com>', emails=['<simon.marti@inventage.com>'], Author='Simon Marti <simon.marti@inventage.com>', github_username='', latest_commit=datetime.date(2017, 12, 31)),\n",
       " Row(project_name='eagle', new_unique_id='<yonzhang@ebay.com>', emails=['<yonzhang@ebay.com>'], Author='yonzhang <yonzhang@ebay.com>', github_username='', latest_commit=datetime.date(2018, 1, 2))]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_distinct_authors_latest_commit.show()\n",
    "print(active_distinct_authors_latest_commit.count())\n",
    "active_distinct_authors_latest_commit.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "| project_name|author_count|\n",
      "+-------------+------------+\n",
      "|         lucy|           2|\n",
      "|       roller|           1|\n",
      "|        geode|          47|\n",
      "|       falcon|           4|\n",
      "|trafficserver|          35|\n",
      "|          tez|           6|\n",
      "|       pdfbox|           4|\n",
      "|        httpd|          14|\n",
      "|   carbondata|          33|\n",
      "|        celix|           6|\n",
      "|     accumulo|          12|\n",
      "|       wicket|          14|\n",
      "|        twill|           2|\n",
      "|      couchdb|          22|\n",
      "|       bigtop|          12|\n",
      "|     marmotta|           2|\n",
      "|          vcl|           1|\n",
      "|   freemarker|           4|\n",
      "|       buildr|           2|\n",
      "|        shiro|           2|\n",
      "+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "active_num_authors_by_project = active_distinct_authors_latest_commit.groupBy(\"project_name\").agg(F.count(\"Author\").alias(\"author_count\"))\n",
    "active_num_authors_by_project.cache()\n",
    "active_num_authors_by_project.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_raw_sample_sizes = active_num_authors_by_project.withColumn(\n",
    "    \"sample_size_1\",\n",
    "    compute_num_required_sample_1(\"author_count\")).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing\n"
     ]
    }
   ],
   "source": [
    "active_sample_sizes = non_blocking_df_save_or_load_csv(\n",
    "    active_raw_sample_sizes,\n",
    "    \"{0}/active_sample_sizes_13\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_active_sample_sizes = active_sample_sizes.filter(\n",
    "    active_sample_sizes.sample_size_1 > 10).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|sum(sample_size_1)|\n",
      "+------------------+\n",
      "|              1605|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_active_sample_sizes.groupby().agg(F.sum(\"sample_size_1\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's probably ok, lets go ahead and compute the sample set for each project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_fractions = filtered_active_sample_sizes.withColumn(\n",
    "    \"sample_fraction\",\n",
    "    filtered_active_sample_sizes.sample_size_1 / filtered_active_sample_sizes.author_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_sample_fractions = sample_fractions.select(sample_fractions.project_name, sample_fractions.sample_fraction).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_authors = active_distinct_authors_latest_commit.sampleBy(\n",
    "    \"project_name\",\n",
    "    fractions=dict(map(lambda r: (r[0], r[1]), local_sample_fractions)),\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing\n"
     ]
    }
   ],
   "source": [
    "sampled_authors_saved = non_blocking_df_save_or_load(\n",
    "    sampled_authors, \"{0}/sampled_authors_3\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+---------------+-------------+\n",
      "|project_name|       new_unique_id|              emails|              Author|github_username|latest_commit|\n",
      "+------------+--------------------+--------------------+--------------------+---------------+-------------+\n",
      "|      ambari|<adoroszlai@apach...|[<adoroszlai@apac...|Doroszlai, Attila...|               |   2018-01-05|\n",
      "|      ambari|<kinow@users.nore...|[<kinow@users.nor...|Bruno P. Kinoshit...|               |   2017-12-31|\n",
      "|       arrow|           JinHai-CN|[<haijin.chn@gmai...|Jin Hai <haijin.c...|      JinHai-CN|   2018-01-03|\n",
      "|        beam|<jmarble@kochava....|[<jmarble@kochava...|Jacob Marble <jma...|               |   2018-01-05|\n",
      "|        beam|<udim@users.norep...|[<udim@users.nore...|Udi Meiri (Ehud) ...|               |   2018-01-05|\n",
      "|     calcite|<maryann.xue@gmai...|[<maryann.xue@gma...|maryannxue <marya...|               |   2018-01-04|\n",
      "|       camel|             gautric|[<gautric@redhat....|gautric <gautric@...|        gautric|   2017-12-31|\n",
      "|  carbondata|<praveenmeenakshi...|[<praveenmeenaksh...|praveenmeenakshi5...|               |   2018-01-04|\n",
      "|  cloudstack|<372575+khos2ow@u...|[<372575+khos2ow@...|Khosrow Moossavi ...|               |   2018-01-02|\n",
      "|     couchdb|<alexander@spotme...|[<alexander@spotm...|AlexanderKarabero...|               |   2018-01-05|\n",
      "|      groovy|<jwagenleitner@ap...|[<jwagenleitner@a...|John Wagenleitner...|               |   2018-01-01|\n",
      "|      ignite|<ilantukh@gridgai...|[<ilantukh@gridga...|Ilya Lantukh <ila...|               |   2018-01-05|\n",
      "|      ignite|<pivanov@gridgain...|[<pivanov@gridgai...|Ivanov Petr <piva...|               |   2018-01-03|\n",
      "|      impala|<dknupp@cloudera....|[<dknupp@cloudera...|David Knupp <dknu...|               |   2018-01-05|\n",
      "|       karaf|    <fpa@openrun.re>|  [<fpa@openrun.re>]|Francois Papon <f...|               |   2018-01-05|\n",
      "|        nifi| <bbende@apache.org>|[<bbende@apache.o...|Bryan Bende <bben...|               |   2018-01-05|\n",
      "|       samza|<tstumpges@ntent....|[<tstumpges@ntent...|thunderstumpges <...|               |   2018-01-01|\n",
      "|      struts|<zalsaeed@cs.uore...|[<zalsaeed@cs.uor...|zalsaeed <zalsaee...|               |   2018-01-01|\n",
      "|        tika|<nassif.lfcn@dpf....|[<nassif.lfcn@dpf...|Nassif <nassif.lf...|               |   2018-01-04|\n",
      "|   cassandra|         mebigfatguy|[<dbrosius@mebigf...|Dave Brosius <dbr...|    mebigfatguy|   2018-01-06|\n",
      "+------------+--------------------+--------------------+--------------------+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampled_authors_saved.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_authors_saved.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the sampled authors with the e-mails on the dev list and find the top 3 most recent responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(project_name,StringType,true),StructField(new_unique_id,StringType,true),StructField(emails,ArrayType(StringType,true),true),StructField(Author,StringType,true),StructField(github_username,StringType,true),StructField(latest_commit,DateType,true)))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_authors_saved.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_name: string (nullable = true)\n",
      " |-- box_type: string (nullable = true)\n",
      " |-- mbox_id: string (nullable = true)\n",
      " |-- backend_name: string (nullable = true)\n",
      " |-- backend_version: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- data: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- perceval_version: string (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- timestamp: double (nullable = true)\n",
      " |-- updated_on: double (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- from: string (nullable = true)\n",
      " |-- from_processed_email: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- message_id: string (nullable = true)\n",
      " |-- in_reply_to: string (nullable = true)\n",
      " |-- content_language: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mailing_list_posts_mbox_df_saved.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_conditions = [\n",
    "    sampled_authors_saved.project_name == mailing_list_posts_mbox_df_saved.project_name,\n",
    "    F.expr(\"array_contains(emails, from_processed_email)\")]\n",
    "posts_by_sampled_authors = sampled_authors_saved.join(\n",
    "    mailing_list_posts_mbox_df_saved,\n",
    "    join_conditions).select(\n",
    "    \"message_id\", \"new_unique_id\").alias(\"posts_by_sampled_authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          message_id|       new_unique_id|\n",
      "+--------------------+--------------------+\n",
      "|<CANDVwqg13PByzo_...|<ankitsinghal59@g...|\n",
      "|<CANDVwqjRaTApW7G...|<ankitsinghal59@g...|\n",
      "|<57465780.7010801...| <elserj@apache.org>|\n",
      "|<CANDVwqgNEhpxw0y...|<ankitsinghal59@g...|\n",
      "|<CALcavhvKkGNey-=...|<maryann.xue@gmai...|\n",
      "|<CALcavhv32a+RvJV...|<maryann.xue@gmai...|\n",
      "|<1391490711.89825...|           lhofhansl|\n",
      "|<1391493045.37071...|           lhofhansl|\n",
      "|<1391667527.54276...|           lhofhansl|\n",
      "|<1391716442.94641...|           lhofhansl|\n",
      "|<1391931449.79690...|           lhofhansl|\n",
      "|<1391931663.53663...|           lhofhansl|\n",
      "|<1392077000.11292...|           lhofhansl|\n",
      "|<1392587082.23386...|           lhofhansl|\n",
      "|<1392599989.30313...|           lhofhansl|\n",
      "|<1392601705.67962...|           lhofhansl|\n",
      "|<CAN=qJ2A4+F926bP...|            jmahonin|\n",
      "|<CAN=qJ2C7j=TfF7_...|            jmahonin|\n",
      "|<CAN=qJ2DseRA3bOd...|            jmahonin|\n",
      "|<CAN=qJ2DUJ_73gBz...|            jmahonin|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts_by_sampled_authors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailing_list_posts_in_reply_to = mailing_list_posts_mbox_df_saved.filter(\n",
    "    mailing_list_posts_mbox_df_saved.in_reply_to.isNotNull()).alias(\"mailing_list_posts_in_reply_to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_with_replies = posts_by_sampled_authors.join(\n",
    "    mailing_list_posts_in_reply_to,\n",
    "    [F.col(\"mailing_list_posts_in_reply_to.in_reply_to\") == posts_by_sampled_authors.message_id],\n",
    "    \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_in_response_to_user = posts_with_replies.select(\n",
    "    posts_with_replies.new_unique_id,\n",
    "    posts_with_replies.timestamp,\n",
    "    posts_with_replies.project_name,\n",
    "    posts_with_replies.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_pronouns = [\"they\", \"ze\", \"he\", \"she\", \"her\", \"his\", \"their\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backing jar /sparklingml/sparklingml/../target/scala-2.11/sparklingml-assembly-0.0.1-SNAPSHOT.jar\n"
     ]
    }
   ],
   "source": [
    "from sparklingml.feature.python_pipelines import SpacyTokenizeTransformer\n",
    "tokenizer = SpacyTokenizeTransformer(inputCol=\"body\", outputCol=\"body_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_in_response_to_user_tokenized = tokenizer.transform(posts_in_response_to_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hack until I fix sparklingml\n",
    "def mini_tokenize(doc):\n",
    "    return doc.lower().split(\" \")\n",
    "\n",
    "mini_tokenize_udf = UserDefinedFunction(mini_tokenize, StringType(), \"mini_tokenize_udf\")\n",
    "\n",
    "\n",
    "posts_in_response_to_user_tokenized = posts_in_response_to_user.withColumn(\n",
    "    \"body_tokens\",\n",
    "    mini_tokenize_udf(posts_in_response_to_user.body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_pronoun(tokens):\n",
    "    return any(pronoun in tokens for pronoun in common_pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_pronoun_udf = UserDefinedFunction(contains_pronoun, BooleanType(), \"contains_pronoun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_in_response_to_user_with_pronouns = posts_in_response_to_user_tokenized.filter(\n",
    "    contains_pronoun_udf(posts_in_response_to_user_tokenized.body_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_in_response_to_user_grouped = posts_in_response_to_user_with_pronouns.orderBy(\n",
    "    posts_in_response_to_user.timestamp).groupBy(posts_in_response_to_user.new_unique_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_in_response_to_user_collected = posts_in_response_to_user_grouped.agg(\n",
    "    F.collect_list(posts_in_response_to_user_with_pronouns.body).alias(\"emails\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving\n"
     ]
    }
   ],
   "source": [
    "posts_in_response_to_user_collected_saved = non_blocking_df_save_or_load(\n",
    "    posts_in_response_to_user_collected,\n",
    "    \"{0}/posts_by_user\".format(fs_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sample for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to infer Gender off of name. This has problems, see https://ironholds.org/names-gender/ for a discussion on why this is problematic, but if it matches our statistical samples from above it can augment our understanding of the data. However without doing this it's difficult to get much of a picture (see above where we attempt to gender from other sources, the hit rate leaves something to be desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}