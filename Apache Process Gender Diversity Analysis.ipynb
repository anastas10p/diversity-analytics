{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook seeks to explore the gender diversity of the different apache projects & the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import collect_set, explode, from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import json\n",
    "import os\n",
    "import meetup.api\n",
    "from copy import copy\n",
    "import time\n",
    "import logging\n",
    "\n",
    "from watson_developer_cloud import ToneAnalyzerV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "API key configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "meetup_key = os.getenv(\"MEETUP_APIKEY\")\n",
    "tone_bluemix_user = os.getenv(\"TONE_BLUEMIX_USERNAME\")\n",
    "tone_bluemix_password = os.getenv(\"TONE_BLUEMIX_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Less secret configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_meetup_events = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "session = SparkSession.builder.appName(\"whatCanWeLearnFromTheSixties\").getOrCreate()\n",
    "sc = session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The first thing we want to get is the committers and PMC members, this information is stored in LDAP but also available in JSON. Eventually we will want to enrich this with mailing list information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loadFlatJsonFile(path, explodeKey, schema=None):\n",
    "    \"\"\"Load a flat multi-line json file and convert into Spark & explode\"\"\"\n",
    "    rdd = sc.wholeTextFiles(path).values()\n",
    "    df = (session.read.schema(schema)\n",
    "            .json(rdd))\n",
    "    return df.select(explode(explodeKey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "apache_people_schema = StructType([StructField(\"lastCreateTimestamp\", StringType()),\n",
    "                     StructField(\"people\",\n",
    "                                 MapType(StringType(), \n",
    "                                         StructType([StructField('name', StringType()),\n",
    "                                                     StructField('key_fingerprints', ArrayType(StringType())),\n",
    "                                                     StructField('urls', ArrayType(StringType())),\n",
    "                                                    ]))\n",
    "                                )])\n",
    "apache_people_df = loadFlatJsonFile(path=\"http_data_sources/public_ldap_people.json\", # http://people.apache.org/public/public_ldap_people.json\n",
    "                                 explodeKey=\"people\", schema=apache_people_schema)\n",
    "apache_people_df = apache_people_df.select(apache_people_df.key.alias(\"username\"), apache_people_df.value.alias(\"extra\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def project_on_github(project):\n",
    "    \"\"\"Returns if a project is on github\"\"\"\n",
    "    import urllib3\n",
    "    http = urllib3.PoolManager()\n",
    "    r = http.request('GET', \"https://github.com/apache/{0}\".format(project))\n",
    "    return r.status == 200\n",
    "session.catalog.registerFunction(\"on_github\", project_on_github, BooleanType())\n",
    "# Except I'm a bad person so....\n",
    "from pyspark.sql.catalog import UserDefinedFunction\n",
    "project_on_github_udf = UserDefinedFunction(project_on_github, BooleanType(), \"on_github\")\n",
    "session.catalog._jsparkSession.udf().registerPython(\"on_github\", project_on_github_udf._judf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_committees_schema = StructType([StructField(\"lastCreateTimestamp\", StringType()),\n",
    "                     StructField(\"committees\",\n",
    "                                 MapType(StringType(), StructType([StructField('roster', ArrayType(StringType())),\n",
    "                                                                  StructField('modifyTimestamp', StringType()),\n",
    "                                                                  StructField('createTimestamp', StringType())\n",
    "                                                                  ])))])\n",
    "apache_committees_df = loadFlatJsonFile(path=\"http_data_sources/public_ldap_committees.json\", # http://people.apache.org/public/public_ldap_people.json\n",
    "                                 explodeKey=\"committees\", schema=apache_committees_schema)\n",
    "apache_committees_on_github_df = apache_committees_df.filter(project_on_github_udf(apache_committees_df.key))\n",
    "apache_committees_on_github_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "committee_names_df = apache_committees_on_github_df.select(apache_committees_df.key.alias(\"project\"))\n",
    "committee_names_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "committee_names_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project_to_user_df = apache_committees_on_github_df.select(\n",
    "    apache_committees_on_github_df.key.alias(\"project\"),\n",
    "    explode(apache_committees_on_github_df.value.roster).alias(\"username\"))\n",
    "\n",
    "\n",
    "user_to_project_df = project_to_user_df.groupBy(project_to_user_df.username).agg(\n",
    "    collect_set(project_to_user_df.project))\n",
    "apache_people_df = apache_people_df.join(user_to_project_df, on=\"username\")\n",
    "apache_people_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(username='a_horuzhenko', extra='{\"name\":\"Artyom Horuzhenko\"}', projects=['openmeetings'], jsontostruct(extra)=Row(name='Artyom Horuzhenko', key_fingerprints=None), collect_set(project)=['openmeetings'], jsontostruct(extra)=Row(name='Artyom Horuzhenko', key_fingerprints=None, urls=None)),\n",
       " Row(username='angela_wang', extra='{\"name\":\"Angela Wang\"}', projects=['oodt'], jsontostruct(extra)=Row(name='Angela Wang', key_fingerprints=None), collect_set(project)=['oodt'], jsontostruct(extra)=Row(name='Angela Wang', key_fingerprints=None, urls=None)),\n",
       " Row(username='elserj', extra='{\"name\":\"Josh Elser\",\"key_fingerprints\":[\"9E62 822F 4668 F17B 0972  ADD9 B7D5 CD45 4677 D66C\",\"ABC8 914C 675F AD3F A74F  39B2 D146 D62C AB47 1AE9\"]}', projects=['accumulo', 'calcite', 'incubator', 'phoenix'], jsontostruct(extra)=Row(name='Josh Elser', key_fingerprints=['9E62 822F 4668 F17B 0972  ADD9 B7D5 CD45 4677 D66C', 'ABC8 914C 675F AD3F A74F  39B2 D146 D62C AB47 1AE9']), collect_set(project)=['incubator', 'phoenix', 'accumulo', 'calcite'], jsontostruct(extra)=Row(name='Josh Elser', key_fingerprints=['9E62 822F 4668 F17B 0972  ADD9 B7D5 CD45 4677 D66C', 'ABC8 914C 675F AD3F A74F  39B2 D146 D62C AB47 1AE9'], urls=None)),\n",
       " Row(username='jasondalycan', extra='{\"name\":\"Jason Daly\"}', projects=['stratos'], jsontostruct(extra)=Row(name='Jason Daly', key_fingerprints=None), collect_set(project)=['stratos'], jsontostruct(extra)=Row(name='Jason Daly', key_fingerprints=None, urls=None)),\n",
       " Row(username='lgross', extra='{\"name\":\"Lukas Gross\",\"key_fingerprints\":[\"293607E3\"]}', projects=['chemistry'], jsontostruct(extra)=Row(name='Lukas Gross', key_fingerprints=['293607E3']), collect_set(project)=['chemistry'], jsontostruct(extra)=Row(name='Lukas Gross', key_fingerprints=['293607E3'], urls=None))]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt to parse the extra information in json\n",
    "schema = StructType([StructField('name', StringType()),\n",
    "                     StructField('key_fingerprints', ArrayType(StringType())),\n",
    "                     StructField('urls', ArrayType(StringType())),\n",
    "                    ])\n",
    "apache_people_df = apache_people_df.select(\"*\", from_json(apache_people_df.extra, schema=schema))\n",
    "apache_people_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Attempt to fetch relevant past & present meetups for each project - idea based on the listing at https://www.apache.org/events/meetups.html but different code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(\"WARN\")\n",
    "def lookup_relevant_meetup(project_name, max_meetup_events=0):\n",
    "    \"\"\"Lookup relevant meetups for a specific project.\"\"\"\n",
    "    import logging\n",
    "    import time\n",
    "    import meetup.api\n",
    "    logger = logging.getLogger()\n",
    "    meetup_delay = 30\n",
    "    meetup_reset_delay = 1800 # 30 minutes\n",
    "    standard_keys = {\"text_format\": \"plain\", \"trending\": \"desc=true\", \"and_text\": \"true\", \"city\": \"san francisco\", \"country\": \"usa\", \"text\": \"apache \" + project_name, \"radius\": 10000}\n",
    "    results = {\"upcoming\": [], \"past\": []}\n",
    "    for status in [\"upcoming\", \"past\"]:\n",
    "        keys = copy(standard_keys)\n",
    "        keys[\"status\"] = status\n",
    "        count = 200\n",
    "        base = 0\n",
    "        while (count == 200 and (max_meetup_events == 0 or base < max_meetup_events)):\n",
    "            logging.debug(\"Fetch {0} meetups for {1} on base {2}\".format(status, project_name, base))\n",
    "            project_name = \"spark\"\n",
    "            client = client = meetup.api.Client(meetup_key)\n",
    "            if base > 0:\n",
    "                keys[\"page\"] = base\n",
    "            # Manually sleep for meetup_reset_delay on failure, the meetup-api package retry logic sometimes breaks :(\n",
    "            response = None\n",
    "            retry_count = 0\n",
    "            while response is None and retry_count < 10:\n",
    "                try:\n",
    "                    response = client.GetOpenEvents(**keys)\n",
    "                except:\n",
    "                    response = None\n",
    "                    retry_count += 1\n",
    "                    time.sleep(meetup_reset_delay)\n",
    "                    try:\n",
    "                        response = client.GetOpenEvents(**keys)\n",
    "                    except:\n",
    "                        response = None\n",
    "            try:\n",
    "                count = response.meta['count']\n",
    "                base = base + count\n",
    "                results[status].append(response.results)\n",
    "                time.sleep(meetup_delay)\n",
    "            except:\n",
    "                count = 0\n",
    "    return (project_name, results)\n",
    "\n",
    "project_meetups_df = committee_names_df.rdd.map(lambda x: x.project).map(lambda name: lookup_relevant_meetup(name, max_meetup_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "project_meetups_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "project_meetups_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the provided projects attempt to lookup their GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lookup_project_git(project):\n",
    "    \"\"\"Returns the project github for a specific project. Assumes project is git hosted\"\"\"\n",
    "    return \"git://git.apache.org/{0}\".format(project)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fetch_project_github_data(project):\n",
    "    from perceval.backends.core.github import GitHub as perceval_github\n",
    "    gh_backend = perceval_github(owner=\"apache\", repository=project)\n",
    "    # The backend return a generator - which is awesome. However since we want to pull this data into Spark \n",
    "    return list(gh_backend.fetch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fetch_project_git_data(project):\n",
    "    project_git = lookup_project_git(project)\n",
    "    from perceval.backends.core.git import Git as perceval_git\n",
    "    git_backend = perceval_git(owner=\"apache\", repository=project)\n",
    "    return list(gh_backend.fetch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LazyDriver(object):\n",
    "    _driver = None\n",
    "    \n",
    "    @classmethod\n",
    "    def get(cls):\n",
    "        if cls._driver is None:\n",
    "            from selenium import webdriver\n",
    "            cls._driver = webdriver.Firefox()\n",
    "        return cls._driver\n",
    "    \n",
    "bcast_driver = sc.broadcast(LazyDriver)\n",
    "\n",
    "def lookup_crunchbase_info(people_and_projects):\n",
    "    \"\"\"Lookup a person a crunch base and see what the gender & company is.\n",
    "    Filter for at least one mention of their projects.\"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    driver = bcast_driver.value.get()\n",
    "    for (name, projects) in people_and_projects:\n",
    "        # robots.txt seems to be ok with person for now, double check before re-running this\n",
    "        url = \"https://www.crunchbase.com/person/{0}\".format(name.replace(\" \", \"-\"))\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            text = driver.page_source\n",
    "            lower_text = text.lower()\n",
    "            if any(project.lower() in lower_text for project in projects):\n",
    "                soup = BeautifulSoup(text, \"html.parser\")\n",
    "                stats = soup.findAll(\"div\", { \"class\" : \"info-card-overview-content\"})[0]\n",
    "                # Hacky but I'm lazy\n",
    "                result = {}\n",
    "                try:\n",
    "                    m = re.search(\"Gender:\\</dt\\>\\<dd\\>(.+?)\\<\", str(stats))\n",
    "                    result[\"gender\"] = m.group(1)\n",
    "                except:\n",
    "                    # If nothing matches thats ok\n",
    "                    pass\n",
    "                try:\n",
    "                    m = re.search(\"data-name=\\\"(.+?)\\\" data-permalink=\\\"\\/organization\", str(stats))\n",
    "                    result[\"company\"] = m.group(1)\n",
    "                except:\n",
    "                    # No match no foul\n",
    "                    pass\n",
    "                return [result]\n",
    "            else:\n",
    "                return None\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'company': 'IBM', 'gender': 'Female'}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = lookup_crunchbase_info([(\"holden karau\", [\"spark\"])])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Augment the committer info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "session.catalog.registerFunction(\"lookup_crunchbase_info\", lookup_crunchbase_info, MapType(StringType(), StringType()))\n",
    "# Except I'm a bad person so....\n",
    "from pyspark.sql.catalog import UserDefinedFunction\n",
    "lookup_crunchbase_info_udf = UserDefinedFunction(lookup_crunchbase_info, MapType(StringType(), StringType()))\n",
    "session.catalog._jsparkSession.udf().registerPython(\"lookup_crunchbase_info\", lookup_crunchbase_info_udf._judf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(key='abdera', value=Row(roster=['antelder', 'calavera', 'ckoppelt', 'dandiep', 'jancona', 'jmsnell', 'jrduncans', 'rooneg', 'ugo'], modifyTimestamp='20100614203655Z', createTimestamp='20100210202116Z')),\n",
       " Row(key='accumulo', value=Row(roster=['acordova', 'afuchs', 'bhavanki', 'billie', 'bimargulies', 'brianloss', 'busbey', 'cawaring', 'cjnolet', 'ctubbsii', 'dhutchis', 'dlmarion', 'drew', 'ecn', 'edcoleman', 'elserj', 'jtrost', 'kturner', 'mdrob', 'medined', 'mjwall', 'mmiller', 'mwalch', 'phrocker', 'rweeks', 'shickey', 'ujustgotbilld', 'vikrams', 'vines'], modifyTimestamp='20161103142247Z', createTimestamp='20120323210423Z')),\n",
       " Row(key='ace', value=Row(roster=['angelos', 'bramk', 'btopping', 'clement', 'cziegeler', 'jawi', 'jbonofre', 'marrs', 'paulb', 'pauls', 'tonit'], modifyTimestamp='20130718091532Z', createTimestamp='20120102142133Z')),\n",
       " Row(key='activemq', value=Row(roster=['artnaseef', 'brianm', 'bsnyder', 'ceposta', 'chirino', 'clebertsuconic', 'cshannon', 'dain', 'davsclaus', 'dejanb', 'djencks', 'dkulp', 'gertv', 'gnodet', 'gtully', 'hogstrom', 'jgenender', 'jgomes', 'jsisson', 'jstrachan', 'martyntaylor', 'rajdavies', 'romkal', 'tabish'], modifyTimestamp='20161027053701Z', createTimestamp='20100210202116Z')),\n",
       " Row(key='airavata', value=Row(roster=['amilaj', 'aslom', 'ate', 'chathura', 'chathuri', 'chinthaka', 'eroma', 'hemapani', 'heshan', 'lahiru', 'mattmann', 'milinda', 'mpierce', 'ndoshi', 'patanachai', 'raminder', 'samindaw', 'scnakandala', 'shahani', 'shameera', 'smarru', 'thilina'], modifyTimestamp='20161212050113Z', createTimestamp='20120920073719Z'))]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apache_committees_on_github_df.select(\"*\", lookup_crunchbase_info_udf).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "One of the things that is interesting is understanding what the tones of the meetup descriptions & mailing list posts are. We can use https://www.ibm.com/watson/developercloud/tone-analyzer/api/v3/?python#introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lookup_tone(document):\n",
    "    \"\"\"Looks up the tone for a specific document. Returns a json blob.\"\"\"\n",
    "    from watson_developer_cloud import ToneAnalyzerV3\n",
    "    tone_analyzer = ToneAnalyzerV3(\n",
    "        username=tone_bluemix_user,\n",
    "        password=tone_bluemix_password,\n",
    "        version='2016-05-19 ')\n",
    "    return tone_analyzer.tone(text=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lookup_tone' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-b855cb4049e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moh_no_you\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_tone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"oh no you didn't girl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lookup_tone' is not defined"
     ]
    }
   ],
   "source": [
    "oh_no_you = lookup_tone(\"oh no you didn't girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oh_no_you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ok its time to find some mailing list info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LazyPool(object):\n",
    "    _pool = None\n",
    "    \n",
    "    @classmethod\n",
    "    def get(cls):\n",
    "        if cls._pool is None:\n",
    "            import urllib3\n",
    "            cls._pool = urllib3.PoolManager()\n",
    "        return cls._pool\n",
    "    \n",
    "bcast_pool = sc.broadcast(LazyPool)\n",
    "\n",
    "def fetch_mbox_data(project_name):\n",
    "    import tempfile\n",
    "    import shutil\n",
    "    from perceval.backends.core.mbox import MBox as perceval_mbox\n",
    "    mbox_user_url = \"http://mail-archives.apache.org/mod_mbox/{0}-user\".format(project_name)\n",
    "    mbox_dev_url = \"http://mail-archives.apache.org/mod_mbox/{0}-dev\".format(project_name)\n",
    "    def fetch_mbox_from_apache_site(root_url):\n",
    "        \"\"\"Fetches all of the mbox files from a given apache url\"\"\"\n",
    "        # Make a temp directory to hold the mbox files, we will return this directory\n",
    "        tempdir = tempfile.mkdtemp()\n",
    "        \n",
    "        # Fetch the page to parse\n",
    "        pool = bcast_pool.value.get()\n",
    "        result = pool.request('GET', root_url)\n",
    "        \n",
    "        \n",
    "        from bs4 import BeautifulSoup\n",
    "        soup = BeautifulSoup(result.data, \"html.parser\")\n",
    "        mbox_ids = set(map(lambda tag: tag.get('id'), soup.findAll(\"span\", { \"class\" : \"links\"})))\n",
    "        \n",
    "        for mbox_id in mbox_ids:\n",
    "            mbox_url = \"{0}/{1}.mbox\".format(root_url, mbox_id)\n",
    "            filename = \"{0}/{1}.mbox\".format(tempdir, mbox_id)\n",
    "            print(\"fetching {0}\".format(mbox_url))\n",
    "            with pool.request('GET', mbox_url, preload_content=False) as r, open(filename, 'wb') as out_file:       \n",
    "                shutil.copyfileobj(r, out_file)\n",
    "        \n",
    "        return tempdir\n",
    "    def process_mbox_directory(base_url, dir_path):\n",
    "        mbox_backend = perceval_mbox(base_url, dir_path)\n",
    "        return mbox_backend.fetch()\n",
    "\n",
    "    def load_and_process(base_url):\n",
    "        output_dir = fetch_mbox_from_apache_site(base_url)\n",
    "        return list(process_mbox_directory(base_url, output_dir))\n",
    "    \n",
    "    return list(map(load_and_process, [mbox_user_url, mbox_dev_url]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching http://mail-archives.apache.org/mod_mbox/spark-user/201601.mbox\n",
      "fetching http://mail-archives.apache.org/mod_mbox/spark-user/201704.mbox\n",
      "fetching http://mail-archives.apache.org/mod_mbox/spark-dev/201601.mbox\n",
      "fetching http://mail-archives.apache.org/mod_mbox/spark-dev/201704.mbox\n"
     ]
    }
   ],
   "source": [
    "fetched_mbox_data = fetch_mbox_data(\"spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backend_name': 'MBox',\n",
       " 'backend_version': '0.7.3',\n",
       " 'category': 'message',\n",
       " 'data': {'Content-type': 'multipart/alternative;\\n\\tboundary=\"B_3534423661_103550\"',\n",
       "  'Date': 'Thu, 31 Dec 2015 16:20:57 -0800',\n",
       "  'Delivered-To': 'mailing list user@spark.apache.org',\n",
       "  'From': 'Andy Davidson <Andy@SantaCruzIntegration.com>',\n",
       "  'In-Reply-To': '<D2AAA15D.2C933%bang_zippy@yahoo.com>',\n",
       "  'List-Help': '<mailto:user-help@spark.apache.org>',\n",
       "  'List-Id': '<user.spark.apache.org>',\n",
       "  'List-Post': '<mailto:user@spark.apache.org>',\n",
       "  'List-Unsubscribe': '<mailto:user-unsubscribe@spark.apache.org>',\n",
       "  'Mailing-List': 'contact user-help@spark.apache.org; run by ezmlm',\n",
       "  'Message-ID': '<D2AB064B.2C995%bang_zippy@yahoo.com>',\n",
       "  'Mime-version': '1.0',\n",
       "  'Precedence': 'bulk',\n",
       "  'Received': 'from [192.168.1.131] (c-69-181-234-49.hsd1.ca.comcast.net [69.181.234.49])\\n\\t(Authenticated sender: Andy@SantaCruzIntegration.com)\\n\\tby omf01.hostedemail.com (Postfix) with ESMTPA\\n\\tfor <user@spark.apache.org>; Fri,  1 Jan 2016 00:21:01 +0000 (UTC)',\n",
       "  'References': '<D2AAA15D.2C933%bang_zippy@yahoo.com>',\n",
       "  'Return-Path': '<user-return-48848-apmail-spark-user-archive=spark.apache.org@spark.apache.org>',\n",
       "  'Sender': 'Andrew Davidson <bang_zippy@yahoo.com>',\n",
       "  'Subject': 'does HashingTF maintain a inverse index?',\n",
       "  'Thread-Topic': 'does HashingTF maintain a inverse index?',\n",
       "  'To': '\"user @spark\" <user@spark.apache.org>',\n",
       "  'User-Agent': 'Microsoft-MacOutlook/14.5.9.151119',\n",
       "  'X-Filterd-Recvd-Size': '2297',\n",
       "  'X-HE-Tag': 'knee66_603d7b2d22529',\n",
       "  'X-Original-To': 'apmail-spark-user-archive@minotaur.apache.org',\n",
       "  'X-Session-Marker': '416E64794053616E74614372757A496E746567726174696F6E2E636F6D',\n",
       "  'X-Spam-Flag': 'NO',\n",
       "  'X-Spam-Level': '******',\n",
       "  'X-Spam-Score': '6.224',\n",
       "  'X-Spam-Status': 'No, score=6.224 tagged_above=-999 required=6.31\\n\\ttests=[FORGED_MSGID_YAHOO=2.244, HTML_MESSAGE=3,\\n\\tKAM_LAZY_DOMAIN_SECURITY=1, RCVD_IN_MSPIKE_H3=-0.01,\\n\\tRCVD_IN_MSPIKE_WL=-0.01] autolearn=disabled',\n",
       "  'X-Spam-Summary': '2,0,0,,d41d8cd98f00b204,andy@santacruzintegration.com,:,RULES_HIT:41:355:379:962:988:989:1189:1221:1260:1261:1313:1314:1345:1359:1381:1437:1516:1517:1518:1534:1539:1567:1575:1588:1589:1594:1711:1714:1730:1749:1777:1792:2559:2562:3138:3139:3140:3141:3142:3690:3865:3866:3867:3868:3871:4184:4361:5007:7652:7903:8603:10004:10400:10848:11658:11914:12043:12517:12519:12663:13018:13019:13071:13139:13200:13229:13618:14659:21060:21080:30026:30051:30054:30090,0,RBL:none,CacheIP:none,Bayesian:0.5,0.5,0.5,Netcheck:none,DomainCache:0,MSF:not bulk,SPF:fn,MSBL:0,DNSBL:none,Custom_rules:0:0:0,LFtime:1,LUA_SUMMARY:none',\n",
       "  'X-Virus-Scanned': 'Debian amavisd-new at spamd1-us-west.apache.org',\n",
       "  'body': {'html': '<html><head></head><body style=\"word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; color: rgb(0, 0, 0); font-size: 14px; font-family: Courier, sans-serif;\"><div><div>Hi&nbsp;</div><div><br></div><div>I am working on proof of concept. I am trying to use spark to classify some documents. I am using tokenizer and hashingTF to convert the documents into vectors. Is there any easy way to map feature back to words or do I need to maintain the reverse index my self? I realize there is a chance some words map to same buck</div><div><br></div><div>Kind regards</div><div><br></div><div>Andy</div></div><div><br></div></body></html>\\n',\n",
       "   'plain': 'Hi \\n\\nI am working on proof of concept. I am trying to use spark to classify some\\ndocuments. I am using tokenizer and hashingTF to convert the documents into\\nvectors. Is there any easy way to map feature back to words or do I need to\\nmaintain the reverse index my self? I realize there is a chance some words\\nmap to same buck\\n\\nKind regards\\n\\nAndy\\n\\n\\n\\n'},\n",
       "  'unixfrom': 'user-return-48848-apmail-spark-user-archive=spark.apache.org@spark.apache.org  Fri Jan  1 00:21:21 2016'},\n",
       " 'origin': 'http://mail-archives.apache.org/mod_mbox/spark-user',\n",
       " 'perceval_version': '0.7.0.dev2',\n",
       " 'tag': 'http://mail-archives.apache.org/mod_mbox/spark-user',\n",
       " 'timestamp': 1492236310.847561,\n",
       " 'updated_on': 1451607657.0,\n",
       " 'uuid': 'e220af8fdd2b32447e9a0a3d12013a3d4ef5a2d4'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetched_mbox_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
